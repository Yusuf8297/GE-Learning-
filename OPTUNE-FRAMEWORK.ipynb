{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd6151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Optional imports\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    import torch\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"[INFO] TabNet not available\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"[INFO] XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    SMOTE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SMOTE_AVAILABLE = False\n",
    "    print(\"[INFO] SMOTE not available\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.pruners import MedianPruner\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"[ERROR] Optuna not available. Install with: pip install optuna\")\n",
    "\n",
    "N_JOBS = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Optuna-based HPO Framework - Comparable to GE Version\")\n",
    "print(f\"Available CPU cores for parallel processing: {N_JOBS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS (Same as GE code)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_confidence_interval(data, confidence=0.95):\n",
    "    if len(data) < 2:\n",
    "        return (np.mean(data), np.mean(data))\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return (mean - h, mean + h)\n",
    "\n",
    "\n",
    "def compute_statistics(data):\n",
    "    if len(data) == 0:\n",
    "        return {'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'median': 0,\n",
    "                'ci_lower': 0, 'ci_upper': 0}\n",
    "    ci_lower, ci_upper = compute_confidence_interval(data)\n",
    "    return {\n",
    "        'mean': float(np.mean(data)),\n",
    "        'std': float(np.std(data)),\n",
    "        'min': float(np.min(data)),\n",
    "        'max': float(np.max(data)),\n",
    "        'median': float(np.median(data)),\n",
    "        'ci_lower': float(ci_lower),\n",
    "        'ci_upper': float(ci_upper)\n",
    "    }\n",
    "\n",
    "\n",
    "def wilcoxon_test(scores1, scores2):\n",
    "    if len(scores1) != len(scores2) or len(scores1) < 2:\n",
    "        return (np.nan, np.nan)\n",
    "    try:\n",
    "        stat, p_value = stats.wilcoxon(scores1, scores2)\n",
    "        return (float(stat), float(p_value))\n",
    "    except Exception:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "def compute_sensitivity_specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    else:\n",
    "        sensitivity = specificity = ppv = npv = 0\n",
    "        tn = fp = fn = tp = 0\n",
    "    \n",
    "    return {\n",
    "        'sensitivity': float(sensitivity),\n",
    "        'specificity': float(specificity),\n",
    "        'ppv': float(ppv),\n",
    "        'npv': float(npv),\n",
    "        'tp': int(tp),\n",
    "        'tn': int(tn),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn)\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETER PRUNING ANALYZER (Same as GE code)\n",
    "# =============================================================================\n",
    "\n",
    "class HyperparameterPruningAnalyzer:\n",
    "    \"\"\"Analyze search space reduction from optimization.\"\"\"\n",
    "    \n",
    "    # Same default hyperparameters as GE code\n",
    "    DEFAULT_HYPERPARAMS = {\n",
    "        'TabNet': {\n",
    "            'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.3, 'lambda_sparse': 1e-3\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': 100, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8\n",
    "        },\n",
    "        'SVM': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'},\n",
    "        'LogisticRegression': {'C': 1.0, 'penalty': 'l2', 'max_iter': 1000},\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Same search space as GE code for fair comparison\n",
    "    SEARCH_SPACE = {\n",
    "        'TabNet': {\n",
    "            'n_d': [8, 16, 32, 64],\n",
    "            'n_a': [8, 16, 32, 64],\n",
    "            'n_steps': [3, 5, 7],\n",
    "            'lambda_sparse': [1e-4, 1e-3, 1e-2]\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [5, 10, 15, None]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pruning_results = {}\n",
    "    \n",
    "    def analyze_model(self, model_name, optimized_configs, best_config,\n",
    "                      default_fitness=None, optimized_fitness=None):\n",
    "        if model_name not in self.SEARCH_SPACE:\n",
    "            return None\n",
    "        \n",
    "        search_space = self.SEARCH_SPACE[model_name]\n",
    "        default_params = self.DEFAULT_HYPERPARAMS.get(model_name, {})\n",
    "        \n",
    "        total_combinations = 1\n",
    "        for param, values in search_space.items():\n",
    "            total_combinations *= len(values)\n",
    "        \n",
    "        param_frequency = {}\n",
    "        for param in search_space.keys():\n",
    "            param_frequency[param] = {}\n",
    "            for value in search_space[param]:\n",
    "                param_frequency[param][str(value)] = 0\n",
    "        \n",
    "        for config in optimized_configs:\n",
    "            for param, value in config.items():\n",
    "                if param in param_frequency:\n",
    "                    str_value = str(value)\n",
    "                    if str_value in param_frequency[param]:\n",
    "                        param_frequency[param][str_value] += 1\n",
    "        \n",
    "        effective_space = 1\n",
    "        pruned_params = {}\n",
    "        for param, freq_dict in param_frequency.items():\n",
    "            selected_values = [v for v, count in freq_dict.items() if count > 0]\n",
    "            pruned_params[param] = {\n",
    "                'original_choices': search_space[param],\n",
    "                'selected_values': selected_values,\n",
    "                'selection_frequency': freq_dict,\n",
    "                'reduction_ratio': len(selected_values) / len(search_space[param])\n",
    "            }\n",
    "            effective_space *= max(1, len(selected_values))\n",
    "        \n",
    "        space_reduction = 1 - (effective_space / total_combinations)\n",
    "        \n",
    "        param_changes = {}\n",
    "        for param in search_space.keys():\n",
    "            default_val = default_params.get(param)\n",
    "            optimized_val = best_config.get(param)\n",
    "            param_changes[param] = {\n",
    "                'default': default_val,\n",
    "                'optimized': optimized_val,\n",
    "                'changed': str(default_val) != str(optimized_val)\n",
    "            }\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'pre_prune': {\n",
    "                'search_space': search_space,\n",
    "                'total_combinations': total_combinations,\n",
    "                'default_params': default_params,\n",
    "                'default_fitness': default_fitness,\n",
    "                'n_hyperparameters': len(search_space)\n",
    "            },\n",
    "            'post_prune': {\n",
    "                'best_config': best_config,\n",
    "                'optimized_fitness': optimized_fitness,\n",
    "                'effective_combinations': effective_space,\n",
    "                'pruned_params': pruned_params,\n",
    "                'n_selected_hyperparameters': sum(1 for p in pruned_params.values()\n",
    "                                                   if len(p['selected_values']) > 0)\n",
    "            },\n",
    "            'reduction_metrics': {\n",
    "                'space_reduction_ratio': space_reduction,\n",
    "                'space_reduction_percent': space_reduction * 100,\n",
    "                'original_combinations': total_combinations,\n",
    "                'effective_combinations': effective_space,\n",
    "                'params_changed': sum(1 for p in param_changes.values() if p['changed']),\n",
    "                'total_params': len(param_changes)\n",
    "            },\n",
    "            'param_changes': param_changes,\n",
    "            'fitness_improvement': {\n",
    "                'default': default_fitness,\n",
    "                'optimized': optimized_fitness,\n",
    "                'absolute_gain': (optimized_fitness - default_fitness) if default_fitness and optimized_fitness else None,\n",
    "                'relative_gain_percent': ((optimized_fitness - default_fitness) / default_fitness * 100)\n",
    "                                         if default_fitness and optimized_fitness and default_fitness > 0 else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.pruning_results[model_name] = result\n",
    "        return result\n",
    "    \n",
    "    def get_summary_table(self):\n",
    "        rows = []\n",
    "        for model_name, result in self.pruning_results.items():\n",
    "            rows.append({\n",
    "                'Model': model_name,\n",
    "                'Pre-Prune HP': result['pre_prune']['n_hyperparameters'],\n",
    "                'Post-Prune HP': result['post_prune']['n_selected_hyperparameters'],\n",
    "                'Pre-Prune Combos': result['pre_prune']['total_combinations'],\n",
    "                'Post-Prune Combos': result['post_prune']['effective_combinations'],\n",
    "                'Reduction (%)': f\"{result['reduction_metrics']['space_reduction_percent']:.1f}\",\n",
    "                'Default AUC': f\"{result['fitness_improvement']['default']:.4f}\" if result['fitness_improvement']['default'] else 'N/A',\n",
    "                'Optimized AUC': f\"{result['fitness_improvement']['optimized']:.4f}\" if result['fitness_improvement']['optimized'] else 'N/A',\n",
    "                'AUC Gain (%)': f\"{result['fitness_improvement']['relative_gain_percent']:.2f}\" if result['fitness_improvement']['relative_gain_percent'] else 'N/A'\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def get_all_results(self):\n",
    "        return self.pruning_results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS CALCULATOR (Same as GE code)\n",
    "# =============================================================================\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(y_true, y_pred, y_proba=None):\n",
    "        metrics = {\n",
    "            'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "            'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "            'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "            'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "            'mcc': float(matthews_corrcoef(y_true, y_pred)),\n",
    "            'kappa': float(cohen_kappa_score(y_true, y_pred)),\n",
    "        }\n",
    "        \n",
    "        sens_spec = compute_sensitivity_specificity(y_true, y_pred)\n",
    "        metrics.update(sens_spec)\n",
    "        \n",
    "        if y_proba is not None:\n",
    "            metrics['roc_auc'] = float(roc_auc_score(y_true, y_proba))\n",
    "            metrics['avg_precision'] = float(average_precision_score(y_true, y_proba))\n",
    "        else:\n",
    "            metrics['roc_auc'] = np.nan\n",
    "            metrics['avg_precision'] = np.nan\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_train_val_gap(train_metrics, val_metrics):\n",
    "        gap = {}\n",
    "        for key in train_metrics:\n",
    "            if key in val_metrics and isinstance(train_metrics[key], (int, float)):\n",
    "                if not np.isnan(train_metrics[key]) and not np.isnan(val_metrics[key]):\n",
    "                    gap[f'{key}_gap'] = float(train_metrics[key] - val_metrics[key])\n",
    "        return gap\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION (Matching GE code)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration matching GE code for fair comparison.\"\"\"\n",
    "    \n",
    "    DATA_CSV: str = \"data.csv\"\n",
    "    TARGET_COLUMN: str = \"diagnosis\"\n",
    "    \n",
    "    # Same data split as GE: 80/10/10\n",
    "    TEST_SIZE: float = 0.1\n",
    "    VAL_SIZE: float = 0.111  # 10% of remaining 90% ≈ 10% of total\n",
    "    \n",
    "    # Optuna parameters (comparable to GE: pop_size=15, generations=10)\n",
    "    # GE evaluations ≈ 15 * 10 = 150 per run\n",
    "    OPTUNA_TRIALS: int = 150  # Match GE evaluations\n",
    "    OPTUNA_TIMEOUT: int = 300  # 5 minutes per run\n",
    "    USE_PRUNING: bool = True\n",
    "    \n",
    "    # 30 independent runs (same as GE)\n",
    "    N_INDEPENDENT_RUNS: int = 30\n",
    "    \n",
    "    # Parallel execution\n",
    "    N_PARALLEL_JOBS: int = -1  # -1 uses all available cores\n",
    "    \n",
    "    # TabNet parameters (same as GE)\n",
    "    TABNET_MAX_EPOCHS: int = 25\n",
    "    TABNET_PATIENCE: int = 6\n",
    "    TABNET_BATCH_SIZE: int = 256\n",
    "    TABNET_GAMMA: float = 1.3\n",
    "    \n",
    "    # SMOTE (same as GE)\n",
    "    USE_SMOTE: bool = True\n",
    "    \n",
    "    RESULTS_DIR: str = \"optuna_results_comparable\"\n",
    "    RANDOM_SEED: int = 42\n",
    "    VERBOSE: bool = False\n",
    "    \n",
    "    SAVE_PLOTS: bool = True\n",
    "    SHOW_PLOTS: bool = False\n",
    "    PLOT_DPI: int = 150\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TABNET OPTIMIZER WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "class TabNetOptunaOptimizer:\n",
    "    \"\"\"TabNet optimizer using Optuna with same search space as GE.\"\"\"\n",
    "    \n",
    "    # Same search space as GE code\n",
    "    SEARCH_SPACE = {\n",
    "        'n_d': [8, 16, 32, 64],\n",
    "        'n_a': [8, 16, 32, 64],\n",
    "        'n_steps': [3, 5, 7],\n",
    "        'lambda_sparse': [1e-4, 1e-3, 1e-2]\n",
    "    }\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                 config, feature_names=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.config = config\n",
    "        self.feature_names = feature_names or [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "    \n",
    "    def _create_objective(self, seed_offset=0):\n",
    "        \"\"\"Create Optuna objective function.\"\"\"\n",
    "        def objective(trial):\n",
    "            # Sample from same discrete search space as GE\n",
    "            params = {\n",
    "                'n_d': trial.suggest_categorical('n_d', self.SEARCH_SPACE['n_d']),\n",
    "                'n_a': trial.suggest_categorical('n_a', self.SEARCH_SPACE['n_a']),\n",
    "                'n_steps': trial.suggest_categorical('n_steps', self.SEARCH_SPACE['n_steps']),\n",
    "                'lambda_sparse': trial.suggest_categorical('lambda_sparse', self.SEARCH_SPACE['lambda_sparse']),\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                X_tr = self.X_train.copy()\n",
    "                y_tr = self.y_train.copy()\n",
    "                \n",
    "                # Apply SMOTE (same as GE)\n",
    "                if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "                    smote = SMOTE(random_state=self.config.RANDOM_SEED + seed_offset)\n",
    "                    X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
    "                \n",
    "                model = TabNetClassifier(\n",
    "                    n_d=params['n_d'],\n",
    "                    n_a=params['n_a'],\n",
    "                    n_steps=params['n_steps'],\n",
    "                    gamma=self.config.TABNET_GAMMA,\n",
    "                    lambda_sparse=params['lambda_sparse'],\n",
    "                    verbose=0,\n",
    "                    seed=self.config.RANDOM_SEED + seed_offset\n",
    "                )\n",
    "                \n",
    "                model.fit(\n",
    "                    X_tr, y_tr,\n",
    "                    eval_set=[(self.X_val, self.y_val)],\n",
    "                    eval_metric=['auc'],\n",
    "                    max_epochs=self.config.TABNET_MAX_EPOCHS,\n",
    "                    patience=self.config.TABNET_PATIENCE,\n",
    "                    batch_size=self.config.TABNET_BATCH_SIZE,\n",
    "                    drop_last=False\n",
    "                )\n",
    "                \n",
    "                # Fitness = ROC-AUC on validation (same as GE)\n",
    "                y_pred_proba = model.predict_proba(self.X_val)[:, 1]\n",
    "                return roc_auc_score(self.y_val, y_pred_proba)\n",
    "                \n",
    "            except Exception as e:\n",
    "                return 0.0\n",
    "        \n",
    "        return objective\n",
    "    \n",
    "    def evaluate_default_params(self):\n",
    "        \"\"\"Evaluate default hyperparameters (same as GE).\"\"\"\n",
    "        default_params = HyperparameterPruningAnalyzer.DEFAULT_HYPERPARAMS.get('TabNet', {})\n",
    "        \n",
    "        try:\n",
    "            X_tr = self.X_train.copy()\n",
    "            y_tr = self.y_train.copy()\n",
    "            \n",
    "            if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "                smote = SMOTE(random_state=self.config.RANDOM_SEED + 9999)\n",
    "                X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
    "            \n",
    "            model = TabNetClassifier(\n",
    "                n_d=default_params['n_d'],\n",
    "                n_a=default_params['n_a'],\n",
    "                n_steps=default_params['n_steps'],\n",
    "                gamma=default_params['gamma'],\n",
    "                lambda_sparse=default_params['lambda_sparse'],\n",
    "                verbose=0,\n",
    "                seed=self.config.RANDOM_SEED + 9999\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(self.X_val, self.y_val)],\n",
    "                eval_metric=['auc'],\n",
    "                max_epochs=self.config.TABNET_MAX_EPOCHS,\n",
    "                patience=self.config.TABNET_PATIENCE,\n",
    "                batch_size=self.config.TABNET_BATCH_SIZE,\n",
    "                drop_last=False\n",
    "            )\n",
    "            \n",
    "            y_pred_proba = model.predict_proba(self.X_val)[:, 1]\n",
    "            return roc_auc_score(self.y_val, y_pred_proba)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _run_single_optimization(self, run_idx):\n",
    "        \"\"\"Run a single Optuna optimization - for parallel execution.\"\"\"\n",
    "        seed = self.config.RANDOM_SEED + run_idx * 1000\n",
    "        \n",
    "        # Create study with TPE sampler\n",
    "        sampler = TPESampler(seed=seed, multivariate=True)\n",
    "        pruner = MedianPruner(n_startup_trials=10) if self.config.USE_PRUNING else optuna.pruners.NopPruner()\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=sampler,\n",
    "            pruner=pruner\n",
    "        )\n",
    "        \n",
    "        # Track optimization history\n",
    "        history = {'best_fitness': [], 'avg_fitness': []}\n",
    "        \n",
    "        def callback(study, trial):\n",
    "            completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "            if completed_trials:\n",
    "                values = [t.value for t in completed_trials]\n",
    "                history['best_fitness'].append(max(values))\n",
    "                history['avg_fitness'].append(np.mean(values))\n",
    "        \n",
    "        objective = self._create_objective(seed_offset=run_idx * 100)\n",
    "        \n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.config.OPTUNA_TRIALS,\n",
    "            timeout=self.config.OPTUNA_TIMEOUT,\n",
    "            callbacks=[callback],\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        best_config = study.best_params.copy()\n",
    "        best_config['gamma'] = self.config.TABNET_GAMMA\n",
    "        \n",
    "        return {\n",
    "            'run_idx': run_idx,\n",
    "            'best_config': best_config,\n",
    "            'best_fitness': study.best_value,\n",
    "            'n_trials': len(study.trials),\n",
    "            'runtime': sum(t.duration.total_seconds() for t in study.trials if t.duration),\n",
    "            'history': history\n",
    "        }\n",
    "    \n",
    "    def optimize_parallel(self, n_jobs=-1):\n",
    "        \"\"\"Run all independent optimizations in parallel.\"\"\"\n",
    "        print(f\"  Running {self.config.N_INDEPENDENT_RUNS} Optuna optimizations in parallel...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "            delayed(self._run_single_optimization)(run_idx)\n",
    "            for run_idx in range(self.config.N_INDEPENDENT_RUNS)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train_final_model(self, params, seed=42):\n",
    "        \"\"\"Train final model with best parameters.\"\"\"\n",
    "        X_train_full = np.vstack([self.X_train, self.X_val])\n",
    "        y_train_full = np.concatenate([self.y_train, self.y_val])\n",
    "        \n",
    "        if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "            smote = SMOTE(random_state=seed)\n",
    "            X_train_full, y_train_full = smote.fit_resample(X_train_full, y_train_full)\n",
    "        \n",
    "        model = TabNetClassifier(\n",
    "            n_d=params['n_d'],\n",
    "            n_a=params['n_a'],\n",
    "            n_steps=params['n_steps'],\n",
    "            gamma=params.get('gamma', self.config.TABNET_GAMMA),\n",
    "            lambda_sparse=params['lambda_sparse'],\n",
    "            verbose=0,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        X_tr, X_es, y_tr, y_es = train_test_split(\n",
    "            X_train_full, y_train_full, test_size=0.1,\n",
    "            stratify=y_train_full, random_state=seed\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_es, y_es)],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs=self.config.TABNET_MAX_EPOCHS,\n",
    "            patience=self.config.TABNET_PATIENCE,\n",
    "            batch_size=self.config.TABNET_BATCH_SIZE,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        y_train_pred = model.predict(X_tr)\n",
    "        y_train_proba = model.predict_proba(X_tr)[:, 1]\n",
    "        results['train_metrics'] = MetricsCalculator.compute_all_metrics(y_tr, y_train_pred, y_train_proba)\n",
    "        \n",
    "        y_test_pred = model.predict(self.X_test)\n",
    "        y_test_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "        results['test_metrics'] = MetricsCalculator.compute_all_metrics(self.y_test, y_test_pred, y_test_proba)\n",
    "        \n",
    "        results['train_val_gap'] = MetricsCalculator.compute_train_val_gap(\n",
    "            results['train_metrics'], results['test_metrics']\n",
    "        )\n",
    "        \n",
    "        results['feature_importance'] = self._get_feature_importance(model)\n",
    "        \n",
    "        return model, results\n",
    "    \n",
    "    def _get_feature_importance(self, model):\n",
    "        try:\n",
    "            importance = model.feature_importances_\n",
    "            importance_dict = {}\n",
    "            for i, imp in enumerate(importance):\n",
    "                importance_dict[self.feature_names[i]] = float(imp)\n",
    "            \n",
    "            sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return {\n",
    "                'raw_importance': importance.tolist(),\n",
    "                'feature_ranking': sorted_importance,\n",
    "                'top_10_features': sorted_importance[:10],\n",
    "                'feature_names': self.feature_names\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BASELINE OPTIMIZER WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "class BaselineOptunaOptimizer:\n",
    "    \"\"\"Baseline models optimizer using Optuna with same search spaces as GE.\"\"\"\n",
    "    \n",
    "    # Same search spaces as GE code\n",
    "    SEARCH_SPACES = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [5, 10, 15, None]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Same fixed params as GE code\n",
    "    FIXED_PARAMS = {\n",
    "        'RandomForest': {'max_features': 'sqrt', 'min_samples_split': 2},\n",
    "        'LogisticRegression': {'max_iter': 1000},\n",
    "        'SVM': {'gamma': 'scale'},\n",
    "        'GradientBoosting': {'subsample': 0.8},\n",
    "        'XGBoost': {'subsample': 0.8, 'colsample_bytree': 0.8},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                 config, feature_names=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.config = config\n",
    "        self.feature_names = feature_names or [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "    \n",
    "    def _create_model(self, model_type, params, seed):\n",
    "        full_params = {**self.FIXED_PARAMS.get(model_type, {}), **params}\n",
    "        \n",
    "        if model_type == 'RandomForest':\n",
    "            max_depth = full_params.get('max_depth')\n",
    "            if max_depth == 'None' or max_depth is None:\n",
    "                max_depth = None\n",
    "            return RandomForestClassifier(\n",
    "                n_estimators=full_params['n_estimators'],\n",
    "                max_depth=max_depth,\n",
    "                max_features=full_params.get('max_features', 'sqrt'),\n",
    "                random_state=seed,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'LogisticRegression':\n",
    "            penalty = full_params.get('penalty', 'l2')\n",
    "            solver = 'liblinear' if penalty == 'l1' else 'lbfgs'\n",
    "            return LogisticRegression(\n",
    "                C=full_params['C'],\n",
    "                penalty=penalty,\n",
    "                solver=solver,\n",
    "                max_iter=full_params.get('max_iter', 1000),\n",
    "                random_state=seed\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'SVM':\n",
    "            return SVC(\n",
    "                C=full_params['C'],\n",
    "                kernel=full_params['kernel'],\n",
    "                gamma=full_params.get('gamma', 'scale'),\n",
    "                probability=True,\n",
    "                random_state=seed\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'GradientBoosting':\n",
    "            return GradientBoostingClassifier(\n",
    "                n_estimators=full_params['n_estimators'],\n",
    "                learning_rate=full_params['learning_rate'],\n",
    "                max_depth=full_params['max_depth'],\n",
    "                subsample=full_params.get('subsample', 0.8),\n",
    "                random_state=seed\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'XGBoost':\n",
    "            if not XGBOOST_AVAILABLE:\n",
    "                raise ImportError(\"XGBoost not available\")\n",
    "            return XGBClassifier(\n",
    "                n_estimators=full_params['n_estimators'],\n",
    "                learning_rate=full_params['learning_rate'],\n",
    "                max_depth=full_params['max_depth'],\n",
    "                subsample=full_params.get('subsample', 0.8),\n",
    "                colsample_bytree=full_params.get('colsample_bytree', 0.8),\n",
    "                random_state=seed,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                verbosity=0\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    def _create_objective(self, model_type, seed_offset=0):\n",
    "        \"\"\"Create Optuna objective function for baseline model.\"\"\"\n",
    "        search_space = self.SEARCH_SPACES[model_type]\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Sample from same discrete search space as GE\n",
    "            params = {}\n",
    "            for param, choices in search_space.items():\n",
    "                params[param] = trial.suggest_categorical(param, choices)\n",
    "            \n",
    "            try:\n",
    "                seed = self.config.RANDOM_SEED + seed_offset\n",
    "                model = self._create_model(model_type, params, seed)\n",
    "                \n",
    "                X_tr = self.X_train.copy()\n",
    "                y_tr = self.y_train.copy()\n",
    "                \n",
    "                if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "                    smote = SMOTE(random_state=seed)\n",
    "                    X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
    "                \n",
    "                model.fit(X_tr, y_tr)\n",
    "                \n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_pred_proba = model.predict_proba(self.X_val)[:, 1]\n",
    "                    return roc_auc_score(self.y_val, y_pred_proba)\n",
    "                else:\n",
    "                    y_pred = model.predict(self.X_val)\n",
    "                    return accuracy_score(self.y_val, y_pred)\n",
    "                    \n",
    "            except Exception:\n",
    "                return 0.0\n",
    "        \n",
    "        return objective\n",
    "    \n",
    "    def evaluate_default_params(self, model_type):\n",
    "        \"\"\"Evaluate default hyperparameters.\"\"\"\n",
    "        default_params = HyperparameterPruningAnalyzer.DEFAULT_HYPERPARAMS.get(model_type, {})\n",
    "        search_params = {}\n",
    "        search_space = self.SEARCH_SPACES.get(model_type, {})\n",
    "        for param in search_space.keys():\n",
    "            if param in default_params:\n",
    "                search_params[param] = default_params[param]\n",
    "        \n",
    "        try:\n",
    "            seed = self.config.RANDOM_SEED + 9999\n",
    "            model = self._create_model(model_type, search_params, seed)\n",
    "            \n",
    "            X_tr = self.X_train.copy()\n",
    "            y_tr = self.y_train.copy()\n",
    "            \n",
    "            if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "                smote = SMOTE(random_state=seed)\n",
    "                X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
    "            \n",
    "            model.fit(X_tr, y_tr)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_proba = model.predict_proba(self.X_val)[:, 1]\n",
    "                return roc_auc_score(self.y_val, y_pred_proba)\n",
    "            else:\n",
    "                y_pred = model.predict(self.X_val)\n",
    "                return accuracy_score(self.y_val, y_pred)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _run_single_optimization(self, model_type, run_idx):\n",
    "        \"\"\"Run a single Optuna optimization - for parallel execution.\"\"\"\n",
    "        seed = self.config.RANDOM_SEED + run_idx * 1000\n",
    "        \n",
    "        sampler = TPESampler(seed=seed, multivariate=True)\n",
    "        pruner = MedianPruner(n_startup_trials=10) if self.config.USE_PRUNING else optuna.pruners.NopPruner()\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=sampler,\n",
    "            pruner=pruner\n",
    "        )\n",
    "        \n",
    "        history = {'best_fitness': [], 'avg_fitness': []}\n",
    "        \n",
    "        def callback(study, trial):\n",
    "            completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "            if completed_trials:\n",
    "                values = [t.value for t in completed_trials]\n",
    "                history['best_fitness'].append(max(values))\n",
    "                history['avg_fitness'].append(np.mean(values))\n",
    "        \n",
    "        objective = self._create_objective(model_type, seed_offset=run_idx * 100)\n",
    "        \n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.config.OPTUNA_TRIALS,\n",
    "            timeout=self.config.OPTUNA_TIMEOUT,\n",
    "            callbacks=[callback],\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        full_config = {**self.FIXED_PARAMS.get(model_type, {}), **study.best_params}\n",
    "        \n",
    "        return {\n",
    "            'run_idx': run_idx,\n",
    "            'best_config': full_config,\n",
    "            'best_fitness': study.best_value,\n",
    "            'n_trials': len(study.trials),\n",
    "            'runtime': sum(t.duration.total_seconds() for t in study.trials if t.duration),\n",
    "            'history': history\n",
    "        }\n",
    "    \n",
    "    def optimize_parallel(self, model_type, n_jobs=-1):\n",
    "        \"\"\"Run all independent optimizations in parallel.\"\"\"\n",
    "        print(f\"  Running {self.config.N_INDEPENDENT_RUNS} Optuna optimizations in parallel...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "            delayed(self._run_single_optimization)(model_type, run_idx)\n",
    "            for run_idx in range(self.config.N_INDEPENDENT_RUNS)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train_final_model(self, model_type, params, seed=42):\n",
    "        \"\"\"Train final model with best parameters.\"\"\"\n",
    "        X_train_full = np.vstack([self.X_train, self.X_val])\n",
    "        y_train_full = np.concatenate([self.y_train, self.y_val])\n",
    "        \n",
    "        if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "            smote = SMOTE(random_state=seed)\n",
    "            X_train_full, y_train_full = smote.fit_resample(X_train_full, y_train_full)\n",
    "        \n",
    "        model = self._create_model(model_type, params, seed)\n",
    "        model.fit(X_train_full, y_train_full)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        y_train_pred = model.predict(X_train_full)\n",
    "        y_train_proba = model.predict_proba(X_train_full)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        results['train_metrics'] = MetricsCalculator.compute_all_metrics(y_train_full, y_train_pred, y_train_proba)\n",
    "        \n",
    "        y_test_pred = model.predict(self.X_test)\n",
    "        y_test_proba = model.predict_proba(self.X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        results['test_metrics'] = MetricsCalculator.compute_all_metrics(self.y_test, y_test_pred, y_test_proba)\n",
    "        \n",
    "        results['train_test_gap'] = MetricsCalculator.compute_train_val_gap(\n",
    "            results['train_metrics'], results['test_metrics']\n",
    "        )\n",
    "        \n",
    "        results['feature_importance'] = self._get_feature_importance(model, model_type)\n",
    "        \n",
    "        return model, results\n",
    "    \n",
    "    def _get_feature_importance(self, model, model_type):\n",
    "        try:\n",
    "            importance = None\n",
    "            \n",
    "            if model_type in ['RandomForest', 'GradientBoosting', 'XGBoost']:\n",
    "                importance = model.feature_importances_\n",
    "            elif model_type == 'LogisticRegression':\n",
    "                importance = np.abs(model.coef_[0])\n",
    "            elif model_type == 'SVM':\n",
    "                if model.kernel == 'linear':\n",
    "                    importance = np.abs(model.coef_[0])\n",
    "                else:\n",
    "                    return {'note': 'Feature importance not available for non-linear SVM'}\n",
    "            \n",
    "            if importance is not None:\n",
    "                if importance.sum() > 0:\n",
    "                    importance = importance / importance.sum()\n",
    "                \n",
    "                importance_dict = {}\n",
    "                for i, imp in enumerate(importance):\n",
    "                    importance_dict[self.feature_names[i]] = float(imp)\n",
    "                \n",
    "                sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                return {\n",
    "                    'raw_importance': importance.tolist(),\n",
    "                    'feature_ranking': sorted_importance,\n",
    "                    'top_10_features': sorted_importance[:10]\n",
    "                }\n",
    "            \n",
    "            return {'note': 'Feature importance not available'}\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS PLOTTER (Same as GE code)\n",
    "# =============================================================================\n",
    "\n",
    "class ResultsPlotter:\n",
    "    \"\"\"Same visualization as GE code for fair comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, results, config, output_dir=None):\n",
    "        self.results = results\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir or config.RESULTS_DIR\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        self.colors = {\n",
    "            'TabNet': '#2ecc71',\n",
    "            'RandomForest': '#3498db',\n",
    "            'XGBoost': '#e74c3c',\n",
    "            'SVM': '#9b59b6',\n",
    "            'LogisticRegression': '#f39c12',\n",
    "            'GradientBoosting': '#1abc9c'\n",
    "        }\n",
    "    \n",
    "    def _get_color(self, name):\n",
    "        return self.colors.get(name, '#95a5a6')\n",
    "    \n",
    "    def plot_all(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"GENERATING PLOTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        plot_methods = [\n",
    "            ('Test performance comparison', self.plot_test_performance_comparison),\n",
    "            ('Sensitivity/Specificity', self.plot_sensitivity_specificity),\n",
    "            ('Train-Test gap', self.plot_train_test_gap),\n",
    "            ('Boxplot (runs)', self.plot_runs_boxplot),\n",
    "            ('Convergence curves', self.plot_convergence_summary),\n",
    "            ('Best vs Average fitness', self.plot_best_vs_average_fitness),\n",
    "            ('Feature importance (all models)', self.plot_all_feature_importance),\n",
    "            ('Feature importance heatmap', self.plot_feature_importance_heatmap),\n",
    "            ('HP pruning analysis', self.plot_hyperparameter_pruning),\n",
    "            ('HP count comparison', self.plot_hp_count_comparison),\n",
    "            ('Pruning tables', self.plot_pruning_comparison_table),\n",
    "        ]\n",
    "        \n",
    "        for idx, (name, method) in enumerate(plot_methods, 1):\n",
    "            print(f\"  [{idx}/{len(plot_methods)}] {name}...\", end=\" \", flush=True)\n",
    "            try:\n",
    "                method()\n",
    "                print(\"done\")\n",
    "            except Exception as e:\n",
    "                print(f\"failed ({e})\")\n",
    "                plt.close('all')\n",
    "        \n",
    "        print(f\"\\nAll plots saved to: {self.output_dir}\")\n",
    "    \n",
    "    def plot_test_performance_comparison(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        test_aucs = []\n",
    "        test_accs = []\n",
    "        test_f1s = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'best_run_test_metrics' in data:\n",
    "                model_names.append(name)\n",
    "                metrics = data['best_run_test_metrics']\n",
    "                test_aucs.append(metrics.get('roc_auc', 0))\n",
    "                test_accs.append(metrics.get('accuracy', 0))\n",
    "                test_f1s.append(metrics.get('f1', 0))\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle('Test Set Performance Comparison (Optuna)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        colors = [self._get_color(name) for name in model_names]\n",
    "        \n",
    "        for ax, values, title, ylabel in [\n",
    "            (axes[0], test_aucs, 'ROC-AUC', 'AUC'),\n",
    "            (axes[1], test_accs, 'Accuracy', 'Accuracy'),\n",
    "            (axes[2], test_f1s, 'F1 Score', 'F1')\n",
    "        ]:\n",
    "            bars = ax.bar(x, values, color=colors, edgecolor='black', linewidth=1)\n",
    "            for bar, val in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_title(title)\n",
    "            ax.set_ylim(0, 1.1)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'test_performance.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_sensitivity_specificity(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        sensitivities = []\n",
    "        specificities = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'best_run_test_metrics' in data:\n",
    "                model_names.append(name)\n",
    "                metrics = data['best_run_test_metrics']\n",
    "                sensitivities.append(metrics.get('sensitivity', 0))\n",
    "                specificities.append(metrics.get('specificity', 0))\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, sensitivities, width, label='Sensitivity (TPR)',\n",
    "                      color='#e74c3c', edgecolor='black')\n",
    "        bars2 = ax.bar(x + width/2, specificities, width, label='Specificity (TNR)',\n",
    "                      color='#3498db', edgecolor='black')\n",
    "        \n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title('Sensitivity vs Specificity (Test Set) - Optuna', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylim(0, 1.15)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'sensitivity_specificity.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_train_test_gap(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        train_aucs = []\n",
    "        test_aucs = []\n",
    "        gaps = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'best_run_train_metrics' in data and 'best_run_test_metrics' in data:\n",
    "                model_names.append(name)\n",
    "                train_auc = data['best_run_train_metrics'].get('roc_auc', 0)\n",
    "                test_auc = data['best_run_test_metrics'].get('roc_auc', 0)\n",
    "                train_aucs.append(train_auc)\n",
    "                test_aucs.append(test_auc)\n",
    "                gaps.append(train_auc - test_auc)\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        fig.suptitle('Train vs Test Performance (Optuna)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, train_aucs, width, label='Train AUC', color='#2ecc71', edgecolor='black')\n",
    "        axes[0].bar(x + width/2, test_aucs, width, label='Test AUC', color='#e74c3c', edgecolor='black')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[0].set_ylabel('AUC')\n",
    "        axes[0].set_title('Train vs Test AUC')\n",
    "        axes[0].set_ylim(0, 1.1)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        colors = ['#e74c3c' if g > 0.05 else '#f39c12' if g > 0.02 else '#2ecc71' for g in gaps]\n",
    "        bars = axes[1].bar(x, gaps, color=colors, edgecolor='black')\n",
    "        \n",
    "        for bar, gap in zip(bars, gaps):\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                        f'{gap:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        axes[1].axhline(y=0.05, color='red', linestyle='--', label='High overfit')\n",
    "        axes[1].axhline(y=0.02, color='orange', linestyle='--', label='Moderate overfit')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[1].set_ylabel('AUC Gap (Train - Test)')\n",
    "        axes[1].set_title('Overfitting Gap')\n",
    "        axes[1].legend(loc='upper right')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'train_test_gap.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_runs_boxplot(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        all_fitness = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'all_runs' in data:\n",
    "                valid_runs = [r['best_fitness'] for r in data['all_runs'] if 'error' not in r]\n",
    "                if valid_runs:\n",
    "                    model_names.append(name)\n",
    "                    all_fitness.append(valid_runs)\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        colors = [self._get_color(name) for name in model_names]\n",
    "        \n",
    "        bp = ax.boxplot(all_fitness, labels=model_names, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_ylabel('Best Fitness (Validation AUC)', fontsize=11)\n",
    "        ax.set_title(f'Distribution Across {self.config.N_INDEPENDENT_RUNS} Independent Runs (Optuna)',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'boxplot_runs.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_convergence_summary(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        has_data = False\n",
    "        for name, data in models_data.items():\n",
    "            if 'convergence_summary' in data:\n",
    "                conv = data['convergence_summary']\n",
    "                best_mean = conv.get('best_mean') or conv.get('mean', [])\n",
    "                best_std = conv.get('best_std') or conv.get('std', [])\n",
    "                if best_mean:\n",
    "                    has_data = True\n",
    "                    trials = range(len(best_mean))\n",
    "                    mean = np.array(best_mean)\n",
    "                    std = np.array(best_std) if best_std else np.zeros_like(mean)\n",
    "                    \n",
    "                    color = self._get_color(name)\n",
    "                    ax.plot(trials, mean, label=name, color=color, linewidth=2)\n",
    "                    ax.fill_between(trials, mean - std, mean + std, color=color, alpha=0.2)\n",
    "        \n",
    "        if not has_data:\n",
    "            plt.close()\n",
    "            return\n",
    "        \n",
    "        ax.set_xlabel('Trial', fontsize=11)\n",
    "        ax.set_ylabel('Best Fitness (AUC)', fontsize=11)\n",
    "        ax.set_title(f'Mean Convergence ({self.config.N_INDEPENDENT_RUNS} Runs) - Optuna',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'convergence.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_best_vs_average_fitness(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        n_models = len(models_data)\n",
    "        \n",
    "        if n_models == 0:\n",
    "            return\n",
    "        \n",
    "        n_cols = min(3, n_models)\n",
    "        n_rows = (n_models + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "        fig.suptitle('Best Individual vs Population Average Fitness (Optuna)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes = axes.flatten() if n_models > 1 else axes\n",
    "        \n",
    "        plot_idx = 0\n",
    "        for name, data in models_data.items():\n",
    "            if plot_idx >= len(axes):\n",
    "                break\n",
    "            \n",
    "            ax = axes[plot_idx] if n_models > 1 else axes[0]\n",
    "            color = self._get_color(name)\n",
    "            \n",
    "            conv = data.get('convergence_summary', {})\n",
    "            \n",
    "            best_mean = np.array(conv.get('best_mean', []))\n",
    "            best_std = np.array(conv.get('best_std', []))\n",
    "            avg_mean = np.array(conv.get('avg_mean', []))\n",
    "            avg_std = np.array(conv.get('avg_std', []))\n",
    "            \n",
    "            if len(best_mean) == 0:\n",
    "                ax.text(0.5, 0.5, 'No convergence data', ha='center', va='center',\n",
    "                       transform=ax.transAxes, fontsize=11)\n",
    "                ax.set_title(name, fontweight='bold')\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "            \n",
    "            trials = np.arange(len(best_mean))\n",
    "            \n",
    "            ax.plot(trials, best_mean, color=color, linewidth=2.5,\n",
    "                   label='Best Trial', marker='o', markersize=5,\n",
    "                   markevery=max(1, len(trials)//8))\n",
    "            if len(best_std) > 0:\n",
    "                ax.fill_between(trials, best_mean - best_std, best_mean + best_std,\n",
    "                              color=color, alpha=0.2)\n",
    "            \n",
    "            if len(avg_mean) > 0:\n",
    "                ax.plot(trials, avg_mean, color='#7f8c8d', linewidth=2,\n",
    "                       linestyle='--', label='Average Trial', marker='s',\n",
    "                       markersize=4, markevery=max(1, len(trials)//8))\n",
    "                if len(avg_std) > 0:\n",
    "                    ax.fill_between(trials, avg_mean - avg_std, avg_mean + avg_std,\n",
    "                                  color='#7f8c8d', alpha=0.15)\n",
    "            \n",
    "            final_best = best_mean[-1]\n",
    "            final_avg = avg_mean[-1] if len(avg_mean) > 0 else 0\n",
    "            gap = final_best - final_avg if len(avg_mean) > 0 else 0\n",
    "            \n",
    "            stats_text = f'Final Best: {final_best:.4f}'\n",
    "            if len(avg_mean) > 0:\n",
    "                stats_text += f'\\nFinal Avg: {final_avg:.4f}\\nGap: {gap:.4f}'\n",
    "            \n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "                   verticalalignment='top', family='monospace',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n",
    "                            alpha=0.9, edgecolor='gray'))\n",
    "            \n",
    "            ax.set_xlabel('Trial', fontsize=10)\n",
    "            ax.set_ylabel('Fitness (AUC)', fontsize=10)\n",
    "            ax.set_title(name, fontweight='bold', fontsize=12)\n",
    "            ax.legend(loc='lower right', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_ylim(0.5, 1.02)\n",
    "            \n",
    "            plot_idx += 1\n",
    "        \n",
    "        for idx in range(plot_idx, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'best_vs_average_fitness.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_all_feature_importance(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        models_with_importance = {}\n",
    "        for name, data in models_data.items():\n",
    "            fi = data.get('feature_importance', {})\n",
    "            if 'top_10_features' in fi and fi['top_10_features']:\n",
    "                models_with_importance[name] = fi['top_10_features']\n",
    "        \n",
    "        if not models_with_importance:\n",
    "            return\n",
    "        \n",
    "        n_models = len(models_with_importance)\n",
    "        n_cols = min(3, n_models)\n",
    "        n_rows = (n_models + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(7*n_cols, 5*n_rows))\n",
    "        fig.suptitle('Feature Importance by Model (Top 10 Features) - Optuna', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for idx, (model_name, top_features) in enumerate(models_with_importance.items()):\n",
    "            ax = axes_flat[idx]\n",
    "            \n",
    "            features = [f[0][:20] for f in top_features]\n",
    "            importances = [f[1] for f in top_features]\n",
    "            \n",
    "            y_pos = np.arange(len(features))\n",
    "            color = self._get_color(model_name)\n",
    "            bars = ax.barh(y_pos, importances, color=color, edgecolor='black', alpha=0.8)\n",
    "            \n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(features, fontsize=9)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel('Importance', fontsize=10)\n",
    "            ax.set_title(model_name, fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            for bar, imp in zip(bars, importances):\n",
    "                ax.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "                       f'{imp:.4f}', va='center', fontsize=8)\n",
    "        \n",
    "        for idx in range(n_models, len(axes_flat)):\n",
    "            axes_flat[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'feature_importance_all_models.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_feature_importance_heatmap(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        all_importance = {}\n",
    "        all_features = set()\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            fi = data.get('feature_importance', {})\n",
    "            if 'feature_ranking' in fi:\n",
    "                all_importance[name] = dict(fi['feature_ranking'])\n",
    "                all_features.update(all_importance[name].keys())\n",
    "            elif 'top_10_features' in fi:\n",
    "                all_importance[name] = dict(fi['top_10_features'])\n",
    "                all_features.update(all_importance[name].keys())\n",
    "        \n",
    "        if not all_importance or len(all_importance) < 2:\n",
    "            return\n",
    "        \n",
    "        avg_importance = {}\n",
    "        for feat in all_features:\n",
    "            values = [all_importance[m].get(feat, 0) for m in all_importance.keys()]\n",
    "            avg_importance[feat] = np.mean(values)\n",
    "        \n",
    "        top_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        top_feature_names = [f[0] for f in top_features]\n",
    "        \n",
    "        model_names = list(all_importance.keys())\n",
    "        heatmap_data = np.zeros((len(top_feature_names), len(model_names)))\n",
    "        \n",
    "        for j, model in enumerate(model_names):\n",
    "            for i, feat in enumerate(top_feature_names):\n",
    "                heatmap_data[i, j] = all_importance[model].get(feat, 0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        im = ax.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')\n",
    "        \n",
    "        ax.set_xticks(np.arange(len(model_names)))\n",
    "        ax.set_yticks(np.arange(len(top_feature_names)))\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_yticklabels([f[:25] for f in top_feature_names], fontsize=10)\n",
    "        \n",
    "        for i in range(len(top_feature_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                val = heatmap_data[i, j]\n",
    "                color = 'white' if val > heatmap_data.max() * 0.5 else 'black'\n",
    "                ax.text(j, i, f'{val:.3f}', ha='center', va='center',\n",
    "                       color=color, fontsize=8, fontweight='bold')\n",
    "        \n",
    "        ax.set_title('Comparative Feature Importance Across Models (Optuna)', fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(im, ax=ax, shrink=0.8, label='Importance')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'feature_importance_heatmap.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_hyperparameter_pruning(self):\n",
    "        pruning_data = self.results.get('hyperparameter_pruning', {})\n",
    "        \n",
    "        if not pruning_data:\n",
    "            return\n",
    "        \n",
    "        model_names = list(pruning_data.keys())\n",
    "        pre_prune_combos = []\n",
    "        post_prune_combos = []\n",
    "        reduction_pct = []\n",
    "        default_aucs = []\n",
    "        optimized_aucs = []\n",
    "        \n",
    "        for name in model_names:\n",
    "            data = pruning_data[name]\n",
    "            pre_prune_combos.append(data['pre_prune']['total_combinations'])\n",
    "            post_prune_combos.append(data['post_prune']['effective_combinations'])\n",
    "            reduction_pct.append(data['reduction_metrics']['space_reduction_percent'])\n",
    "            default_aucs.append(data['fitness_improvement'].get('default', 0))\n",
    "            optimized_aucs.append(data['fitness_improvement'].get('optimized', 0))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('Optuna Hyperparameter Pruning Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1 = axes[0, 0]\n",
    "        bars1 = ax1.bar(x - width/2, pre_prune_combos, width, label='Pre-Prune',\n",
    "                       color='#e74c3c', edgecolor='black')\n",
    "        bars2 = ax1.bar(x + width/2, post_prune_combos, width, label='Post-Prune',\n",
    "                       color='#2ecc71', edgecolor='black')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax1.set_ylabel('Number of Combinations')\n",
    "        ax1.set_title('Search Space Size (log scale)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        ax2 = axes[0, 1]\n",
    "        colors = [self._get_color(name) for name in model_names]\n",
    "        bars = ax2.bar(x, reduction_pct, color=colors, edgecolor='black')\n",
    "        for bar, pct in zip(bars, reduction_pct):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Reduction (%)')\n",
    "        ax2.set_title('Search Space Reduction')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        ax2.set_ylim(0, 100)\n",
    "        \n",
    "        ax3 = axes[1, 0]\n",
    "        valid_idx = [i for i, (d, o) in enumerate(zip(default_aucs, optimized_aucs))\n",
    "                    if d and o and d > 0 and o > 0]\n",
    "        if valid_idx:\n",
    "            valid_models = [model_names[i] for i in valid_idx]\n",
    "            valid_default = [default_aucs[i] for i in valid_idx]\n",
    "            valid_optimized = [optimized_aucs[i] for i in valid_idx]\n",
    "            \n",
    "            x_valid = np.arange(len(valid_models))\n",
    "            ax3.bar(x_valid - width/2, valid_default, width, label='Default HP',\n",
    "                   color='#95a5a6', edgecolor='black')\n",
    "            ax3.bar(x_valid + width/2, valid_optimized, width, label='Optuna-Optimized',\n",
    "                   color='#2ecc71', edgecolor='black')\n",
    "            ax3.set_xticks(x_valid)\n",
    "            ax3.set_xticklabels(valid_models, rotation=45, ha='right')\n",
    "            ax3.set_ylabel('Validation AUC')\n",
    "            ax3.set_title('Default vs Optimized Performance')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3, axis='y')\n",
    "            ax3.set_ylim(0.5, 1.05)\n",
    "        \n",
    "        ax4 = axes[1, 1]\n",
    "        if valid_idx:\n",
    "            gains = [(optimized_aucs[i] - default_aucs[i]) * 100 / default_aucs[i]\n",
    "                    for i in valid_idx]\n",
    "            colors_gain = ['#2ecc71' if g > 0 else '#e74c3c' for g in gains]\n",
    "            bars = ax4.bar(x_valid, gains, color=colors_gain, edgecolor='black')\n",
    "            for bar, g in zip(bars, gains):\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2,\n",
    "                        bar.get_height() + (0.1 if g > 0 else -0.3),\n",
    "                        f'{g:.2f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "            ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            ax4.set_xticks(x_valid)\n",
    "            ax4.set_xticklabels(valid_models, rotation=45, ha='right')\n",
    "            ax4.set_ylabel('AUC Improvement (%)')\n",
    "            ax4.set_title('Performance Gain from Optuna Optimization')\n",
    "            ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'hyperparameter_pruning.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_hp_count_comparison(self):\n",
    "        pruning_data = self.results.get('hyperparameter_pruning', {})\n",
    "        \n",
    "        if not pruning_data:\n",
    "            return\n",
    "        \n",
    "        model_names = list(pruning_data.keys())\n",
    "        pre_hp_count = []\n",
    "        post_hp_count = []\n",
    "        \n",
    "        for name in model_names:\n",
    "            data = pruning_data[name]\n",
    "            pre_hp_count.append(data['pre_prune']['n_hyperparameters'])\n",
    "            post_hp_count.append(data['post_prune']['n_selected_hyperparameters'])\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, pre_hp_count, width, label='Pre-Prune (Full Search Space)',\n",
    "                      color='#e74c3c', edgecolor='black')\n",
    "        bars2 = ax.bar(x + width/2, post_hp_count, width, label='Post-Prune (Selected by Optuna)',\n",
    "                      color='#2ecc71', edgecolor='black')\n",
    "        \n",
    "        for bar in bars1:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                   f'{int(bar.get_height())}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        for bar in bars2:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                   f'{int(bar.get_height())}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        for i, (pre, post) in enumerate(zip(pre_hp_count, post_hp_count)):\n",
    "            reduction = pre - post\n",
    "            if reduction > 0:\n",
    "                ax.annotate(f'-{reduction}', xy=(i, max(pre, post) + 0.5),\n",
    "                           ha='center', fontsize=10, color='darkred', fontweight='bold')\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_ylabel('Number of Hyperparameters', fontsize=12)\n",
    "        ax.set_title('Pre-Prune vs Post-Prune Hyperparameter Count (Optuna)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='upper right', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_ylim(0, max(pre_hp_count) + 2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'hp_count_comparison.png'),\n",
    "                   dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_pruning_comparison_table(self):\n",
    "        pruning_data = self.results.get('hyperparameter_pruning', {})\n",
    "        \n",
    "        if not pruning_data:\n",
    "            return\n",
    "        \n",
    "        for model_name, data in pruning_data.items():\n",
    "            param_changes = data.get('param_changes', {})\n",
    "            if not param_changes:\n",
    "                continue\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, max(3, len(param_changes) * 0.8)))\n",
    "            ax.axis('off')\n",
    "            \n",
    "            table_data = []\n",
    "            for param, change in param_changes.items():\n",
    "                default_val = str(change['default'])\n",
    "                optimized_val = str(change['optimized'])\n",
    "                changed = 'Yes' if change['changed'] else 'No'\n",
    "                table_data.append([param, default_val, optimized_val, changed])\n",
    "            \n",
    "            metrics = data.get('reduction_metrics', {})\n",
    "            fitness = data.get('fitness_improvement', {})\n",
    "            \n",
    "            table = ax.table(\n",
    "                cellText=table_data,\n",
    "                colLabels=['Parameter', 'Pre-Prune (Default)', 'Post-Prune (Optuna)', 'Changed'],\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.3, 0.25, 0.25, 0.2]\n",
    "            )\n",
    "            \n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(10)\n",
    "            table.scale(1.2, 1.5)\n",
    "            \n",
    "            for i in range(4):\n",
    "                table[(0, i)].set_facecolor('#3498db')\n",
    "                table[(0, i)].set_text_props(color='white', fontweight='bold')\n",
    "            \n",
    "            for row_idx, row in enumerate(table_data):\n",
    "                if row[3] == 'Yes':\n",
    "                    table[(row_idx + 1, 2)].set_facecolor('#d5f5e3')\n",
    "                    table[(row_idx + 1, 3)].set_facecolor('#d5f5e3')\n",
    "            \n",
    "            title = f'{model_name}: Hyperparameter Changes by Optuna'\n",
    "            summary = f\"Combinations: {metrics.get('original_combinations', 'N/A')} -> {metrics.get('effective_combinations', 'N/A')} \"\n",
    "            summary += f\"(Reduction: {metrics.get('space_reduction_percent', 0):.1f}%)\"\n",
    "            \n",
    "            if fitness.get('relative_gain_percent'):\n",
    "                summary += f\" | AUC Gain: {fitness['relative_gain_percent']:.2f}%\"\n",
    "            \n",
    "            ax.set_title(title, fontsize=12, fontweight='bold', pad=20)\n",
    "            fig.text(0.5, 0.02, summary, ha='center', fontsize=10, style='italic')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, f'pruning_table_{model_name}.png'),\n",
    "                       dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXPERIMENT CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class OptunaExperiment:\n",
    "    \"\"\"Optuna experiment matching GE experimental setup.\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or Config()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_names = None\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.results = {}\n",
    "        self.pruning_analyzer = HyperparameterPruningAnalyzer()\n",
    "        \n",
    "        np.random.seed(self.config.RANDOM_SEED)\n",
    "        random.seed(self.config.RANDOM_SEED)\n",
    "        if TABNET_AVAILABLE:\n",
    "            torch.manual_seed(self.config.RANDOM_SEED)\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"LOADING DATA\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        df = pd.read_csv(self.config.DATA_CSV)\n",
    "        print(f\"  Loaded: {df.shape[0]} samples, {df.shape[1]} columns\")\n",
    "        \n",
    "        unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
    "        if unnamed_cols:\n",
    "            df = df.drop(columns=unnamed_cols)\n",
    "        \n",
    "        id_cols = [col for col in df.columns if str(col).lower() == 'id']\n",
    "        if id_cols:\n",
    "            df = df.drop(columns=id_cols)\n",
    "        \n",
    "        target_col = self.config.TARGET_COLUMN\n",
    "        if target_col not in df.columns:\n",
    "            target_col = df.columns[-1]\n",
    "        \n",
    "        print(f\"  Target column: {target_col}\")\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object':\n",
    "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
    "        \n",
    "        if X.isnull().any().any():\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "        \n",
    "        if y.dtype == 'object':\n",
    "            y = self.label_encoder.fit_transform(y)\n",
    "        else:\n",
    "            y = y.values\n",
    "        \n",
    "        X = X.values.astype(np.float32)\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        print(f\"  Features: {X.shape[1]}\")\n",
    "        print(f\"  Class distribution: {np.bincount(y)}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def prepare_splits(self, X, y):\n",
    "        \"\"\"Same 80/10/10 split as GE code.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DATA SPLITS (80/10/10)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.config.TEST_SIZE, stratify=y,\n",
    "            random_state=self.config.RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=self.config.VAL_SIZE, stratify=y_temp,\n",
    "            random_state=self.config.RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        total = len(y)\n",
    "        print(f\"  Train: {len(self.y_train)} samples ({100*len(self.y_train)/total:.1f}%)\")\n",
    "        print(f\"  Val:   {len(self.y_val)} samples ({100*len(self.y_val)/total:.1f}%)\")\n",
    "        print(f\"  Test:  {len(self.y_test)} samples ({100*len(self.y_test)/total:.1f}%)\")\n",
    "        \n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "    \n",
    "    def _process_convergence_data(self, all_convergence):\n",
    "        if not all_convergence:\n",
    "            return {\n",
    "                'best_mean': [], 'best_std': [],\n",
    "                'avg_mean': [], 'avg_std': [],\n",
    "                'mean': [], 'std': []\n",
    "            }\n",
    "        \n",
    "        best_histories = [c['best_fitness'] for c in all_convergence]\n",
    "        avg_histories = [c['avg_fitness'] for c in all_convergence]\n",
    "        \n",
    "        max_len = max(len(h) for h in best_histories)\n",
    "        \n",
    "        padded_best = []\n",
    "        padded_avg = []\n",
    "        for best, avg in zip(best_histories, avg_histories):\n",
    "            if len(best) < max_len:\n",
    "                best = best + [best[-1]] * (max_len - len(best))\n",
    "            if len(avg) < max_len:\n",
    "                avg = avg + [avg[-1]] * (max_len - len(avg))\n",
    "            padded_best.append(best)\n",
    "            padded_avg.append(avg)\n",
    "        \n",
    "        best_array = np.array(padded_best)\n",
    "        avg_array = np.array(padded_avg)\n",
    "        \n",
    "        return {\n",
    "            'best_mean': best_array.mean(axis=0).tolist(),\n",
    "            'best_std': best_array.std(axis=0).tolist(),\n",
    "            'avg_mean': avg_array.mean(axis=0).tolist(),\n",
    "            'avg_std': avg_array.std(axis=0).tolist(),\n",
    "            'mean': best_array.mean(axis=0).tolist(),\n",
    "            'std': best_array.std(axis=0).tolist()\n",
    "        }\n",
    "    \n",
    "    def run(self):\n",
    "        experiment_start = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"OPTUNA-BASED HYPERPARAMETER OPTIMIZATION\")\n",
    "        print(\"(Comparable to GE Version)\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"  Optuna Trials:      {self.config.OPTUNA_TRIALS}\")\n",
    "        print(f\"  Independent Runs:   {self.config.N_INDEPENDENT_RUNS}\")\n",
    "        print(f\"  Parallel Jobs:      {self.config.N_PARALLEL_JOBS}\")\n",
    "        print(f\"  SMOTE Enabled:      {self.config.USE_SMOTE}\")\n",
    "        print(f\"  Validation:         Holdout (no CV)\")\n",
    "        print(f\"  Data Split:         80% train / 10% val / 10% test\")\n",
    "        print(f\"  Fitness Function:   ROC-AUC (Validation)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        X, y = self.load_data()\n",
    "        self.prepare_splits(X, y)\n",
    "        \n",
    "        results = {\n",
    "            'config': {\n",
    "                'optimizer': 'Optuna',\n",
    "                'n_independent_runs': self.config.N_INDEPENDENT_RUNS,\n",
    "                'optuna_trials': self.config.OPTUNA_TRIALS,\n",
    "                'validation': 'holdout',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            },\n",
    "            'models': {},\n",
    "            'statistical_tests': {},\n",
    "            'hyperparameter_pruning': {}\n",
    "        }\n",
    "        \n",
    "        all_model_fitness = {}\n",
    "        \n",
    "        # TabNet\n",
    "        if TABNET_AVAILABLE:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"TabNet - {self.config.N_INDEPENDENT_RUNS} Independent Runs (Parallel)\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            tabnet_opt = TabNetOptunaOptimizer(\n",
    "                self.X_train, self.y_train, self.X_val, self.y_val,\n",
    "                self.X_test, self.y_test, self.config, self.feature_names\n",
    "            )\n",
    "            \n",
    "            print(\"\\n  Evaluating default hyperparameters...\")\n",
    "            default_fitness = tabnet_opt.evaluate_default_params()\n",
    "            print(f\"    Default AUC: {default_fitness:.4f}\")\n",
    "            \n",
    "            run_start = time.time()\n",
    "            parallel_results = tabnet_opt.optimize_parallel(n_jobs=self.config.N_PARALLEL_JOBS)\n",
    "            parallel_time = time.time() - run_start\n",
    "            \n",
    "            all_runs = []\n",
    "            all_convergence = []\n",
    "            all_configs = []\n",
    "            \n",
    "            for result in parallel_results:\n",
    "                if 'error' not in result:\n",
    "                    all_runs.append(result)\n",
    "                    all_convergence.append(result['history'])\n",
    "                    all_configs.append(result['best_config'])\n",
    "                else:\n",
    "                    all_runs.append(result)\n",
    "            \n",
    "            print(f\"\\n  Completed {len(all_configs)} runs in {parallel_time:.1f}s\")\n",
    "            \n",
    "            fitness_values = np.array([r['best_fitness'] for r in all_runs if 'error' not in r])\n",
    "            all_model_fitness['TabNet'] = fitness_values\n",
    "            \n",
    "            valid_runs = [r for r in all_runs if 'error' not in r]\n",
    "            if valid_runs:\n",
    "                best_run_idx = np.argmax([r['best_fitness'] for r in valid_runs])\n",
    "                best_run = valid_runs[best_run_idx]\n",
    "                \n",
    "                print(f\"  Best validation AUC: {best_run['best_fitness']:.4f}\")\n",
    "                print(\"\\n  Training final model...\")\n",
    "                _, final_eval = tabnet_opt.train_final_model(best_run['best_config'],\n",
    "                                                             seed=self.config.RANDOM_SEED)\n",
    "                \n",
    "                convergence_summary = self._process_convergence_data(all_convergence)\n",
    "                \n",
    "                pruning_result = self.pruning_analyzer.analyze_model(\n",
    "                    'TabNet', all_configs, best_run['best_config'],\n",
    "                    default_fitness=default_fitness, optimized_fitness=best_run['best_fitness']\n",
    "                )\n",
    "                results['hyperparameter_pruning']['TabNet'] = pruning_result\n",
    "                \n",
    "                results['models']['TabNet'] = {\n",
    "                    'all_runs': all_runs,\n",
    "                    'runs_summary': {\n",
    "                        'best_fitness': compute_statistics(fitness_values),\n",
    "                        'n_successful_runs': len(fitness_values)\n",
    "                    },\n",
    "                    'convergence_summary': convergence_summary,\n",
    "                    'best_run': best_run,\n",
    "                    'best_run_train_metrics': final_eval['train_metrics'],\n",
    "                    'best_run_test_metrics': final_eval['test_metrics'],\n",
    "                    'feature_importance': final_eval['feature_importance'],\n",
    "                    'default_fitness': default_fitness\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n  TabNet: Test AUC={final_eval['test_metrics']['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Baseline models\n",
    "        baseline_models = ['RandomForest', 'LogisticRegression', 'SVM', 'GradientBoosting']\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            baseline_models.append('XGBoost')\n",
    "        \n",
    "        baseline_opt = BaselineOptunaOptimizer(\n",
    "            self.X_train, self.y_train, self.X_val, self.y_val,\n",
    "            self.X_test, self.y_test, self.config, self.feature_names\n",
    "        )\n",
    "        \n",
    "        for model_name in baseline_models:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"{model_name} - {self.config.N_INDEPENDENT_RUNS} Independent Runs (Parallel)\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            print(\"\\n  Evaluating default hyperparameters...\")\n",
    "            default_fitness = baseline_opt.evaluate_default_params(model_name)\n",
    "            print(f\"    Default AUC: {default_fitness:.4f}\")\n",
    "            \n",
    "            run_start = time.time()\n",
    "            parallel_results = baseline_opt.optimize_parallel(model_name, n_jobs=self.config.N_PARALLEL_JOBS)\n",
    "            parallel_time = time.time() - run_start\n",
    "            \n",
    "            all_runs = []\n",
    "            all_convergence = []\n",
    "            all_configs = []\n",
    "            \n",
    "            for result in parallel_results:\n",
    "                if 'error' not in result:\n",
    "                    all_runs.append(result)\n",
    "                    all_convergence.append(result['history'])\n",
    "                    all_configs.append(result['best_config'])\n",
    "                else:\n",
    "                    all_runs.append(result)\n",
    "            \n",
    "            print(f\"\\n  Completed {len(all_configs)} runs in {parallel_time:.1f}s\")\n",
    "            \n",
    "            fitness_values = np.array([r['best_fitness'] for r in all_runs if 'error' not in r])\n",
    "            all_model_fitness[model_name] = fitness_values\n",
    "            \n",
    "            valid_runs = [r for r in all_runs if 'error' not in r]\n",
    "            if valid_runs:\n",
    "                best_run_idx = np.argmax([r['best_fitness'] for r in valid_runs])\n",
    "                best_run = valid_runs[best_run_idx]\n",
    "                \n",
    "                print(f\"  Best validation AUC: {best_run['best_fitness']:.4f}\")\n",
    "                print(f\"\\n  Training final {model_name} model...\")\n",
    "                _, final_eval = baseline_opt.train_final_model(model_name, best_run['best_config'],\n",
    "                                                               seed=self.config.RANDOM_SEED)\n",
    "                \n",
    "                pruning_result = self.pruning_analyzer.analyze_model(\n",
    "                    model_name, all_configs, best_run['best_config'],\n",
    "                    default_fitness=default_fitness, optimized_fitness=best_run['best_fitness']\n",
    "                )\n",
    "                results['hyperparameter_pruning'][model_name] = pruning_result\n",
    "            else:\n",
    "                best_run = None\n",
    "                final_eval = {'train_metrics': {}, 'test_metrics': {}, 'feature_importance': {}}\n",
    "            \n",
    "            convergence_summary = self._process_convergence_data(all_convergence)\n",
    "            \n",
    "            results['models'][model_name] = {\n",
    "                'all_runs': all_runs,\n",
    "                'runs_summary': {\n",
    "                    'best_fitness': compute_statistics(fitness_values) if len(fitness_values) > 0 else {},\n",
    "                    'n_successful_runs': len(fitness_values)\n",
    "                },\n",
    "                'convergence_summary': convergence_summary,\n",
    "                'best_run': best_run,\n",
    "                'best_run_train_metrics': final_eval.get('train_metrics', {}),\n",
    "                'best_run_test_metrics': final_eval.get('test_metrics', {}),\n",
    "                'feature_importance': final_eval.get('feature_importance', {}),\n",
    "                'default_fitness': default_fitness\n",
    "            }\n",
    "            \n",
    "            if final_eval.get('test_metrics'):\n",
    "                print(f\"  {model_name}: Test AUC={final_eval['test_metrics'].get('roc_auc', 0):.4f}\")\n",
    "        \n",
    "        # Pruning summary\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"HYPERPARAMETER PRUNING SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        pruning_summary = self.pruning_analyzer.get_summary_table()\n",
    "        print(pruning_summary.to_string(index=False))\n",
    "        \n",
    "        # Statistical tests\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STATISTICAL TESTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        model_names = list(all_model_fitness.keys())\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                m1, m2 = model_names[i], model_names[j]\n",
    "                scores1, scores2 = all_model_fitness[m1], all_model_fitness[m2]\n",
    "                \n",
    "                if len(scores1) == len(scores2) and len(scores1) > 0:\n",
    "                    w_stat, w_p = wilcoxon_test(scores1, scores2)\n",
    "                    \n",
    "                    results['statistical_tests'][f'{m1}_vs_{m2}'] = {\n",
    "                        'wilcoxon_statistic': w_stat,\n",
    "                        'wilcoxon_p': w_p,\n",
    "                        'mean_diff': float(scores1.mean() - scores2.mean())\n",
    "                    }\n",
    "                    \n",
    "                    sig = \"***\" if w_p < 0.001 else \"**\" if w_p < 0.01 else \"*\" if w_p < 0.05 else \"\"\n",
    "                    print(f\"  {m1} vs {m2}: p={w_p:.4f} {sig}\")\n",
    "        \n",
    "        total_time = time.time() - experiment_start\n",
    "        results['total_runtime_seconds'] = total_time\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TEST SET RESULTS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Model':<20} {'AUC':>8} {'Acc':>8} {'Sens':>8} {'Spec':>8} {'F1':>8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for name, data in results['models'].items():\n",
    "            if 'best_run_test_metrics' in data and data['best_run_test_metrics']:\n",
    "                m = data['best_run_test_metrics']\n",
    "                print(f\"{name:<20} {m.get('roc_auc', 0):>8.4f} {m.get('accuracy', 0):>8.4f} \"\n",
    "                      f\"{m.get('sensitivity', 0):>8.4f} {m.get('specificity', 0):>8.4f} \"\n",
    "                      f\"{m.get('f1', 0):>8.4f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Runtime: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "        \n",
    "        # Save results\n",
    "        os.makedirs(self.config.RESULTS_DIR, exist_ok=True)\n",
    "        results_file = os.path.join(\n",
    "            self.config.RESULTS_DIR,\n",
    "            f\"optuna_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        )\n",
    "        \n",
    "        def convert(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=convert)\n",
    "        print(f\"\\nResults saved: {results_file}\")\n",
    "        \n",
    "        self.results = results\n",
    "        \n",
    "        # Generate plots\n",
    "        plotter = ResultsPlotter(results, self.config)\n",
    "        plotter.plot_all()\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def run_experiment():\n",
    "    config = Config()\n",
    "    experiment = OptunaExperiment(config)\n",
    "    return experiment.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Optuna-based Hyperparameter Optimization\")\n",
    "    print(\"(Comparable to GE Version)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration Summary:\")\n",
    "    print(f\"  Optuna Trials:      {Config.OPTUNA_TRIALS}\")\n",
    "    print(f\"  Independent Runs:   {Config.N_INDEPENDENT_RUNS}\")\n",
    "    print(f\"  Parallel Jobs:      {Config.N_PARALLEL_JOBS} (-1 = all cores)\")\n",
    "    print(f\"  Validation:         Holdout (no CV)\")\n",
    "    print(f\"  SMOTE:              {Config.USE_SMOTE}\")\n",
    "    print(f\"  TabNet Max Epochs:  {Config.TABNET_MAX_EPOCHS}\")\n",
    "    print(f\"  Models:             6 (TabNet + 5 baselines)\")\n",
    "    print(f\"  Fitness Function:   ROC-AUC (Validation)\")\n",
    "    print(\"\\nKey Features for Fair Comparison with GE:\")\n",
    "    print(\"  - SAME search spaces (discrete categorical)\")\n",
    "    print(\"  - SAME data split (80/10/10 holdout)\")\n",
    "    print(\"  - SAME fitness function (ROC-AUC on validation)\")\n",
    "    print(\"  - SAME 30 independent runs\")\n",
    "    print(\"  - SAME SMOTE for class imbalance\")\n",
    "    print(\"  - SAME visualizations and metrics\")\n",
    "    print(\"  - SAME statistical tests (Wilcoxon)\")\n",
    "    print(\"\\nEstimated Runtime: 10-20 minutes (with parallel execution)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = run_experiment()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
