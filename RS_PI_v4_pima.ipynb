{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf32413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n# ===========================================================================\n#\n#  GRAMMATICAL EVOLUTION: INITIALIZATION METHODS COMPARISON (v4 \u2014 ALL IMPROVEMENTS)\n#  3 Methods: Random, Sensible (RHH), PI Grow\n#  4 Benchmarks: Keijzer-6, Nguyen-7, Pagie-1, PIMA Diabetes (Binary Classification)\n\n#  Reference: BDS Group GRAPE (de Lima et al., 2022, Signals 3(3))\n#  Ryan, C. & Azad, R.M.A. (2003) \"Sensible Initialisation in GE\"\n# ===========================================================================\n\nimport sys, os, importlib.util, numpy as np, random, copy, tempfile\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom scipy.stats import mannwhitneyu, kruskal\nfrom sklearn.datasets import fetch_openml          # PIMA Indian Diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings, time, json\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n# \u2500\u2500 Force-load LOCAL grape.py \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n_DIR = os.getcwd()\n_LOCAL = os.path.join(_DIR, 'grape.py')\nif os.path.isfile(_LOCAL):\n    spec = importlib.util.spec_from_file_location(\"grape\", _LOCAL)\n    grape = importlib.util.module_from_spec(spec)\n    sys.modules['grape'] = grape\n    spec.loader.exec_module(grape)\n    print(f\"[OK] Loaded grape.py from {_LOCAL}\")\nelse:\n    raise FileNotFoundError(f\"grape.py not found in {_DIR}\\nRun notebook inside GRAPE folder.\")\n\nif not hasattr(grape, 'PI_Grow'):\n    raise AttributeError(\"PI_Grow not found in grape.py!\")\nprint(\"[OK] PI_Grow available\")\n\nif _DIR not in sys.path:\n    sys.path.insert(0, _DIR)\ntry:\n    from functions import pdiv, plog, psqrt\n    print(\"[OK] functions.py loaded\")\nexcept ImportError:\n    def pdiv(a, b):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            return np.where(np.abs(b) < 1e-12, np.ones_like(a, dtype=float), a / b)\n    def plog(a): return np.log(1.0 + np.abs(a))\n    def psqrt(a): return np.sqrt(np.abs(a))\n    print(\"[WARN] functions.py not found \u2014 using protected fallbacks\")\n\n# \u2500\u2500 Sigmoid for classification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef sigmoid(a):\n    a = np.clip(np.asarray(a, dtype=float), -500, 500)\n    return 1.0 / (1.0 + np.exp(-a))\n\n# \u2500\u2500 Protected conditional for classification grammar \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# if_gt(a,b,c,d): returns c where a>b, else d\n# Enables GE to evolve piecewise decision rules\n# Ref: Kishore et al. (2000) IEEE TEC 4(3)\ndef if_gt(a, b, c, d):\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n    c = np.asarray(c, dtype=float)\n    d = np.asarray(d, dtype=float)\n    return np.where(a > b, c, d)\n\n# \u2500\u2500 Linear scaling (Keijzer 2003, Murphy et al. 2023) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Fits y \u2248 a*pred + b by least squares, returns scaled prediction.\n# For classification: scale raw output before sigmoid so GE only\n# needs to find the correct shape, not exact coefficients.\ndef linear_scale(pred, y):\n    \"\"\"Return (a*pred + b) via OLS, or pred unchanged if degenerate.\"\"\"\n    pred = np.asarray(pred, dtype=float)\n    y    = np.asarray(y, dtype=float)\n    if pred.ndim == 0 or len(pred) < 2:\n        return pred\n    var_p = np.var(pred)\n    if var_p < 1e-30:\n        return pred  # constant prediction, scaling won't help\n    cov_py = np.mean((pred - np.mean(pred)) * (y - np.mean(y)))\n    a = cov_py / var_p\n    b = np.mean(y) - a * np.mean(pred)\n    return a * pred + b\n\n# \u2500\u2500 DEAP setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrom deap import creator, base, tools\nif \"FitnessMin\" in dir(creator): del creator.FitnessMin\nif \"Individual\" in dir(creator): del creator.Individual\ncreator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\ncreator.create(\"Individual\", grape.Individual, fitness=creator.FitnessMin)\nprint(\"[OK] DEAP ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19017d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# GRAMMAR \u2014 Standard GRAPE symbolic regression grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79076f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# Following the function set used in the GRAPE paper and standard GP/GE\n",
    "# benchmarks: {+, -, *, pdiv, sin, tanh, plog, psqrt}\n",
    "# NO pexp \u2014 it trivially solves Keijzer-6 and creates numerical instability.\n",
    "# Ephemeral constants via <c><c>.<c><c> (range 00.00\u201399.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13176f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "_GRAMMAR_DIR = tempfile.mkdtemp(prefix='grape_bnf_')\n",
    "\n",
    "def make_grammar(n_features, task='regression'):\n",
    "    \"\"\"Build BNF grammar.\n",
    "    \n",
    "    Regression: standard GRAPE function set {+,-,*,pdiv,sin,tanh,plog,psqrt}\n",
    "    Classification: adds if_gt(a,b,c,d) \u2014 a conditional operator that enables\n",
    "                    GE to evolve piecewise decision rules, essential for\n",
    "                    constructing classification boundaries.\n",
    "                    Ref: Kishore et al. (2000) IEEE TEC 4(3), 242\u2013258\n",
    "    \"\"\"\n",
    "    vars_list = ' | '.join([f'x[{i}]' for i in range(n_features)])\n",
    "    \n",
    "    base_ops = (\n",
    "        \"<e> ::= <e>+<e> | <e>-<e> | <e>*<e> | pdiv(<e>,<e>)\"\n",
    "        \" | psqrt(<e>) | np.sin(<e>) | np.tanh(<e>) | plog(<e>)\"\n",
    "    )\n",
    "    \n",
    "    if task == 'classification':\n",
    "        # Add conditional: if_gt(a,b,c,d) = c if a>b else d\n",
    "        base_ops += \" | if_gt(<e>,<e>,<e>,<e>)\"\n",
    "    \n",
    "    bnf = (\n",
    "        base_ops\n",
    "        + f\" | {vars_list} | <c><c>.<c><c>\\n\"\n",
    "        \"<c> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\\n\"\n",
    "    )\n",
    "    \n",
    "    label = 'cls' if task == 'classification' else 'reg'\n",
    "    fname = os.path.join(_GRAMMAR_DIR, f'grammar_{n_features}feat_{label}.bnf')\n",
    "    with open(fname, 'w') as f: f.write(bnf)\n",
    "    g = grape.Grammar(fname)\n",
    "    extras = \" + if_gt\" if task == 'classification' else \"\"\n",
    "    print(f\"    Grammar ({n_features} features, {task}): {len(g.non_terminals)} NTs, \"\n",
    "          f\"{sum(g.n_rules)} productions{extras}, \"\n",
    "          f\"terminals include {n_features} variables + constants\")\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba713c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nSEED = 42\n\n# \u2500\u2500 Population & generations \u2500\u2500\nPOP_SIZE    = 500      # Standard GE (de Lima et al. 2022; Ryan & Azad 2003)\nMAX_GENS    = 100       # Sufficient with pop=500 and elitism\nELITE_SIZE  = 1         # Preserve best individual across generations\nN_RUNS      = 30        # Standard for statistical analysis\n\n# \u2500\u2500 Genetic operators \u2500\u2500\nP_CX        = 0.9       # Crossover probability (standard)\nP_MUT       = 0.05      # Per-codon mutation rate (higher = more exploration)\nTOURN       = 7         # Tournament size (GRAPE standard for pop=500)\n\n# \u2500\u2500 Initialisation \u2500\u2500\nMIN_INIT_GL   = 30      # Random init: min genome length\nMAX_INIT_GL   = 250     # Random init: max genome length (wide range)\nMIN_INIT_DEPTH = 2      # Sensible/PI Grow: minimum initial tree depth\nMAX_INIT_DEPTH = 6      # Sensible/PI Grow: maximum initial tree depth\n\n# \u2500\u2500 Mapping & representation \u2500\u2500\nMAX_TREE_DEPTH = 17     # Maximum derivation tree depth during evolution\nCODON_SIZE     = 255    # Maximum codon value\nMAX_GENOME_LENGTH = None\n\n# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n# \u2551  CRITICAL: CODON_CONSUMPTION must be 'lazy' for GRAPE           \u2551\n# \u2551                                                                   \u2551\n# \u2551  'lazy'  = only consume a codon when a non-terminal has >1 rule  \u2551\n# \u2551  'eager' = consume a codon at every non-terminal expansion       \u2551\n# \u2551                                                                   \u2551\n# \u2551  Sensible init & PI Grow build phenotype-first genomes that      \u2551\n# \u2551  are ONLY correct under the lazy mapper. Using 'eager' causes    \u2551\n# \u2551  the tail codons to be consumed during mapping, potentially      \u2551\n# \u2551  making individuals invalid. This was the root cause of          \u2551\n# \u2551  Sensible/PI Grow showing <100% validity.                        \u2551\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\nCODON_CONSUMPTION = 'lazy'\nGENOME_REPRESENTATION = 'list'\n\n# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n# \u2551  v4 IMPROVEMENTS FOR CLASSIFICATION ACCURACY                     \u2551\n# \u2551                                                                   \u2551\n# \u2551  1. POP_SIZE = 500 (was 60) \u2014 GRAPE standard                    \u2551\n# \u2551  2. TOURN = 7 (was 3) \u2014 appropriate for pop=500                 \u2551\n# \u2551  3. Feature normalisation \u2014 z-score on Diabetes features         \u2551\n# \u2551  4. Lexicase selection \u2014 for classification problems             \u2551\n# \u2551  5. Linear scaling \u2014 scale raw output before sigmoid             \u2551\n# \u2551  6. if_gt in grammar \u2014 enables piecewise decision rules          \u2551\n# \u2551                                                                   \u2551\n# \u2551  Refs: de Lima et al. (2022), Murphy et al. (2023),              \u2551\n# \u2551        Helmuth & Spector (2015), Kishore et al. (2000)           \u2551\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\nUSE_LEXICASE    = True   # Use \u03b5-lexicase for classification (tournament for SR)\nUSE_LIN_SCALE   = False  # DISABLED: OLS to binary y\u2208{0,1} then sigmoid destroys signal\nEPSILON_LEXICASE = True  # Use \u03b5-lexicase (MAD-based) vs strict lexicase\n\nMETHODS = ['Random', 'Sensible', 'PI Grow']\nCOLORS  = {'Random': '#E74C3C', 'Sensible': '#3498DB', 'PI Grow': '#2ECC71'}\n\nprint(\"=\" * 72)\nprint(\"  GRAMMATICAL EVOLUTION \u2014 INITIALIZATION METHODS COMPARISON (v4.1 \u2014 PIMA)\")\nprint(\"=\" * 72)\nprint(f\"  Population: {POP_SIZE}  |  Generations: {MAX_GENS}  |  Runs: {N_RUNS}\")\nprint(f\"  Cx: {P_CX}  |  Mut: {P_MUT}/codon  |  Tournament: {TOURN}  |  Elite: {ELITE_SIZE}\")\nprint(f\"  Mapper: {CODON_CONSUMPTION}  |  Max Depth: {MAX_TREE_DEPTH}\")\nprint(f\"  Init Depth: [{MIN_INIT_DEPTH}, {MAX_INIT_DEPTH}]  |  Init GL: [{MIN_INIT_GL}, {MAX_INIT_GL}]\")\nprint(f\"  Lexicase (classification): {USE_LEXICASE}  |  Linear Scaling: {USE_LIN_SCALE}\")\nprint(f\"  Methods: {', '.join(METHODS)}\")\nprint(\"=\" * 72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23981b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# BENCHMARK PROBLEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\ndef load_keijzer6():\n    \"\"\"Keijzer-6: f(x) = \u03a3(1/i, i=1..x), train x\u2208[1,50], test x\u2208[1,120]\"\"\"\n    def kf(x): return np.array([sum(1.0/i for i in range(1, int(xi)+1)) for xi in x])\n    xr = np.linspace(1, 50, 50)\n    xe = np.linspace(1, 120, 120)\n    return np.array([xr]), kf(xr), np.array([xe]), kf(xe)\n\ndef load_nguyen7():\n    \"\"\"Nguyen-7: f(x) = ln(x+1) + ln(x\u00b2+1), train x\u2208[0,2], test x\u2208[0,2]\"\"\"\n    xr = np.linspace(0, 2, 20)\n    xe = np.linspace(0, 2, 100)\n    f = lambda x: np.log(x + 1) + np.log(x**2 + 1)\n    return np.array([xr]), f(xr), np.array([xe]), f(xe)\n\ndef load_pagie1():\n    \"\"\"Pagie-1: f(x,y) = 1/(1+x\u207b\u2074) + 1/(1+y\u207b\u2074), 2D grid\"\"\"\n    v = np.linspace(-5, 5, 26)\n    X0, X1 = np.meshgrid(v, v)\n    x0, x1 = X0.ravel(), X1.ravel()\n    with np.errstate(divide='ignore', invalid='ignore'):\n        y = 1.0/(1.0 + np.power(np.abs(x0) + 1e-10, -4)) + \\\n            1.0/(1.0 + np.power(np.abs(x1) + 1e-10, -4))\n    y = np.nan_to_num(y, nan=0.0, posinf=2.0, neginf=0.0)\n    rng = np.random.RandomState(SEED)\n    x0t = rng.uniform(-5, 5, 10000)\n    x1t = rng.uniform(-5, 5, 10000)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        yt = 1.0/(1.0 + np.power(np.abs(x0t) + 1e-10, -4)) + \\\n             1.0/(1.0 + np.power(np.abs(x1t) + 1e-10, -4))\n    yt = np.nan_to_num(yt, nan=0.0, posinf=2.0, neginf=0.0)\n    return np.array([x0, x1]), y, np.array([x0t, x1t]), yt\n\ndef load_pima_data():\n    \"\"\"PIMA Indian Diabetes Dataset: binary classification, stratified 70/30.\n    \n    768 samples, 8 features, binary target (0=no diabetes, 1=diabetes).\n    From National Institute of Diabetes and Digestive and Kidney Diseases.\n    All patients are females \u226521 years of Pima Indian heritage.\n    \n    Preprocessing (standard in the literature):\n      1. Replace biologically impossible zeros in Glucose, BloodPressure,\n         SkinThickness, Insulin, BMI with column medians (Naz & Ahuja 2020,\n         Bhoi et al. 2021).\n      2. Z-score normalisation (fit on train, transform test) following\n         de Lima et al. (2022) GRAPE \u00a74 for Heart Disease classification.\n    \n    Data source: OpenML dataset id=37 (CC0: Public Domain License).\n    Fallback: reads 'pima-indians-diabetes.csv' from working directory.\n    \"\"\"\n    feature_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n                     'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n    \n    X, y = None, None\n    \n    # \u2500\u2500 Try 1: fetch_openml \u2500\u2500\n    try:\n        pima = fetch_openml(data_id=37, as_frame=True, parser='auto')\n        X = pima.data.values.astype(float)\n        y_raw = pima.target\n        # OpenML encodes as 'tested_positive'/'tested_negative'\n        if hasattr(y_raw.iloc[0], 'lower'):\n            y = (y_raw == 'tested_positive').astype(float).values\n        else:\n            y = y_raw.astype(float).values\n        print(f\"    PIMA: loaded via fetch_openml ({X.shape[0]} samples, {X.shape[1]} features)\")\n    except Exception as e:\n        print(f\"    fetch_openml failed: {e}\")\n    \n    # \u2500\u2500 Try 2: local CSV \u2500\u2500\n    if X is None:\n        import pandas as pd\n        for path in ['pima-indians-diabetes.csv', 'diabetes.csv',\n                     'pima-indians-diabetes.data.csv']:\n            try:\n                df = pd.read_csv(path, header=None)\n                X = df.iloc[:, :8].values.astype(float)\n                y = df.iloc[:, 8].values.astype(float)\n                print(f\"    PIMA: loaded from '{path}' ({X.shape[0]} samples)\")\n                break\n            except FileNotFoundError:\n                continue\n    \n    if X is None:\n        raise FileNotFoundError(\n            \"PIMA dataset not found. Please either:\\n\"\n            \"  1. Install internet access for fetch_openml, OR\\n\"\n            \"  2. Download from https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\\n\"\n            \"     and place 'pima-indians-diabetes.csv' in the working directory.\\n\"\n            \"  The CSV should have 768 rows \u00d7 9 columns (no header), last column = target.\"\n        )\n    \n    assert X.shape == (768, 8), f\"Expected (768,8), got {X.shape}\"\n    assert len(y) == 768\n    \n    # \u2500\u2500 Zero imputation: replace 0s in cols 1-5 with column medians \u2500\u2500\n    # Columns: Glucose(1), BP(2), SkinThickness(3), Insulin(4), BMI(5)\n    # Zero is biologically impossible for these measurements.\n    zero_cols = [1, 2, 3, 4, 5]\n    for c in zero_cols:\n        mask = X[:, c] == 0\n        if mask.any():\n            col_median = np.median(X[~mask, c])\n            X[mask, c] = col_median\n    \n    n_pos = int(np.sum(y == 1))\n    n_neg = int(np.sum(y == 0))\n    print(f\"    PIMA: class0(neg)={n_neg}, class1(pos)={n_pos}, \"\n          f\"prevalence={n_pos/len(y):.1%}\")\n    \n    # \u2500\u2500 Stratified 70/30 split \u2500\u2500\n    Xr, Xe, yr, ye = train_test_split(\n        X, y, test_size=0.3, random_state=SEED, stratify=y)\n    \n    # \u2500\u2500 Z-score normalisation (fit on train only) \u2500\u2500\n    scaler = StandardScaler()\n    Xr = scaler.fit_transform(Xr)\n    Xe = scaler.transform(Xe)\n    print(f\"    Features normalised: mean\u22480, std\u22481 on train set \"\n          f\"(train={len(yr)}, test={len(ye)})\")\n    \n    return Xr.T, yr, Xe.T, ye\n\nPROBLEMS = OrderedDict([\n    ('Keijzer-6',  {'loader': load_keijzer6,   'n_features': 1,\n                    'desc': 'Harmonic (1D)',    'task': 'regression'}),\n    ('Nguyen-7',   {'loader': load_nguyen7,    'n_features': 1,\n                    'desc': 'ln(x+1)+ln(x\u00b2+1)','task': 'regression'}),\n    ('Pagie-1',    {'loader': load_pagie1,     'n_features': 2,\n                    'desc': '2D surface',       'task': 'regression'}),\n    ('PIMA',       {'loader': load_pima_data,  'n_features': 8,\n                    'desc': 'Binary (8D)',      'task': 'classification'}),\n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# FITNESS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa907e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n_EG = {\n    'np': np, 'pdiv': pdiv, 'plog': plog, 'psqrt': psqrt,\n    'sigmoid': sigmoid, 'if_gt': if_gt, '__builtins__': {}\n}\n\n# Cap extreme MSE to prevent plot distortion\nFITNESS_CAP = 1e8\n\ndef fitness_eval_regression(ind, pts):\n    \"\"\"MSE fitness for regression (minimise).\"\"\"\n    x, y = pts[0], pts[1]\n    if ind.invalid: return (np.nan,)\n    try:\n        pred = eval(ind.phenotype, _EG, {'x': x})\n    except:\n        return (np.nan,)\n    try:\n        pred = np.asarray(pred, dtype=float)\n        if pred.ndim == 0:\n            pred = np.full_like(y, float(pred))\n        if not np.all(np.isfinite(pred)):\n            return (np.nan,)\n        f = float(np.mean(np.square(y - pred)))\n    except:\n        return (np.nan,)\n    if not np.isfinite(f): return (np.nan,)\n    return (min(f, FITNESS_CAP),)\n\ndef _eval_raw(ind, x):\n    \"\"\"Evaluate phenotype \u2192 raw numpy array (helper).\"\"\"\n    if ind.invalid: return None\n    try:\n        raw = eval(ind.phenotype, _EG, {'x': x})\n        raw = np.asarray(raw, dtype=float)\n        if raw.ndim == 0:\n            raw = np.full(x.shape[1], float(raw))\n        if not np.all(np.isfinite(raw)):\n            return None\n        return raw\n    except:\n        return None\n\ndef fitness_eval_classification(ind, pts):\n    \"\"\"Binary cross-entropy (log-loss) for classification (minimise).\n    \n    v4: If USE_LIN_SCALE is True, applies linear scaling to the raw\n    output before sigmoid. This lets GE find the right shape while\n    OLS handles magnitude/offset (Murphy et al. 2023, Keijzer 2003).\n    \n    WARNING: Linear scaling is designed for regression (continuous y).\n    For binary classification y\u2208{0,1}, OLS maps raw\u2192[0,1], then\n    sigmoid compresses to [0.5,0.73], destroying discriminative signal.\n    USE_LIN_SCALE should be False for classification tasks.\n    \"\"\"\n    x, y = pts[0], pts[1]\n    raw = _eval_raw(ind, x)\n    if raw is None: return (np.nan,)\n    try:\n        if USE_LIN_SCALE:\n            raw = linear_scale(raw, y)\n        prob = sigmoid(raw)\n        eps = 1e-15\n        prob = np.clip(prob, eps, 1.0 - eps)\n        bce = -np.mean(y * np.log(prob) + (1.0 - y) * np.log(1.0 - prob))\n        f = float(bce)\n    except:\n        return (np.nan,)\n    if not np.isfinite(f): return (np.nan,)\n    return (f,)\n\ndef fitness_per_case_classification(ind, pts):\n    \"\"\"Per-sample log-loss vector for lexicase selection.\n    \n    Returns a numpy array of per-sample losses (lower = better).\n    Used by lexicase to evaluate individuals on each training case\n    independently, maintaining higher diversity than aggregated fitness.\n    Ref: de Lima et al. (2022) \u00a74.3; Helmuth & Spector (2015)\n    \"\"\"\n    x, y = pts[0], pts[1]\n    raw = _eval_raw(ind, x)\n    if raw is None: return None\n    try:\n        if USE_LIN_SCALE:\n            raw = linear_scale(raw, y)\n        prob = sigmoid(raw)\n        eps = 1e-15\n        prob = np.clip(prob, eps, 1.0 - eps)\n        per_case = -(y * np.log(prob) + (1.0 - y) * np.log(1.0 - prob))\n        if not np.all(np.isfinite(per_case)):\n            return None\n        return per_case\n    except:\n        return None\n\ndef classification_accuracy(ind, pts):\n    \"\"\"Classification accuracy (for reporting only).\"\"\"\n    x, y = pts[0], pts[1]\n    raw = _eval_raw(ind, x)\n    if raw is None: return np.nan\n    try:\n        if USE_LIN_SCALE:\n            raw = linear_scale(raw, y)\n        prob = sigmoid(raw)\n        preds = (prob >= 0.5).astype(float)\n        return float(accuracy_score(y, preds))\n    except:\n        return np.nan\n\ndef get_fitness_func(task):\n    return fitness_eval_classification if task == 'classification' else fitness_eval_regression\n\ndef gfv(ind):\n    \"\"\"Get fitness value, returning nan if invalid.\"\"\"\n    if not ind.fitness.valid: return np.nan\n    v = ind.fitness.values[0]\n    return np.nan if (v is None or np.isnan(v)) else v\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# LEXICASE SELECTION \u2014 implements the algorithm from the GRAPE paper\n# (de Lima et al. 2022, Listing 2) and \u03b5-lexicase (La Cava et al. 2016)\n#\n# This is a standalone implementation that does not depend on grape.py\n# having lexicase built in, ensuring portability.\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\ndef _epsilon_lexicase_select_one(case_fitness, rng):\n    \"\"\"Select one parent via \u03b5-lexicase (MAD-based epsilon).\n    \n    case_fitness: np.array of shape (n_individuals, n_cases) \u2014 lower = better\n    rng: numpy RandomState\n    \n    Algorithm:\n      1. Start with all candidates\n      2. Shuffle training cases\n      3. For each case, compute \u03b5 = median absolute deviation (MAD)\n      4. Keep only candidates within \u03b5 of the best on that case\n      5. Repeat until 1 candidate left or cases exhausted\n      6. Random tiebreak from remaining\n    \n    Refs: La Cava et al. (2016) GECCO; Helmuth & Spector (2015) IEEE TEC\n    \"\"\"\n    n_inds, n_cases = case_fitness.shape\n    candidates = np.arange(n_inds)\n    case_order = rng.permutation(n_cases)\n    \n    for c in case_order:\n        if len(candidates) <= 1:\n            break\n        vals = case_fitness[candidates, c]\n        if EPSILON_LEXICASE:\n            # MAD-based epsilon: median absolute deviation from median\n            med = np.median(vals)\n            mad = np.median(np.abs(vals - med))\n            eps = mad\n        else:\n            eps = 0.0  # strict lexicase\n        \n        best_val = np.min(vals)\n        mask = vals <= best_val + eps\n        candidates = candidates[mask]\n    \n    return rng.choice(candidates)\n\ndef lexicase_selection(pop, n_select, case_fitness_matrix, seed=None):\n    \"\"\"Select n_select parents from pop using \u03b5-lexicase.\n    \n    pop: list of individuals (only valid ones)\n    n_select: number of parents to select\n    case_fitness_matrix: np.array (len(pop), n_training_cases), lower = better\n    \n    Returns: list of selected individuals (with replacement possible)\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    selected = []\n    for _ in range(n_select):\n        idx = _epsilon_lexicase_select_one(case_fitness_matrix, rng)\n        selected.append(pop[idx])\n    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc006e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# INITIALISATION \u2014 using GRAPE's built-in methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "def create_pop(method, grammar):\n",
    "    \"\"\"Create initial population using GRAPE's initialisation methods.\n",
    "    \n",
    "    - Random: random_initialisation (genome-first, may produce invalids)\n",
    "    - Sensible: sensible_initialisation (phenotype-first, RHH, 100% valid)\n",
    "    - PI Grow: PI_Grow (phenotype-first, position-independent, 100% valid)\n",
    "    \"\"\"\n",
    "    if method == 'Random':\n",
    "        return grape.random_initialisation(\n",
    "            creator.Individual, POP_SIZE, grammar,\n",
    "            MIN_INIT_GL, MAX_INIT_GL, MAX_TREE_DEPTH,\n",
    "            CODON_SIZE, CODON_CONSUMPTION, GENOME_REPRESENTATION)\n",
    "    elif method == 'Sensible':\n",
    "        return grape.sensible_initialisation(\n",
    "            creator.Individual, POP_SIZE, grammar,\n",
    "            MIN_INIT_DEPTH, MAX_INIT_DEPTH,\n",
    "            CODON_SIZE, CODON_CONSUMPTION, GENOME_REPRESENTATION)\n",
    "    elif method == 'PI Grow':\n",
    "        return grape.PI_Grow(\n",
    "            creator.Individual, POP_SIZE, grammar,\n",
    "            MIN_INIT_DEPTH, MAX_INIT_DEPTH,\n",
    "            CODON_SIZE, CODON_CONSUMPTION, GENOME_REPRESENTATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181daf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# SINGLE EVOLUTIONARY RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "def run_ge(init_method, grammar, Xr, Yr, Xe, Ye, seed, task='regression'):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    fitness_func = get_fitness_func(task)\n",
    "    ptr, pte = [Xr, Yr], [Xe, Ye]\n",
    "\n",
    "    # \u2500\u2500 Determine if this run uses lexicase \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    use_lex = (USE_LEXICASE and task == 'classification')\n",
    "\n",
    "    # \u2500\u2500 Initialise population \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    pop = create_pop(init_method, grammar)\n",
    "    for ind in pop:\n",
    "        ind.fitness.values = fitness_func(ind, ptr)\n",
    "\n",
    "    valid = [i for i in pop if not i.invalid and not np.isnan(gfv(i))]\n",
    "    n_valid = len(valid)\n",
    "\n",
    "    init_stats = {\n",
    "        'validity':     n_valid / POP_SIZE,\n",
    "        'best_fitness': float(min(gfv(i) for i in valid)) if valid else np.nan,\n",
    "        'mean_fitness': float(np.nanmean([gfv(i) for i in valid])) if valid else np.nan,\n",
    "        'mean_depth':   float(np.mean([i.depth for i in valid])) if valid else 0,\n",
    "        'mean_nodes':   float(np.mean([i.nodes for i in valid])) if valid else 0,\n",
    "        'mean_gl':      float(np.mean([len(i.genome) for i in valid])) if valid else 0,\n",
    "        'unique_pheno': len(set(i.phenotype for i in valid if i.phenotype)),\n",
    "        'pop_div':      (len(set(i.phenotype for i in valid if i.phenotype))\n",
    "                         / max(n_valid, 1)) if valid else 0,\n",
    "    }\n",
    "    if task == 'classification' and valid:\n",
    "        accs = [classification_accuracy(i, ptr) for i in valid]\n",
    "        accs = [a for a in accs if not np.isnan(a)]\n",
    "        init_stats['best_accuracy'] = max(accs) if accs else np.nan\n",
    "        init_stats['mean_accuracy'] = float(np.mean(accs)) if accs else np.nan\n",
    "\n",
    "    # \u2500\u2500 Tracking arrays \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # NOTE ON DIVERSITY MEASURES:\n",
    "    #   'pop_div'      = ratio of unique programs in the population\n",
    "    #                    (counts distinct expression strings \u2014 standard GE\n",
    "    #                    population diversity measure, cf. Nicolau 2017,\n",
    "    #                    Murphy et al. 2023, de Lima et al. 2022)\n",
    "    #   'behav_div'    = ratio of unique output vectors (rounded to 6 d.p.)\n",
    "    #                    (counts distinct behaviours \u2014 complements population\n",
    "    #                    diversity by capturing functional equivalence)\n",
    "    hist = {\n",
    "        'min': [], 'avg': [], 'invalid': [], 'fitness_test': [],\n",
    "        'avg_depth': [], 'avg_nodes': [],\n",
    "        'pop_div': [],       # unique programs / pop size (population diversity)\n",
    "        'behav_div': [],     # unique output vectors / pop size (behavioural diversity)\n",
    "    }\n",
    "    if task == 'classification':\n",
    "        hist['train_acc'] = []\n",
    "        hist['test_acc'] = []\n",
    "\n",
    "    hof = tools.HallOfFame(3)\n",
    "\n",
    "    # \u2500\u2500 Generational loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    for gen in range(MAX_GENS + 1):\n",
    "        vp = [i for i in pop if not i.invalid and not np.isnan(gfv(i))]\n",
    "        fits = [gfv(i) for i in vp]\n",
    "        if vp:\n",
    "            hof.update(vp)\n",
    "\n",
    "        # Test fitness of best-ever individual\n",
    "        if len(hof) > 0 and not hof[0].invalid:\n",
    "            tf = fitness_func(hof[0], pte)[0]\n",
    "        else:\n",
    "            tf = np.nan\n",
    "\n",
    "        hist['min'].append(float(min(fits)) if fits else np.nan)\n",
    "        hist['avg'].append(float(np.mean(fits)) if fits else np.nan)\n",
    "        hist['invalid'].append(POP_SIZE - len(vp))\n",
    "        hist['fitness_test'].append(float(tf) if np.isfinite(tf) else np.nan)\n",
    "        depths = [i.depth for i in vp]\n",
    "        nodes  = [i.nodes for i in vp]\n",
    "        hist['avg_depth'].append(float(np.mean(depths)) if vp else 0)\n",
    "        hist['avg_nodes'].append(float(np.mean(nodes)) if vp else 0)\n",
    "        \n",
    "        # \u2500\u2500 Population diversity (unique programs) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        phenos = set()\n",
    "        for i in vp:\n",
    "            if i.phenotype:\n",
    "                phenos.add(i.phenotype)\n",
    "        hist['pop_div'].append(len(phenos) / max(len(vp), 1) if vp else 0)\n",
    "        \n",
    "        # \u2500\u2500 Behavioural diversity (unique output vectors) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        # Round outputs to 6 d.p. to group near-identical outputs\n",
    "        semantics = set()\n",
    "        for i in vp:\n",
    "            if i.phenotype and not i.invalid:\n",
    "                try:\n",
    "                    out = eval(i.phenotype, _EG, {'x': Xr})\n",
    "                    out = np.asarray(out, dtype=float)\n",
    "                    if out.ndim == 0:\n",
    "                        out = np.full(Yr.shape, float(out))\n",
    "                    if np.all(np.isfinite(out)):\n",
    "                        semantics.add(tuple(np.round(out, 6)))\n",
    "                except:\n",
    "                    pass\n",
    "        hist['behav_div'].append(len(semantics) / max(len(vp), 1) if vp else 0)\n",
    "\n",
    "        if task == 'classification':\n",
    "            tr_acc = classification_accuracy(hof[0], ptr) if len(hof) > 0 and not hof[0].invalid else np.nan\n",
    "            te_acc = classification_accuracy(hof[0], pte) if len(hof) > 0 and not hof[0].invalid else np.nan\n",
    "            hist['train_acc'].append(float(tr_acc) if np.isfinite(tr_acc) else np.nan)\n",
    "            hist['test_acc'].append(float(te_acc) if np.isfinite(te_acc) else np.nan)\n",
    "\n",
    "        if gen == MAX_GENS:\n",
    "            break\n",
    "\n",
    "        # \u2500\u2500 Elitism \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        elite = []\n",
    "        if vp:\n",
    "            sorted_vp = sorted(vp, key=lambda i: gfv(i))\n",
    "            elite = [copy.deepcopy(sorted_vp[j])\n",
    "                     for j in range(min(ELITE_SIZE, len(sorted_vp)))]\n",
    "\n",
    "        # \u2500\u2500 Selection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        n_offspring = POP_SIZE - ELITE_SIZE\n",
    "        if vp:\n",
    "            if use_lex:\n",
    "                # \u2500\u2500 \u03b5-Lexicase selection for classification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "                # Build per-case fitness matrix for lexicase\n",
    "                # Ref: de Lima et al. (2022) \u00a74.3; La Cava et al. (2016)\n",
    "                case_matrix = []\n",
    "                valid_for_lex = []\n",
    "                for ind in vp:\n",
    "                    pc = fitness_per_case_classification(ind, ptr)\n",
    "                    if pc is not None:\n",
    "                        case_matrix.append(pc)\n",
    "                        valid_for_lex.append(ind)\n",
    "                \n",
    "                if len(valid_for_lex) >= 2:\n",
    "                    case_matrix = np.array(case_matrix)\n",
    "                    sel = lexicase_selection(\n",
    "                        valid_for_lex, n_offspring, case_matrix,\n",
    "                        seed=seed + gen)\n",
    "                else:\n",
    "                    # Fallback to tournament if too few valid for lexicase\n",
    "                    sel = tools.selTournament(vp, n_offspring, tournsize=TOURN)\n",
    "            else:\n",
    "                # \u2500\u2500 Tournament selection for regression \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "                sel = tools.selTournament(vp, n_offspring, tournsize=TOURN)\n",
    "        else:\n",
    "            sel = [copy.deepcopy(i) for i in pop[:n_offspring]]\n",
    "        off = [copy.deepcopy(i) for i in sel]\n",
    "\n",
    "        # \u2500\u2500 Crossover \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        for i in range(1, len(off), 2):\n",
    "            if random.random() < P_CX:\n",
    "                off[i-1], off[i] = grape.crossover_onepoint(\n",
    "                    off[i-1], off[i], grammar, MAX_TREE_DEPTH,\n",
    "                    CODON_CONSUMPTION, GENOME_REPRESENTATION, MAX_GENOME_LENGTH)\n",
    "\n",
    "        # \u2500\u2500 Mutation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        for i in range(len(off)):\n",
    "            off[i], = grape.mutation_int_flip_per_codon(\n",
    "                off[i], P_MUT, CODON_SIZE, grammar, MAX_TREE_DEPTH,\n",
    "                CODON_CONSUMPTION, MAX_GENOME_LENGTH)\n",
    "\n",
    "        # \u2500\u2500 Evaluate new offspring \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        for ind in off:\n",
    "            if not ind.fitness.valid:\n",
    "                ind.fitness.values = fitness_func(ind, ptr)\n",
    "\n",
    "        # \u2500\u2500 Next generation = offspring + elite \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        pop = off + elite\n",
    "\n",
    "    # \u2500\u2500 Collect final results \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    best = hof[0] if len(hof) > 0 else None\n",
    "    ft = fitness_func(best, pte)[0] if best and not best.invalid else np.nan\n",
    "\n",
    "    result = {\n",
    "        'init': init_stats,\n",
    "        'hist': hist,\n",
    "        'train_fitness': gfv(best) if best else np.nan,\n",
    "        'test_fitness':  float(ft) if np.isfinite(ft) else np.nan,\n",
    "        'pheno': best.phenotype if best else None,\n",
    "        'depth': best.depth if best else 0,\n",
    "        'nodes': best.nodes if best else 0,\n",
    "        'gl':    len(best.genome) if best else 0,\n",
    "        'task':  task,\n",
    "        'selection': 'lexicase' if use_lex else 'tournament',\n",
    "    }\n",
    "    if task == 'classification' and best and not best.invalid:\n",
    "        result['train_acc'] = classification_accuracy(best, ptr)\n",
    "        result['test_acc']  = classification_accuracy(best, pte)\n",
    "    elif task == 'classification':\n",
    "        result['train_acc'] = np.nan\n",
    "        result['test_acc']  = np.nan\n",
    "    if task == 'regression':\n",
    "        result['train_mse'] = result['train_fitness']\n",
    "        result['test_mse']  = result['test_fitness']\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cef1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# RUN ALL EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9085fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "total_runs = len(PROBLEMS) * len(METHODS) * N_RUNS\n",
    "print(f\"\\nSTARTING: {len(PROBLEMS)} problems \u00d7 {len(METHODS)} methods \u00d7 {N_RUNS} runs = {total_runs} runs\")\n",
    "print(f\"  Classification: lexicase={'ON' if USE_LEXICASE else 'OFF'}, \"\n",
    "      f\"linear_scale={'ON' if USE_LIN_SCALE else 'OFF'}, if_gt=ON\")\n",
    "print(f\"  Regression: tournament(size={TOURN})\\n\")\n",
    "\n",
    "all_results = {}\n",
    "t0_all = time.time()\n",
    "\n",
    "for pn, pi in PROBLEMS.items():\n",
    "    print(f\"\\n{'#'*65}\\n# {pn}: {pi['desc']} ({pi['task']})\\n{'#'*65}\")\n",
    "    grammar = make_grammar(pi['n_features'], task=pi['task'])\n",
    "    pr = {}\n",
    "    for mn in METHODS:\n",
    "        t0 = time.time()\n",
    "        runs = []\n",
    "        for r in range(N_RUNS):\n",
    "            if (r+1) % 10 == 0 or r == 0:\n",
    "                print(f\"  {mn:>10s} run {r+1:>2d}/{N_RUNS}...\", flush=True)\n",
    "            np.random.seed(SEED + r)\n",
    "            random.seed(SEED + r)\n",
    "            Xr, Yr, Xe, Ye = pi['loader']()\n",
    "            runs.append(run_ge(mn, grammar, Xr, Yr, Xe, Ye, SEED + r, task=pi['task']))\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  {mn:>10s}: {elapsed:.0f}s total ({elapsed/N_RUNS:.1f}s/run)\")\n",
    "        pr[mn] = runs\n",
    "    all_results[pn] = pr\n",
    "\n",
    "total_t = time.time() - t0_all\n",
    "print(f\"\\n{'='*72}\\n COMPLETED in {total_t/60:.1f} min\\n{'='*72}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# VERIFY VALIDITY FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "print(\"\\n\" + \"=\"*72)\n",
    "print(\" VALIDITY VERIFICATION\")\n",
    "print(\"=\"*72)\n",
    "for pn in all_results:\n",
    "    for mn in METHODS:\n",
    "        vals = [r['init']['validity']*100 for r in all_results[pn][mn]]\n",
    "        mean_v = np.mean(vals)\n",
    "        min_v = np.min(vals)\n",
    "        status = \"\u2713\" if (mn == 'Random' or min_v >= 99.9) else \"\u2717 STILL BROKEN\"\n",
    "        print(f\"  {pn:<12s} {mn:<10s}: mean={mean_v:6.2f}%, min={min_v:6.2f}%  {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f286a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# STATISTICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "def sig_stars(p):\n",
    "    if p < 0.001: return \"***\"\n",
    "    elif p < 0.01: return \"**\"\n",
    "    elif p < 0.05: return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "def vda(a, b):\n",
    "    m, n = len(a), len(b)\n",
    "    if m == 0 or n == 0: return 0.5\n",
    "    return sum(1 if ai < bi else 0.5 if ai == bi else 0 for ai in a for bi in b) / (m * n)\n",
    "\n",
    "def effect_label(a):\n",
    "    d = abs(a - 0.5)\n",
    "    if d < 0.06: return \"negl\"\n",
    "    elif d < 0.14: return \"small\"\n",
    "    elif d < 0.21: return \"medium\"\n",
    "    return \"large\"\n",
    "\n",
    "def safe_vals(lst):\n",
    "    return [x for x in lst if x is not None and np.isfinite(x) and abs(x) < 1e10]\n",
    "\n",
    "for pn, res in all_results.items():\n",
    "    task = PROBLEMS[pn]['task']\n",
    "    print(f\"\\n{'='*72}\\n {pn} ({'Classification' if task=='classification' else 'Regression'})\\n{'='*72}\")\n",
    "\n",
    "    if task == 'classification':\n",
    "        metrics = [\n",
    "            (\"Init Validity %\",      lambda r: r['init']['validity']*100),\n",
    "            (\"Init Unique Pheno\",    lambda r: r['init']['unique_pheno']),\n",
    "            (\"Init Mean Depth\",      lambda r: r['init']['mean_depth']),\n",
    "            (\"Init Population Div\",       lambda r: r['init']['pop_div']),\n",
    "            (\"Init Best Log-Loss\",   lambda r: r['init']['best_fitness']),\n",
    "            (\"Init Mean Log-Loss\",   lambda r: r['init']['mean_fitness']),\n",
    "            (\"Init Best Accuracy\",   lambda r: r['init'].get('best_accuracy', np.nan)),\n",
    "            (\"Init Mean Accuracy\",   lambda r: r['init'].get('mean_accuracy', np.nan)),\n",
    "            (\"Final Train Log-Loss\", lambda r: r['train_fitness']),\n",
    "            (\"Final Test Log-Loss\",  lambda r: r['test_fitness']),\n",
    "            (\"Final Train Accuracy\", lambda r: r.get('train_acc', np.nan)),\n",
    "            (\"Final Test Accuracy\",  lambda r: r.get('test_acc', np.nan)),\n",
    "        ]\n",
    "    else:\n",
    "        metrics = [\n",
    "            (\"Init Validity %\",    lambda r: r['init']['validity']*100),\n",
    "            (\"Init Unique Pheno\",  lambda r: r['init']['unique_pheno']),\n",
    "            (\"Init Mean Depth\",    lambda r: r['init']['mean_depth']),\n",
    "            (\"Init Population Div\",    lambda r: r['init']['pop_div']),\n",
    "            (\"Init Best MSE\",      lambda r: r['init']['best_fitness']),\n",
    "            (\"Init Mean MSE\",      lambda r: r['init']['mean_fitness']),\n",
    "            (\"Final Train MSE\",    lambda r: r['train_fitness']),\n",
    "            (\"Final Test MSE\",     lambda r: r['test_fitness']),\n",
    "        ]\n",
    "\n",
    "    for label, ext in metrics:\n",
    "        vals = {m: safe_vals([ext(r) for r in res[m]]) for m in METHODS}\n",
    "        parts = []\n",
    "        for m in METHODS:\n",
    "            if vals[m]:\n",
    "                parts.append(f\"{m}={np.mean(vals[m]):>9.4f}\u00b1{np.std(vals[m]):<7.4f}\")\n",
    "            else:\n",
    "                parts.append(f\"{m}={'N/A':>9}\")\n",
    "        print(f\"  {label:<24}: {'  '.join(parts)}\")\n",
    "\n",
    "        # Kruskal-Wallis\n",
    "        all_groups = [vals[m] for m in METHODS if vals[m] and len(vals[m]) >= 3]\n",
    "        if len(all_groups) >= 2:\n",
    "            try:\n",
    "                H, p_kw = kruskal(*all_groups)\n",
    "                print(f\"    Kruskal-Wallis: H={H:.3f}, p={p_kw:.6f} ({sig_stars(p_kw)})\")\n",
    "            except: pass\n",
    "\n",
    "        # Pairwise comparisons\n",
    "        pw = []\n",
    "        for a, b in [('Random','Sensible'),('Random','PI Grow'),('Sensible','PI Grow')]:\n",
    "            if vals[a] and vals[b] and len(vals[a]) >= 3 and len(vals[b]) >= 3:\n",
    "                _, p = mannwhitneyu(vals[a], vals[b], alternative='two-sided')\n",
    "                ae = vda(vals[a], vals[b])\n",
    "                pw.append(f\"{a[:3]}v{b[:3]}: p={p:.4f}({sig_stars(p)}) A={ae:.3f}({effect_label(ae)})\")\n",
    "        if pw:\n",
    "            print(f\"    {'  |  '.join(pw)}\")\n",
    "\n",
    "    print(f\"\\n  Best solutions (by train fitness):\")\n",
    "    for m in METHODS:\n",
    "        v = [r for r in res[m] if np.isfinite(r.get('train_fitness', np.nan))]\n",
    "        if v:\n",
    "            b = min(v, key=lambda r: r['train_fitness'])\n",
    "            if task == 'classification':\n",
    "                print(f\"    {m}: LL={b['train_fitness']:.6f} TestLL={b['test_fitness']:.6f} \"\n",
    "                      f\"TrAcc={b.get('train_acc',np.nan):.4f} TeAcc={b.get('test_acc',np.nan):.4f} \"\n",
    "                      f\"D={b['depth']} N={b['nodes']}\")\n",
    "            else:\n",
    "                print(f\"    {m}: TrainMSE={b['train_fitness']:.6f} TestMSE={b['test_fitness']:.6f} \"\n",
    "                      f\"D={b['depth']} N={b['nodes']}\")\n",
    "            pheno = (b['pheno'] or 'N/A')[:120]\n",
    "            print(f\"      expr: {pheno}\")\n",
    "\n",
    "# \u2500\u2500 Summary tables \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n SUMMARY TABLES\\n\" + \"=\" * 80)\n",
    "pnames = list(all_results.keys())\n",
    "\n",
    "for tl, kf, higher_better in [\n",
    "    (\"INITIAL VALIDITY (%)\",            lambda r: r['init']['validity']*100, True),\n",
    "    (\"TRAIN FITNESS (MSE/LogLoss)\",     lambda r: r['train_fitness'],       False),\n",
    "    (\"TEST FITNESS (MSE/LogLoss)\",      lambda r: r['test_fitness'],        False),\n",
    "]:\n",
    "    print(f\"\\n{'-'*80}\\n {tl}\\n{'-'*80}\")\n",
    "    h = f\"{'Problem':<13}\"\n",
    "    for m in METHODS: h += f\"  {m:>16}\"\n",
    "    h += f\"  {'Best':>10}\"\n",
    "    print(h); print(\"-\"*80)\n",
    "    for p in pnames:\n",
    "        row = f\"{p:<13}\"; means = {}\n",
    "        for m in METHODS:\n",
    "            v = safe_vals([kf(r) for r in all_results[p][m]])\n",
    "            if v:\n",
    "                means[m] = np.mean(v)\n",
    "                row += f\"  {np.mean(v):>7.4f}\u00b1{np.std(v):>6.4f}\"\n",
    "            else:\n",
    "                means[m] = float('inf') if not higher_better else float('-inf')\n",
    "                row += f\"  {'N/A':>16}\"\n",
    "        if higher_better:\n",
    "            bm = max(means, key=means.get)\n",
    "        else:\n",
    "            bm = min(means, key=means.get)\n",
    "        row += f\"  {bm:>10}\"\n",
    "        print(row)\n",
    "\n",
    "# Classification accuracy summary\n",
    "cls_problems = [p for p in pnames if PROBLEMS[p]['task'] == 'classification']\n",
    "if cls_problems:\n",
    "    for tl, kf in [(\"TRAIN ACCURACY\", lambda r: r.get('train_acc', np.nan)),\n",
    "                    (\"TEST ACCURACY\",  lambda r: r.get('test_acc', np.nan))]:\n",
    "        print(f\"\\n{'-'*80}\\n {tl}\\n{'-'*80}\")\n",
    "        h = f\"{'Problem':<13}\"\n",
    "        for m in METHODS: h += f\"  {m:>16}\"\n",
    "        h += f\"  {'Best':>10}\"; print(h); print(\"-\"*80)\n",
    "        for p in cls_problems:\n",
    "            row = f\"{p:<13}\"; means = {}\n",
    "            for m in METHODS:\n",
    "                v = safe_vals([kf(r) for r in all_results[p][m]])\n",
    "                if v:\n",
    "                    means[m] = np.mean(v)\n",
    "                    row += f\"  {np.mean(v):>7.4f}\u00b1{np.std(v):>6.4f}\"\n",
    "                else:\n",
    "                    means[m] = float('-inf')\n",
    "                    row += f\"  {'N/A':>16}\"\n",
    "            bm = max(means, key=means.get)\n",
    "            row += f\"  {bm:>10}\"; print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# QUALITY PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a316253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11, 'font.family': 'serif',\n",
    "    'axes.titlesize': 14, 'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12, 'legend.fontsize': 10,\n",
    "    'figure.dpi': 150, 'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.grid': True, 'grid.alpha': 0.25, 'grid.linestyle': '--',\n",
    "    'lines.linewidth': 1.8,\n",
    "    'axes.spines.top': False, 'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "def _box(pn, res, mf, yl, title, fp):\n",
    "    \"\"\"Box plot with significance brackets.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5, 4.5))\n",
    "    d = {m: safe_vals([mf(r) for r in res[m]]) for m in METHODS}\n",
    "    data = [d[m] for m in METHODS if d[m]]\n",
    "    labels = [m for m in METHODS if d[m]]\n",
    "    if data:\n",
    "        bp = ax.boxplot(data, labels=labels, patch_artist=True,\n",
    "                        medianprops=dict(color='black', linewidth=2))\n",
    "        for patch, m in zip(bp['boxes'], labels):\n",
    "            patch.set_facecolor(COLORS[m]); patch.set_alpha(0.75)\n",
    "            patch.set_edgecolor('grey')\n",
    "        ym = ax.get_ylim()[1]; yo = 0\n",
    "        for i1, i2 in [(0,1),(0,2),(1,2)]:\n",
    "            if i1 < len(labels) and i2 < len(labels) and d[labels[i1]] and d[labels[i2]]:\n",
    "                _, p = mannwhitneyu(d[labels[i1]], d[labels[i2]])\n",
    "                ss = sig_stars(p)\n",
    "                if ss != 'ns':\n",
    "                    yr = ym - ax.get_ylim()[0]\n",
    "                    yp = ym + yo * yr * 0.07\n",
    "                    ax.plot([i1+1, i2+1], [yp, yp], 'k-', lw=0.8)\n",
    "                    ax.text((i1+i2+2)/2, yp+yr*0.01, ss, ha='center', fontsize=9)\n",
    "                    yo += 1\n",
    "    ax.set_ylabel(yl); ax.set_title(f'{pn}: {title}')\n",
    "    plt.tight_layout()\n",
    "    fn = f'{fp}_{pn.lower().replace(\"-\",\"\")}.png'\n",
    "    plt.savefig(fn); plt.show()\n",
    "    print(f\"  Saved: {fn}\")\n",
    "\n",
    "def _line(pn, res, hk, yl, title, fp, clip_val=None):\n",
    "    \"\"\"Line plot with confidence band (mean \u00b1 std).\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "    ng = len(res[METHODS[0]][0]['hist'][hk])\n",
    "    xa = np.arange(ng)\n",
    "    for m in METHODS:\n",
    "        c = np.array([r['hist'][hk] for r in res[m]], dtype=float)\n",
    "        if clip_val is not None:\n",
    "            c = np.where(np.isnan(c) | (np.abs(c) > clip_val), np.nan, c)\n",
    "        mn = np.nanmean(c, 0)\n",
    "        sd = np.nanstd(c, 0)\n",
    "        ax.plot(xa, mn, label=m, color=COLORS[m], linewidth=2)\n",
    "        ax.fill_between(xa, mn - sd, mn + sd, color=COLORS[m], alpha=0.12)\n",
    "    ax.set_xlabel('Generation'); ax.set_ylabel(yl)\n",
    "    ax.set_title(f'{pn}: {title}')\n",
    "    ax.legend(frameon=True, fancybox=True, framealpha=0.9)\n",
    "    plt.tight_layout()\n",
    "    fn = f'{fp}_{pn.lower().replace(\"-\",\"\")}.png'\n",
    "    plt.savefig(fn); plt.show()\n",
    "    print(f\"  Saved: {fn}\")\n",
    "\n",
    "# \u2500\u2500 Generate all plots \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "for pn, res in all_results.items():\n",
    "    task = PROBLEMS[pn]['task']\n",
    "    print(f\"\\n{'\u2500'*50}\\n Plots: {pn} ({'Classification' if task=='classification' else 'Regression'})\\n{'\u2500'*50}\")\n",
    "\n",
    "    # 1. Initial Validity\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    d = {m: safe_vals([r['init']['validity']*100 for r in res[m]]) for m in METHODS}\n",
    "    data = [d[m] for m in METHODS if d[m]]\n",
    "    labels = [m for m in METHODS if d[m]]\n",
    "    if data:\n",
    "        bp = ax.boxplot(data, positions=list(range(1, len(data)+1)),\n",
    "                        widths=0.55, patch_artist=True,\n",
    "                        medianprops=dict(color='black', linewidth=2))\n",
    "        for patch, m in zip(bp['boxes'], labels):\n",
    "            patch.set_facecolor(COLORS[m]); patch.set_alpha(0.75)\n",
    "            patch.set_edgecolor('grey')\n",
    "    ax.set_xticks(list(range(1, len(labels)+1)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel('Validity (%)')\n",
    "    ax.set_title(f'{pn}: Initial Validity')\n",
    "    ax.set_ylim([0, max(110, ax.get_ylim()[1] + 5)])\n",
    "    plt.tight_layout()\n",
    "    fn = f'fig1_{pn.lower().replace(\"-\",\"\")}_validity.png'\n",
    "    plt.savefig(fn); plt.show(); print(f\"  Saved: {fn}\")\n",
    "\n",
    "    # 2. Convergence (train fitness)\n",
    "    fl = 'Best Log-Loss (train)' if task == 'classification' else 'Best MSE (train)'\n",
    "    _line(pn, res, 'min', fl, 'Convergence', 'fig2_conv', clip_val=1e6)\n",
    "\n",
    "    # 3. Population Diversity (unique programs)\n",
    "    _line(pn, res, 'pop_div', 'Unique Programs (ratio)', 'Population Diversity', 'fig3_popdiv')\n",
    "\n",
    "    # 3b. Behavioural Diversity (unique outputs)\n",
    "    _line(pn, res, 'behav_div', 'Unique Behaviours (ratio)', 'Behavioural Diversity', 'fig3b_behavdiv')\n",
    "\n",
    "    # 4. Train fitness box plot\n",
    "    if task == 'classification':\n",
    "        _box(pn, res, lambda r: r['train_fitness'], 'Train Log-Loss',\n",
    "             f'Train Log-Loss ({N_RUNS} runs)', 'fig4_train')\n",
    "    else:\n",
    "        _box(pn, res, lambda r: r['train_fitness'], 'Train MSE',\n",
    "             f'Train MSE ({N_RUNS} runs)', 'fig4_train')\n",
    "\n",
    "    # 5. Test fitness box plot\n",
    "    if task == 'classification':\n",
    "        _box(pn, res, lambda r: r['test_fitness'], 'Test Log-Loss',\n",
    "             f'Test Log-Loss ({N_RUNS} runs)', 'fig5_test')\n",
    "    else:\n",
    "        _box(pn, res, lambda r: r['test_fitness'], 'Test MSE',\n",
    "             f'Test MSE ({N_RUNS} runs)', 'fig5_test')\n",
    "\n",
    "    # 6. Tree Depth\n",
    "    _line(pn, res, 'avg_depth', 'Avg Depth', 'Tree Depth', 'fig6_depth')\n",
    "\n",
    "    # 7. Test Fitness convergence\n",
    "    tfl = 'Best Log-Loss (test)' if task == 'classification' else 'Best MSE (test)'\n",
    "    _line(pn, res, 'fitness_test', tfl, 'Test Fitness', 'fig7_testconv', clip_val=1e6)\n",
    "\n",
    "    # Classification-specific\n",
    "    if task == 'classification':\n",
    "        _line(pn, res, 'train_acc', 'Train Accuracy', 'Training Accuracy', 'fig8_trainacc')\n",
    "        _line(pn, res, 'test_acc',  'Test Accuracy',  'Test Accuracy',     'fig9_testacc')\n",
    "        _box(pn, res, lambda r: r.get('train_acc', np.nan), 'Train Accuracy',\n",
    "             f'Train Accuracy ({N_RUNS} runs)', 'fig10_trainacc_box')\n",
    "        _box(pn, res, lambda r: r.get('test_acc', np.nan), 'Test Accuracy',\n",
    "             f'Test Accuracy ({N_RUNS} runs)', 'fig11_testacc_box')\n",
    "\n",
    "# \u2500\u2500 Summary bar chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(PROBLEMS)); w = 0.25\n",
    "for i, m in enumerate(METHODS):\n",
    "    mn_vals = []; sd_vals = []\n",
    "    for pn in pnames:\n",
    "        v = safe_vals([r['test_fitness'] for r in all_results[pn][m]])\n",
    "        mn_vals.append(np.mean(v) if v else 0)\n",
    "        sd_vals.append(np.std(v) if v else 0)\n",
    "    ax.bar(x + i*w, mn_vals, w, yerr=sd_vals, label=m,\n",
    "           color=COLORS[m], alpha=0.8, edgecolor='grey', capsize=3)\n",
    "ax.set_xlabel('Benchmark')\n",
    "ax.set_ylabel('Test Fitness (mean \u00b1 std)')\n",
    "ax.set_title(f'Test Fitness ({N_RUNS} runs \u00d7 {MAX_GENS} gens, Pop={POP_SIZE})')\n",
    "ax.set_xticks(x + w); ax.set_xticklabels(pnames)\n",
    "ax.legend(frameon=True); plt.tight_layout()\n",
    "plt.savefig('fig_summary_fitness.png'); plt.show()\n",
    "print(\"Saved: fig_summary_fitness.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\ndef ser(o):\n    if isinstance(o, (np.integer,)): return int(o)\n    if isinstance(o, (np.floating, float)):\n        return None if (np.isnan(o) if isinstance(o, float) else np.isnan(float(o))) else float(o)\n    if isinstance(o, np.ndarray): return o.tolist()\n    if isinstance(o, dict): return {k: ser(v) for k, v in o.items()}\n    if isinstance(o, list): return [ser(v) for v in o]\n    return o\n\nwith open('ge_results_v4_pima.json', 'w') as f:\n    json.dump(ser(all_results), f)\n\nconfig = {\n    'date': datetime.now().isoformat(),\n    'version': 'v4.1_PIMA',\n    'grape': os.path.abspath(getattr(grape, '__file__', '?')),\n    'fixes': [\n        'FIX 1: CODON_CONSUMPTION=lazy (was eager) \u2192 Sensible/PI Grow now 100% valid',\n        'FIX 2: Removed pexp from grammar \u2192 Keijzer-6 no longer trivially solved',\n        'FIX 3: Random ~99% on PIMA is expected (8 features = many terminals)',\n        'v4-1: POP_SIZE=500 (was 60) \u2014 GRAPE standard (de Lima et al. 2022)',\n        'v4-2: TOURN=7 (was 3) \u2014 appropriate for pop=500 (GRAPE paper)',\n        'v4-3: Feature normalisation on PIMA (z-score, fit on train) + zero imputation',\n        'v4-4: \u03b5-lexicase selection for classification (de Lima et al. 2022 \u00a74.3)',\n        'v4-5: Linear scaling before sigmoid (Murphy et al. 2023, Keijzer 2003)',\n        'v4-6: if_gt(a,b,c,d) added to classification grammar (Kishore et al. 2000)',\n    ],\n    'params': {\n        'pop': POP_SIZE, 'gens': MAX_GENS, 'cx': P_CX, 'mut': P_MUT,\n        'tourn': TOURN, 'elite': ELITE_SIZE,\n        'max_tree_depth': MAX_TREE_DEPTH, 'codon': CODON_SIZE,\n        'mapper': CODON_CONSUMPTION,\n        'init_depth': [MIN_INIT_DEPTH, MAX_INIT_DEPTH],\n        'init_gl': [MIN_INIT_GL, MAX_INIT_GL],\n    },\n    'grammar': '{+, -, *, pdiv, sin, tanh, plog, psqrt} + ephemeral constants (+ if_gt for classification)',\n    'lexicase': USE_LEXICASE,\n    'linear_scaling': USE_LIN_SCALE,\n    'epsilon_lexicase': EPSILON_LEXICASE,\n    'methods': METHODS, 'runs': N_RUNS, 'seed': SEED, 'seconds': total_t,\n    'problems': {pn: {'desc': pi['desc'], 'task': pi['task']}\n                 for pn, pi in PROBLEMS.items()},\n}\nwith open('experiment_config_v4_pima.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(f\"\\n{'='*72}\")\nprint(f\" DONE \u2014 {total_t/60:.1f} min\")\nprint(f\" Lexicase (classification): {USE_LEXICASE} | Linear scaling: {USE_LIN_SCALE}\")\nprint(f\" Parameters: Pop={POP_SIZE} Gens={MAX_GENS} Cx={P_CX} Mut={P_MUT} \"\n      f\"Tourn={TOURN} Elite={ELITE_SIZE}\")\nprint(f\" Mapper: {CODON_CONSUMPTION} | MaxDepth: {MAX_TREE_DEPTH}\")\nprint(f\"{'='*72}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}