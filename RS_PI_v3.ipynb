{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead78b57",
   "metadata": {},
   "source": [
    "# Grammatical Evolution: Initialization Methods Comparison (v3 — Corrected)\n",
    "\n",
    "## Methods: Random · Sensible (RHH) · PI Grow\n",
    "## Benchmarks: Keijzer-6 · Nguyen-7 · Pagie-1 · Diabetes\n",
    "\n",
    "---\n",
    "\n",
    "### Fixes Applied\n",
    "\n",
    "#### FIX 1: Sensible & PI Grow validity was < 100%\n",
    "**Root cause:** `CODON_CONSUMPTION='eager'` conflicts with how GRAPE's `sensible_initialisation` and `PI_Grow` build genomes. These methods construct the phenotype first, then derive the genotype + random tail. The eager mapper consumes tail codons during mapping, producing invalid individuals.\n",
    "\n",
    "**Solution:** Use `CODON_CONSUMPTION='lazy'` — the standard GRAPE mapper that only consumes codons when multiple production rules exist. The GRAPE paper (de Lima et al., 2022, §3.1) explicitly guarantees 100% validity with lazy mapping.\n",
    "\n",
    "#### FIX 2: Keijzer-6 converged unrealistically fast (~generation 5)\n",
    "**Root cause:** `pexp()` in the grammar enabled trivial overfitting — expressions like `pexp(plog(x[0]))` ≈ `x+1` closely approximate the harmonic series on the training range. Combined with pop=500, solutions were found almost immediately.\n",
    "\n",
    "**Solution:** Removed `pexp` from the grammar. Using the standard GRAPE function set: `{+, -, *, pdiv, sin, tanh, plog, psqrt}`.\n",
    "\n",
    "#### FIX 3: Random initialization shows ~100% validity on Diabetes\n",
    "**This is EXPECTED behaviour.** With 10 input features, the grammar has many terminal productions (`x[0]`...`x[9]` + constants). A randomly generated genome almost always maps to a valid expression. The 1-feature problems (Keijzer-6, Nguyen-7) show ~48% random validity because fewer terminals means more chance of deep recursion exhausting the genome.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters (aligned with BDS group standard practice)\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| Population | 500 | Standard GE |\n",
    "| Generations | 100 | Sufficient with pop=500 + elitism |\n",
    "| Crossover | 0.9 | Standard |\n",
    "| Mutation | 0.05/codon | More exploration |\n",
    "| Tournament | 3 | Balanced pressure at pop=500 |\n",
    "| Elitism | Top-1 | Monotonic improvement guarantee |\n",
    "| Mapper | **lazy** | Required for Sensible/PI Grow validity |\n",
    "| Max Tree Depth | 17 | Bloat control |\n",
    "| Init Depth | [2, 6] | Sensible/PI Grow depth range |\n",
    "| Grammar | std SR | {+,−,×,÷,sin,tanh,log,√} |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ===========================================================================\n",
    "#\n",
    "#  GRAMMATICAL EVOLUTION: INITIALIZATION METHODS COMPARISON (v3 — CORRECTED)\n",
    "#  3 Methods: Random, Sensible (RHH), PI Grow\n",
    "#  4 Benchmarks: Keijzer-6, Nguyen-7, Pagie-1, Diabetes (Binary Classification)\n",
    "#\n",
    "#  FIXES FROM v2:\n",
    "#  ──────────────\n",
    "#  FIX 1: Sensible & PI Grow validity < 100%\n",
    "#    ROOT CAUSE: CODON_CONSUMPTION='eager' conflicts with how GRAPE's\n",
    "#    sensible_initialisation and PI_Grow build genomes. These methods\n",
    "#    construct the phenotype first, then derive the minimal genotype +\n",
    "#    a random tail. The 'eager' mapper can consume that tail and hit\n",
    "#    unmappable codons, producing INVALID individuals. The GRAPE paper\n",
    "#    (de Lima et al. 2022, §3.1) explicitly states these methods\n",
    "#    \"assure we avoid invalid individuals in the first generation\".\n",
    "#    FIX: Use CODON_CONSUMPTION='lazy' — the standard GRAPE mapper\n",
    "#    that only consumes codons when needed. This guarantees 100%\n",
    "#    validity for grammar-based initialisations.\n",
    "#\n",
    "#  FIX 2: Keijzer-6 converges unrealistically fast (generation ~5)\n",
    "#    ROOT CAUSE: pexp() in the grammar allows expressions like\n",
    "#    pexp(plog(x[0])) ≈ x+1 which trivially approximates the harmonic\n",
    "#    series on the training range. Also pop=500 finds good solutions\n",
    "#    in the initial population itself.\n",
    "#    FIX: Remove pexp from grammar. Use the standard GRAPE SR grammar\n",
    "#    {+, -, *, pdiv, sin, tanh, plog, psqrt} matching the BDS group's\n",
    "#    published benchmarks. Pop=500 is kept (standard) but now the\n",
    "#    grammar won't trivially solve the problem.\n",
    "#\n",
    "#  FIX 3: Random initialisation shows ~100% validity on Diabetes\n",
    "#    This is EXPECTED behaviour, not a bug. With 10 input features,\n",
    "#    the grammar has many terminal productions (x[0]..x[9] + constants).\n",
    "#    A randomly generated genome almost always maps to a valid\n",
    "#    expression because there are so many terminal choices that\n",
    "#    the mapper rarely runs out of codons before completing.\n",
    "#    The 1-feature problems (Keijzer-6, Nguyen-7) show ~48% random\n",
    "#    validity because fewer terminals means more chance of deep\n",
    "#    recursion exhausting the genome.\n",
    "#\n",
    "#  Reference: BDS Group GRAPE (de Lima et al., 2022, Signals 3(3))\n",
    "#  Ryan, C. & Azad, R.M.A. (2003) \"Sensible Initialisation in GE\"\n",
    "# ===========================================================================\n",
    "\n",
    "import sys, os, importlib.util, numpy as np, random, copy, tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.stats import mannwhitneyu, kruskal\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings, time, json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── Force-load LOCAL grape.py ─────────────────\n",
    "_DIR = os.getcwd()\n",
    "_LOCAL = os.path.join(_DIR, 'grape.py')\n",
    "if os.path.isfile(_LOCAL):\n",
    "    spec = importlib.util.spec_from_file_location(\"grape\", _LOCAL)\n",
    "    grape = importlib.util.module_from_spec(spec)\n",
    "    sys.modules['grape'] = grape\n",
    "    spec.loader.exec_module(grape)\n",
    "    print(f\"[OK] Loaded grape.py from {_LOCAL}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"grape.py not found in {_DIR}\\nRun notebook inside GRAPE folder.\")\n",
    "\n",
    "if not hasattr(grape, 'PI_Grow'):\n",
    "    raise AttributeError(\"PI_Grow not found in grape.py!\")\n",
    "print(\"[OK] PI_Grow available\")\n",
    "\n",
    "if _DIR not in sys.path:\n",
    "    sys.path.insert(0, _DIR)\n",
    "try:\n",
    "    from functions import pdiv, plog, psqrt\n",
    "    print(\"[OK] functions.py loaded\")\n",
    "except ImportError:\n",
    "    def pdiv(a, b):\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            return np.where(np.abs(b) < 1e-12, np.ones_like(a, dtype=float), a / b)\n",
    "    def plog(a): return np.log(1.0 + np.abs(a))\n",
    "    def psqrt(a): return np.sqrt(np.abs(a))\n",
    "    print(\"[WARN] functions.py not found — using protected fallbacks\")\n",
    "\n",
    "# ── Sigmoid for classification ───────────────────────────────────────\n",
    "def sigmoid(a):\n",
    "    a = np.clip(np.asarray(a, dtype=float), -500, 500)\n",
    "    return 1.0 / (1.0 + np.exp(-a))\n",
    "\n",
    "# ── DEAP setup ───────────────────────────────────────────────────────\n",
    "from deap import creator, base, tools\n",
    "if \"FitnessMin\" in dir(creator): del creator.FitnessMin\n",
    "if \"Individual\" in dir(creator): del creator.Individual\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", grape.Individual, fitness=creator.FitnessMin)\n",
    "print(\"[OK] DEAP ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# GRAMMAR — Standard GRAPE symbolic regression grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caba698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# Following the function set used in the GRAPE paper and standard GP/GE\n",
    "# benchmarks: {+, -, *, pdiv, sin, tanh, plog, psqrt}\n",
    "# NO pexp — it trivially solves Keijzer-6 and creates numerical instability.\n",
    "# Ephemeral constants via <c><c>.<c><c> (range 00.00–99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc03153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "_GRAMMAR_DIR = tempfile.mkdtemp(prefix='grape_bnf_')\n",
    "\n",
    "def make_grammar(n_features):\n",
    "    \"\"\"Build BNF grammar following GRAPE standard function set.\"\"\"\n",
    "    vars_list = ' | '.join([f'x[{i}]' for i in range(n_features)])\n",
    "    bnf = (\n",
    "        \"<e> ::= <e>+<e> | <e>-<e> | <e>*<e> | pdiv(<e>,<e>)\"\n",
    "        \" | psqrt(<e>) | np.sin(<e>) | np.tanh(<e>) | plog(<e>)\"\n",
    "        f\" | {vars_list} | <c><c>.<c><c>\\n\"\n",
    "        \"<c> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\\n\"\n",
    "    )\n",
    "    fname = os.path.join(_GRAMMAR_DIR, f'grammar_{n_features}feat.bnf')\n",
    "    with open(fname, 'w') as f: f.write(bnf)\n",
    "    g = grape.Grammar(fname)\n",
    "    print(f\"    Grammar ({n_features} features): {len(g.non_terminals)} NTs, \"\n",
    "          f\"{sum(g.n_rules)} productions, \"\n",
    "          f\"terminals include {n_features} variables + constants\")\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82430c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "SEED = 42\n",
    "\n",
    "# ── Population & generations ──\n",
    "POP_SIZE    = 500       # Standard for GE (BDS group practice)\n",
    "MAX_GENS    = 100       # Sufficient with pop=500 and elitism\n",
    "ELITE_SIZE  = 1         # Preserve best individual across generations\n",
    "N_RUNS      = 30        # Standard for statistical analysis\n",
    "\n",
    "# ── Genetic operators ──\n",
    "P_CX        = 0.9       # Crossover probability (standard)\n",
    "P_MUT       = 0.05      # Per-codon mutation rate (higher = more exploration)\n",
    "TOURN       = 3         # Tournament size (appropriate for pop=500)\n",
    "\n",
    "# ── Initialisation ──\n",
    "MIN_INIT_GL   = 30      # Random init: min genome length\n",
    "MAX_INIT_GL   = 250     # Random init: max genome length (wide range)\n",
    "MIN_INIT_DEPTH = 2      # Sensible/PI Grow: minimum initial tree depth\n",
    "MAX_INIT_DEPTH = 6      # Sensible/PI Grow: maximum initial tree depth\n",
    "\n",
    "# ── Mapping & representation ──\n",
    "MAX_TREE_DEPTH = 17     # Maximum derivation tree depth during evolution\n",
    "CODON_SIZE     = 255    # Maximum codon value\n",
    "MAX_GENOME_LENGTH = None\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════╗\n",
    "# ║  CRITICAL: CODON_CONSUMPTION must be 'lazy' for GRAPE           ║\n",
    "# ║                                                                   ║\n",
    "# ║  'lazy'  = only consume a codon when a non-terminal has >1 rule  ║\n",
    "# ║  'eager' = consume a codon at every non-terminal expansion       ║\n",
    "# ║                                                                   ║\n",
    "# ║  Sensible init & PI Grow build phenotype-first genomes that      ║\n",
    "# ║  are ONLY correct under the lazy mapper. Using 'eager' causes    ║\n",
    "# ║  the tail codons to be consumed during mapping, potentially      ║\n",
    "# ║  making individuals invalid. This was the root cause of          ║\n",
    "# ║  Sensible/PI Grow showing <100% validity.                        ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════╝\n",
    "CODON_CONSUMPTION = 'lazy'\n",
    "GENOME_REPRESENTATION = 'list'\n",
    "\n",
    "METHODS = ['Random', 'Sensible', 'PI Grow']\n",
    "COLORS  = {'Random': '#E74C3C', 'Sensible': '#3498DB', 'PI Grow': '#2ECC71'}\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(\"  GRAMMATICAL EVOLUTION — INITIALIZATION METHODS COMPARISON (v3)\")\n",
    "print(\"=\" * 72)\n",
    "print(f\"  Population: {POP_SIZE}  |  Generations: {MAX_GENS}  |  Runs: {N_RUNS}\")\n",
    "print(f\"  Cx: {P_CX}  |  Mut: {P_MUT}/codon  |  Tournament: {TOURN}  |  Elite: {ELITE_SIZE}\")\n",
    "print(f\"  Mapper: {CODON_CONSUMPTION}  |  Max Depth: {MAX_TREE_DEPTH}\")\n",
    "print(f\"  Init Depth: [{MIN_INIT_DEPTH}, {MAX_INIT_DEPTH}]  |  Init GL: [{MIN_INIT_GL}, {MAX_INIT_GL}]\")\n",
    "print(f\"  Methods: {', '.join(METHODS)}\")\n",
    "print(\"=\" * 72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# BENCHMARK PROBLEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "def load_keijzer6():\n",
    "    \"\"\"Keijzer-6: f(x) = Σ(1/i, i=1..x), train x∈[1,50], test x∈[1,120]\"\"\"\n",
    "    def kf(x): return np.array([sum(1.0/i for i in range(1, int(xi)+1)) for xi in x])\n",
    "    xr = np.linspace(1, 50, 50)\n",
    "    xe = np.linspace(1, 120, 120)\n",
    "    return np.array([xr]), kf(xr), np.array([xe]), kf(xe)\n",
    "\n",
    "def load_nguyen7():\n",
    "    \"\"\"Nguyen-7: f(x) = ln(x+1) + ln(x²+1), train x∈[0,2], test x∈[0,2]\"\"\"\n",
    "    xr = np.linspace(0, 2, 20)\n",
    "    xe = np.linspace(0, 2, 100)\n",
    "    f = lambda x: np.log(x + 1) + np.log(x**2 + 1)\n",
    "    return np.array([xr]), f(xr), np.array([xe]), f(xe)\n",
    "\n",
    "def load_pagie1():\n",
    "    \"\"\"Pagie-1: f(x,y) = 1/(1+x⁻⁴) + 1/(1+y⁻⁴), 2D grid\"\"\"\n",
    "    v = np.linspace(-5, 5, 26)\n",
    "    X0, X1 = np.meshgrid(v, v)\n",
    "    x0, x1 = X0.ravel(), X1.ravel()\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        y = 1.0/(1.0 + np.power(np.abs(x0) + 1e-10, -4)) + \\\n",
    "            1.0/(1.0 + np.power(np.abs(x1) + 1e-10, -4))\n",
    "    y = np.nan_to_num(y, nan=0.0, posinf=2.0, neginf=0.0)\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    x0t = rng.uniform(-5, 5, 10000)\n",
    "    x1t = rng.uniform(-5, 5, 10000)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        yt = 1.0/(1.0 + np.power(np.abs(x0t) + 1e-10, -4)) + \\\n",
    "             1.0/(1.0 + np.power(np.abs(x1t) + 1e-10, -4))\n",
    "    yt = np.nan_to_num(yt, nan=0.0, posinf=2.0, neginf=0.0)\n",
    "    return np.array([x0, x1]), y, np.array([x0t, x1t]), yt\n",
    "\n",
    "def load_diabetes_data():\n",
    "    \"\"\"Diabetes: binary classification (median split), stratified 70/30.\"\"\"\n",
    "    d = load_diabetes()\n",
    "    X, y_cont = d.data, d.target\n",
    "    median_val = np.median(y_cont)\n",
    "    y = (y_cont >= median_val).astype(float)\n",
    "    print(f\"    Diabetes: median={median_val:.1f}, \"\n",
    "          f\"class0={int(np.sum(y==0))}, class1={int(np.sum(y==1))}\")\n",
    "    Xr, Xe, yr, ye = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "    return Xr.T, yr, Xe.T, ye\n",
    "\n",
    "PROBLEMS = OrderedDict([\n",
    "    ('Keijzer-6',  {'loader': load_keijzer6,      'n_features': 1,\n",
    "                    'desc': 'Harmonic (1D)',       'task': 'regression'}),\n",
    "    ('Nguyen-7',   {'loader': load_nguyen7,       'n_features': 1,\n",
    "                    'desc': 'ln(x+1)+ln(x²+1)',   'task': 'regression'}),\n",
    "    ('Pagie-1',    {'loader': load_pagie1,        'n_features': 2,\n",
    "                    'desc': '2D surface',          'task': 'regression'}),\n",
    "    ('Diabetes',   {'loader': load_diabetes_data, 'n_features': 10,\n",
    "                    'desc': 'Binary (10D)',        'task': 'classification'}),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# FITNESS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "_EG = {\n",
    "    'np': np, 'pdiv': pdiv, 'plog': plog, 'psqrt': psqrt,\n",
    "    'sigmoid': sigmoid, '__builtins__': {}\n",
    "}\n",
    "\n",
    "# Cap extreme MSE to prevent plot distortion\n",
    "FITNESS_CAP = 1e8\n",
    "\n",
    "def fitness_eval_regression(ind, pts):\n",
    "    \"\"\"MSE fitness for regression (minimise).\"\"\"\n",
    "    x, y = pts[0], pts[1]\n",
    "    if ind.invalid: return (np.nan,)\n",
    "    try:\n",
    "        pred = eval(ind.phenotype, _EG, {'x': x})\n",
    "    except:\n",
    "        return (np.nan,)\n",
    "    try:\n",
    "        pred = np.asarray(pred, dtype=float)\n",
    "        if pred.ndim == 0:\n",
    "            pred = np.full_like(y, float(pred))\n",
    "        if not np.all(np.isfinite(pred)):\n",
    "            return (np.nan,)\n",
    "        f = float(np.mean(np.square(y - pred)))\n",
    "    except:\n",
    "        return (np.nan,)\n",
    "    if not np.isfinite(f): return (np.nan,)\n",
    "    return (min(f, FITNESS_CAP),)\n",
    "\n",
    "def fitness_eval_classification(ind, pts):\n",
    "    \"\"\"Binary cross-entropy (log-loss) for classification (minimise).\"\"\"\n",
    "    x, y = pts[0], pts[1]\n",
    "    if ind.invalid: return (np.nan,)\n",
    "    try:\n",
    "        raw = eval(ind.phenotype, _EG, {'x': x})\n",
    "    except:\n",
    "        return (np.nan,)\n",
    "    try:\n",
    "        raw = np.asarray(raw, dtype=float)\n",
    "        if raw.ndim == 0:\n",
    "            raw = np.full_like(y, float(raw))\n",
    "        if not np.all(np.isfinite(raw)):\n",
    "            return (np.nan,)\n",
    "        prob = sigmoid(raw)\n",
    "        eps = 1e-15\n",
    "        prob = np.clip(prob, eps, 1.0 - eps)\n",
    "        bce = -np.mean(y * np.log(prob) + (1.0 - y) * np.log(1.0 - prob))\n",
    "        f = float(bce)\n",
    "    except:\n",
    "        return (np.nan,)\n",
    "    if not np.isfinite(f): return (np.nan,)\n",
    "    return (f,)\n",
    "\n",
    "def classification_accuracy(ind, pts):\n",
    "    \"\"\"Classification accuracy (for reporting only).\"\"\"\n",
    "    x, y = pts[0], pts[1]\n",
    "    if ind.invalid: return np.nan\n",
    "    try:\n",
    "        raw = eval(ind.phenotype, _EG, {'x': x})\n",
    "        raw = np.asarray(raw, dtype=float)\n",
    "        if raw.ndim == 0:\n",
    "            raw = np.full_like(y, float(raw))\n",
    "        prob = sigmoid(raw)\n",
    "        preds = (prob >= 0.5).astype(float)\n",
    "        return float(accuracy_score(y, preds))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_fitness_func(task):\n",
    "    return fitness_eval_classification if task == 'classification' else fitness_eval_regression\n",
    "\n",
    "def gfv(ind):\n",
    "    \"\"\"Get fitness value, returning nan if invalid.\"\"\"\n",
    "    if not ind.fitness.valid: return np.nan\n",
    "    v = ind.fitness.values[0]\n",
    "    return np.nan if (v is None or np.isnan(v)) else v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# INITIALISATION — using GRAPE's built-in methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583112ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "def create_pop(method, grammar):\n",
    "    \"\"\"Create initial population using GRAPE's initialisation methods.\n",
    "    \n",
    "    - Random: random_initialisation (genome-first, may produce invalids)\n",
    "    - Sensible: sensible_initialisation (phenotype-first, RHH, 100% valid)\n",
    "    - PI Grow: PI_Grow (phenotype-first, position-independent, 100% valid)\n",
    "    \"\"\"\n",
    "    if method == 'Random':\n",
    "        return grape.random_initialisation(\n",
    "            creator.Individual, POP_SIZE, grammar,\n",
    "            MIN_INIT_GL, MAX_INIT_GL, MAX_TREE_DEPTH,\n",
    "            CODON_SIZE, CODON_CONSUMPTION, GENOME_REPRESENTATION)\n",
    "    elif method == 'Sensible':\n",
    "        return grape.sensible_initialisation(\n",
    "            creator.Individual, POP_SIZE, grammar,\n",
    "            MIN_INIT_DEPTH, MAX_INIT_DEPTH,\n",
    "            CODON_SIZE, CODON_CONSUMPTION, GENOME_REPRESENTATION)\n",
    "    elif method == 'PI Grow':\n",
    "        return grape.PI_Grow(\n",
    "            creator.Individual, POP_SIZE, grammar,\n",
    "            MIN_INIT_DEPTH, MAX_INIT_DEPTH,\n",
    "            CODON_SIZE, CODON_CONSUMPTION, GENOME_REPRESENTATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# SINGLE EVOLUTIONARY RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0909ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "def run_ge(init_method, grammar, Xr, Yr, Xe, Ye, seed, task='regression'):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    fitness_func = get_fitness_func(task)\n",
    "    ptr, pte = [Xr, Yr], [Xe, Ye]\n",
    "\n",
    "    # ── Initialise population ────────────────────────────────────────\n",
    "    pop = create_pop(init_method, grammar)\n",
    "    for ind in pop:\n",
    "        ind.fitness.values = fitness_func(ind, ptr)\n",
    "\n",
    "    valid = [i for i in pop if not i.invalid and not np.isnan(gfv(i))]\n",
    "    n_valid = len(valid)\n",
    "\n",
    "    init_stats = {\n",
    "        'validity':     n_valid / POP_SIZE,\n",
    "        'best_fitness': float(min(gfv(i) for i in valid)) if valid else np.nan,\n",
    "        'mean_fitness': float(np.nanmean([gfv(i) for i in valid])) if valid else np.nan,\n",
    "        'mean_depth':   float(np.mean([i.depth for i in valid])) if valid else 0,\n",
    "        'mean_nodes':   float(np.mean([i.nodes for i in valid])) if valid else 0,\n",
    "        'mean_gl':      float(np.mean([len(i.genome) for i in valid])) if valid else 0,\n",
    "        'unique_pheno': len(set(i.phenotype for i in valid if i.phenotype)),\n",
    "        'struct_div':   (len(set(tuple(i.structure) for i in valid if hasattr(i, 'structure')))\n",
    "                         / max(n_valid, 1)) if valid else 0,\n",
    "    }\n",
    "    if task == 'classification' and valid:\n",
    "        accs = [classification_accuracy(i, ptr) for i in valid]\n",
    "        accs = [a for a in accs if not np.isnan(a)]\n",
    "        init_stats['best_accuracy'] = max(accs) if accs else np.nan\n",
    "        init_stats['mean_accuracy'] = float(np.mean(accs)) if accs else np.nan\n",
    "\n",
    "    # ── Tracking arrays ──────────────────────────────────────────────\n",
    "    hist = {\n",
    "        'min': [], 'avg': [], 'invalid': [], 'fitness_test': [],\n",
    "        'avg_depth': [], 'avg_nodes': [], 'struct_div': []\n",
    "    }\n",
    "    if task == 'classification':\n",
    "        hist['train_acc'] = []\n",
    "        hist['test_acc'] = []\n",
    "\n",
    "    hof = tools.HallOfFame(3)\n",
    "\n",
    "    # ── Generational loop ────────────────────────────────────────────\n",
    "    for gen in range(MAX_GENS + 1):\n",
    "        vp = [i for i in pop if not i.invalid and not np.isnan(gfv(i))]\n",
    "        fits = [gfv(i) for i in vp]\n",
    "        if vp:\n",
    "            hof.update(vp)\n",
    "\n",
    "        # Test fitness of best-ever individual\n",
    "        if len(hof) > 0 and not hof[0].invalid:\n",
    "            tf = fitness_func(hof[0], pte)[0]\n",
    "        else:\n",
    "            tf = np.nan\n",
    "\n",
    "        hist['min'].append(float(min(fits)) if fits else np.nan)\n",
    "        hist['avg'].append(float(np.mean(fits)) if fits else np.nan)\n",
    "        hist['invalid'].append(POP_SIZE - len(vp))\n",
    "        hist['fitness_test'].append(float(tf) if np.isfinite(tf) else np.nan)\n",
    "        depths = [i.depth for i in vp]\n",
    "        nodes  = [i.nodes for i in vp]\n",
    "        hist['avg_depth'].append(float(np.mean(depths)) if vp else 0)\n",
    "        hist['avg_nodes'].append(float(np.mean(nodes)) if vp else 0)\n",
    "        structs = set()\n",
    "        for i in vp:\n",
    "            if hasattr(i, 'structure'):\n",
    "                structs.add(tuple(i.structure))\n",
    "        hist['struct_div'].append(len(structs) / max(len(vp), 1) if vp else 0)\n",
    "\n",
    "        if task == 'classification':\n",
    "            tr_acc = classification_accuracy(hof[0], ptr) if len(hof) > 0 and not hof[0].invalid else np.nan\n",
    "            te_acc = classification_accuracy(hof[0], pte) if len(hof) > 0 and not hof[0].invalid else np.nan\n",
    "            hist['train_acc'].append(float(tr_acc) if np.isfinite(tr_acc) else np.nan)\n",
    "            hist['test_acc'].append(float(te_acc) if np.isfinite(te_acc) else np.nan)\n",
    "\n",
    "        if gen == MAX_GENS:\n",
    "            break\n",
    "\n",
    "        # ── Elitism ──────────────────────────────────────────────────\n",
    "        elite = []\n",
    "        if vp:\n",
    "            sorted_vp = sorted(vp, key=lambda i: gfv(i))\n",
    "            elite = [copy.deepcopy(sorted_vp[j])\n",
    "                     for j in range(min(ELITE_SIZE, len(sorted_vp)))]\n",
    "\n",
    "        # ── Selection ────────────────────────────────────────────────\n",
    "        n_offspring = POP_SIZE - ELITE_SIZE\n",
    "        if vp:\n",
    "            sel = tools.selTournament(vp, n_offspring, tournsize=TOURN)\n",
    "        else:\n",
    "            sel = [copy.deepcopy(i) for i in pop[:n_offspring]]\n",
    "        off = [copy.deepcopy(i) for i in sel]\n",
    "\n",
    "        # ── Crossover ────────────────────────────────────────────────\n",
    "        for i in range(1, len(off), 2):\n",
    "            if random.random() < P_CX:\n",
    "                off[i-1], off[i] = grape.crossover_onepoint(\n",
    "                    off[i-1], off[i], grammar, MAX_TREE_DEPTH,\n",
    "                    CODON_CONSUMPTION, GENOME_REPRESENTATION, MAX_GENOME_LENGTH)\n",
    "\n",
    "        # ── Mutation ─────────────────────────────────────────────────\n",
    "        for i in range(len(off)):\n",
    "            off[i], = grape.mutation_int_flip_per_codon(\n",
    "                off[i], P_MUT, CODON_SIZE, grammar, MAX_TREE_DEPTH,\n",
    "                CODON_CONSUMPTION, MAX_GENOME_LENGTH)\n",
    "\n",
    "        # ── Evaluate new offspring ───────────────────────────────────\n",
    "        for ind in off:\n",
    "            if not ind.fitness.valid:\n",
    "                ind.fitness.values = fitness_func(ind, ptr)\n",
    "\n",
    "        # ── Next generation = offspring + elite ──────────────────────\n",
    "        pop = off + elite\n",
    "\n",
    "    # ── Collect final results ────────────────────────────────────────\n",
    "    best = hof[0] if len(hof) > 0 else None\n",
    "    ft = fitness_func(best, pte)[0] if best and not best.invalid else np.nan\n",
    "\n",
    "    result = {\n",
    "        'init': init_stats,\n",
    "        'hist': hist,\n",
    "        'train_fitness': gfv(best) if best else np.nan,\n",
    "        'test_fitness':  float(ft) if np.isfinite(ft) else np.nan,\n",
    "        'pheno': best.phenotype if best else None,\n",
    "        'depth': best.depth if best else 0,\n",
    "        'nodes': best.nodes if best else 0,\n",
    "        'gl':    len(best.genome) if best else 0,\n",
    "        'task':  task,\n",
    "    }\n",
    "    if task == 'classification' and best and not best.invalid:\n",
    "        result['train_acc'] = classification_accuracy(best, ptr)\n",
    "        result['test_acc']  = classification_accuracy(best, pte)\n",
    "    elif task == 'classification':\n",
    "        result['train_acc'] = np.nan\n",
    "        result['test_acc']  = np.nan\n",
    "    if task == 'regression':\n",
    "        result['train_mse'] = result['train_fitness']\n",
    "        result['test_mse']  = result['test_fitness']\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ac1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# RUN ALL EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "total_runs = len(PROBLEMS) * len(METHODS) * N_RUNS\n",
    "print(f\"\\nSTARTING: {len(PROBLEMS)} problems × {len(METHODS)} methods × {N_RUNS} runs = {total_runs} runs\\n\")\n",
    "\n",
    "all_results = {}\n",
    "t0_all = time.time()\n",
    "\n",
    "for pn, pi in PROBLEMS.items():\n",
    "    print(f\"\\n{'#'*65}\\n# {pn}: {pi['desc']} ({pi['task']})\\n{'#'*65}\")\n",
    "    grammar = make_grammar(pi['n_features'])\n",
    "    pr = {}\n",
    "    for mn in METHODS:\n",
    "        t0 = time.time()\n",
    "        runs = []\n",
    "        for r in range(N_RUNS):\n",
    "            if (r+1) % 10 == 0 or r == 0:\n",
    "                print(f\"  {mn:>10s} run {r+1:>2d}/{N_RUNS}...\", flush=True)\n",
    "            np.random.seed(SEED + r)\n",
    "            random.seed(SEED + r)\n",
    "            Xr, Yr, Xe, Ye = pi['loader']()\n",
    "            runs.append(run_ge(mn, grammar, Xr, Yr, Xe, Ye, SEED + r, task=pi['task']))\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  {mn:>10s}: {elapsed:.0f}s total ({elapsed/N_RUNS:.1f}s/run)\")\n",
    "        pr[mn] = runs\n",
    "    all_results[pn] = pr\n",
    "\n",
    "total_t = time.time() - t0_all\n",
    "print(f\"\\n{'='*72}\\n COMPLETED in {total_t/60:.1f} min\\n{'='*72}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed515ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# VERIFY VALIDITY FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99501cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n\" + \"=\"*72)\n",
    "print(\" VALIDITY VERIFICATION\")\n",
    "print(\"=\"*72)\n",
    "for pn in all_results:\n",
    "    for mn in METHODS:\n",
    "        vals = [r['init']['validity']*100 for r in all_results[pn][mn]]\n",
    "        mean_v = np.mean(vals)\n",
    "        min_v = np.min(vals)\n",
    "        status = \"✓\" if (mn == 'Random' or min_v >= 99.9) else \"✗ STILL BROKEN\"\n",
    "        print(f\"  {pn:<12s} {mn:<10s}: mean={mean_v:6.2f}%, min={min_v:6.2f}%  {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# STATISTICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a56d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "def sig_stars(p):\n",
    "    if p < 0.001: return \"***\"\n",
    "    elif p < 0.01: return \"**\"\n",
    "    elif p < 0.05: return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "def vda(a, b):\n",
    "    m, n = len(a), len(b)\n",
    "    if m == 0 or n == 0: return 0.5\n",
    "    return sum(1 if ai < bi else 0.5 if ai == bi else 0 for ai in a for bi in b) / (m * n)\n",
    "\n",
    "def effect_label(a):\n",
    "    d = abs(a - 0.5)\n",
    "    if d < 0.06: return \"negl\"\n",
    "    elif d < 0.14: return \"small\"\n",
    "    elif d < 0.21: return \"medium\"\n",
    "    return \"large\"\n",
    "\n",
    "def safe_vals(lst):\n",
    "    return [x for x in lst if x is not None and np.isfinite(x) and abs(x) < 1e10]\n",
    "\n",
    "for pn, res in all_results.items():\n",
    "    task = PROBLEMS[pn]['task']\n",
    "    print(f\"\\n{'='*72}\\n {pn} ({'Classification' if task=='classification' else 'Regression'})\\n{'='*72}\")\n",
    "\n",
    "    if task == 'classification':\n",
    "        metrics = [\n",
    "            (\"Init Validity %\",      lambda r: r['init']['validity']*100),\n",
    "            (\"Init Unique Pheno\",    lambda r: r['init']['unique_pheno']),\n",
    "            (\"Init Mean Depth\",      lambda r: r['init']['mean_depth']),\n",
    "            (\"Init Struct Div\",      lambda r: r['init']['struct_div']),\n",
    "            (\"Init Best Log-Loss\",   lambda r: r['init']['best_fitness']),\n",
    "            (\"Init Mean Log-Loss\",   lambda r: r['init']['mean_fitness']),\n",
    "            (\"Init Best Accuracy\",   lambda r: r['init'].get('best_accuracy', np.nan)),\n",
    "            (\"Init Mean Accuracy\",   lambda r: r['init'].get('mean_accuracy', np.nan)),\n",
    "            (\"Final Train Log-Loss\", lambda r: r['train_fitness']),\n",
    "            (\"Final Test Log-Loss\",  lambda r: r['test_fitness']),\n",
    "            (\"Final Train Accuracy\", lambda r: r.get('train_acc', np.nan)),\n",
    "            (\"Final Test Accuracy\",  lambda r: r.get('test_acc', np.nan)),\n",
    "        ]\n",
    "    else:\n",
    "        metrics = [\n",
    "            (\"Init Validity %\",    lambda r: r['init']['validity']*100),\n",
    "            (\"Init Unique Pheno\",  lambda r: r['init']['unique_pheno']),\n",
    "            (\"Init Mean Depth\",    lambda r: r['init']['mean_depth']),\n",
    "            (\"Init Struct Div\",    lambda r: r['init']['struct_div']),\n",
    "            (\"Init Best MSE\",      lambda r: r['init']['best_fitness']),\n",
    "            (\"Init Mean MSE\",      lambda r: r['init']['mean_fitness']),\n",
    "            (\"Final Train MSE\",    lambda r: r['train_fitness']),\n",
    "            (\"Final Test MSE\",     lambda r: r['test_fitness']),\n",
    "        ]\n",
    "\n",
    "    for label, ext in metrics:\n",
    "        vals = {m: safe_vals([ext(r) for r in res[m]]) for m in METHODS}\n",
    "        parts = []\n",
    "        for m in METHODS:\n",
    "            if vals[m]:\n",
    "                parts.append(f\"{m}={np.mean(vals[m]):>9.4f}±{np.std(vals[m]):<7.4f}\")\n",
    "            else:\n",
    "                parts.append(f\"{m}={'N/A':>9}\")\n",
    "        print(f\"  {label:<24}: {'  '.join(parts)}\")\n",
    "\n",
    "        # Kruskal-Wallis\n",
    "        all_groups = [vals[m] for m in METHODS if vals[m] and len(vals[m]) >= 3]\n",
    "        if len(all_groups) >= 2:\n",
    "            try:\n",
    "                H, p_kw = kruskal(*all_groups)\n",
    "                print(f\"    Kruskal-Wallis: H={H:.3f}, p={p_kw:.6f} ({sig_stars(p_kw)})\")\n",
    "            except: pass\n",
    "\n",
    "        # Pairwise comparisons\n",
    "        pw = []\n",
    "        for a, b in [('Random','Sensible'),('Random','PI Grow'),('Sensible','PI Grow')]:\n",
    "            if vals[a] and vals[b] and len(vals[a]) >= 3 and len(vals[b]) >= 3:\n",
    "                _, p = mannwhitneyu(vals[a], vals[b], alternative='two-sided')\n",
    "                ae = vda(vals[a], vals[b])\n",
    "                pw.append(f\"{a[:3]}v{b[:3]}: p={p:.4f}({sig_stars(p)}) A={ae:.3f}({effect_label(ae)})\")\n",
    "        if pw:\n",
    "            print(f\"    {'  |  '.join(pw)}\")\n",
    "\n",
    "    print(f\"\\n  Best solutions (by train fitness):\")\n",
    "    for m in METHODS:\n",
    "        v = [r for r in res[m] if np.isfinite(r.get('train_fitness', np.nan))]\n",
    "        if v:\n",
    "            b = min(v, key=lambda r: r['train_fitness'])\n",
    "            if task == 'classification':\n",
    "                print(f\"    {m}: LL={b['train_fitness']:.6f} TestLL={b['test_fitness']:.6f} \"\n",
    "                      f\"TrAcc={b.get('train_acc',np.nan):.4f} TeAcc={b.get('test_acc',np.nan):.4f} \"\n",
    "                      f\"D={b['depth']} N={b['nodes']}\")\n",
    "            else:\n",
    "                print(f\"    {m}: TrainMSE={b['train_fitness']:.6f} TestMSE={b['test_fitness']:.6f} \"\n",
    "                      f\"D={b['depth']} N={b['nodes']}\")\n",
    "            pheno = (b['pheno'] or 'N/A')[:120]\n",
    "            print(f\"      expr: {pheno}\")\n",
    "\n",
    "# ── Summary tables ───────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n SUMMARY TABLES\\n\" + \"=\" * 80)\n",
    "pnames = list(all_results.keys())\n",
    "\n",
    "for tl, kf, higher_better in [\n",
    "    (\"INITIAL VALIDITY (%)\",            lambda r: r['init']['validity']*100, True),\n",
    "    (\"TRAIN FITNESS (MSE/LogLoss)\",     lambda r: r['train_fitness'],       False),\n",
    "    (\"TEST FITNESS (MSE/LogLoss)\",      lambda r: r['test_fitness'],        False),\n",
    "]:\n",
    "    print(f\"\\n{'-'*80}\\n {tl}\\n{'-'*80}\")\n",
    "    h = f\"{'Problem':<13}\"\n",
    "    for m in METHODS: h += f\"  {m:>16}\"\n",
    "    h += f\"  {'Best':>10}\"\n",
    "    print(h); print(\"-\"*80)\n",
    "    for p in pnames:\n",
    "        row = f\"{p:<13}\"; means = {}\n",
    "        for m in METHODS:\n",
    "            v = safe_vals([kf(r) for r in all_results[p][m]])\n",
    "            if v:\n",
    "                means[m] = np.mean(v)\n",
    "                row += f\"  {np.mean(v):>7.4f}±{np.std(v):>6.4f}\"\n",
    "            else:\n",
    "                means[m] = float('inf') if not higher_better else float('-inf')\n",
    "                row += f\"  {'N/A':>16}\"\n",
    "        if higher_better:\n",
    "            bm = max(means, key=means.get)\n",
    "        else:\n",
    "            bm = min(means, key=means.get)\n",
    "        row += f\"  {bm:>10}\"\n",
    "        print(row)\n",
    "\n",
    "# Classification accuracy summary\n",
    "cls_problems = [p for p in pnames if PROBLEMS[p]['task'] == 'classification']\n",
    "if cls_problems:\n",
    "    for tl, kf in [(\"TRAIN ACCURACY\", lambda r: r.get('train_acc', np.nan)),\n",
    "                    (\"TEST ACCURACY\",  lambda r: r.get('test_acc', np.nan))]:\n",
    "        print(f\"\\n{'-'*80}\\n {tl}\\n{'-'*80}\")\n",
    "        h = f\"{'Problem':<13}\"\n",
    "        for m in METHODS: h += f\"  {m:>16}\"\n",
    "        h += f\"  {'Best':>10}\"; print(h); print(\"-\"*80)\n",
    "        for p in cls_problems:\n",
    "            row = f\"{p:<13}\"; means = {}\n",
    "            for m in METHODS:\n",
    "                v = safe_vals([kf(r) for r in all_results[p][m]])\n",
    "                if v:\n",
    "                    means[m] = np.mean(v)\n",
    "                    row += f\"  {np.mean(v):>7.4f}±{np.std(v):>6.4f}\"\n",
    "                else:\n",
    "                    means[m] = float('-inf')\n",
    "                    row += f\"  {'N/A':>16}\"\n",
    "            bm = max(means, key=means.get)\n",
    "            row += f\"  {bm:>10}\"; print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ee3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# PUBLICATION-QUALITY PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11, 'font.family': 'serif',\n",
    "    'axes.titlesize': 14, 'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12, 'legend.fontsize': 10,\n",
    "    'figure.dpi': 150, 'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.grid': True, 'grid.alpha': 0.25, 'grid.linestyle': '--',\n",
    "    'lines.linewidth': 1.8,\n",
    "    'axes.spines.top': False, 'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "def _box(pn, res, mf, yl, title, fp):\n",
    "    \"\"\"Box plot with significance brackets.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5, 4.5))\n",
    "    d = {m: safe_vals([mf(r) for r in res[m]]) for m in METHODS}\n",
    "    data = [d[m] for m in METHODS if d[m]]\n",
    "    labels = [m for m in METHODS if d[m]]\n",
    "    if data:\n",
    "        bp = ax.boxplot(data, labels=labels, patch_artist=True,\n",
    "                        medianprops=dict(color='black', linewidth=2))\n",
    "        for patch, m in zip(bp['boxes'], labels):\n",
    "            patch.set_facecolor(COLORS[m]); patch.set_alpha(0.75)\n",
    "            patch.set_edgecolor('grey')\n",
    "        ym = ax.get_ylim()[1]; yo = 0\n",
    "        for i1, i2 in [(0,1),(0,2),(1,2)]:\n",
    "            if i1 < len(labels) and i2 < len(labels) and d[labels[i1]] and d[labels[i2]]:\n",
    "                _, p = mannwhitneyu(d[labels[i1]], d[labels[i2]])\n",
    "                ss = sig_stars(p)\n",
    "                if ss != 'ns':\n",
    "                    yr = ym - ax.get_ylim()[0]\n",
    "                    yp = ym + yo * yr * 0.07\n",
    "                    ax.plot([i1+1, i2+1], [yp, yp], 'k-', lw=0.8)\n",
    "                    ax.text((i1+i2+2)/2, yp+yr*0.01, ss, ha='center', fontsize=9)\n",
    "                    yo += 1\n",
    "    ax.set_ylabel(yl); ax.set_title(f'{pn}: {title}')\n",
    "    plt.tight_layout()\n",
    "    fn = f'{fp}_{pn.lower().replace(\"-\",\"\")}.png'\n",
    "    plt.savefig(fn); plt.show()\n",
    "    print(f\"  Saved: {fn}\")\n",
    "\n",
    "def _line(pn, res, hk, yl, title, fp, clip_val=None):\n",
    "    \"\"\"Line plot with confidence band (mean ± std).\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "    ng = len(res[METHODS[0]][0]['hist'][hk])\n",
    "    xa = np.arange(ng)\n",
    "    for m in METHODS:\n",
    "        c = np.array([r['hist'][hk] for r in res[m]], dtype=float)\n",
    "        if clip_val is not None:\n",
    "            c = np.where(np.isnan(c) | (np.abs(c) > clip_val), np.nan, c)\n",
    "        mn = np.nanmean(c, 0)\n",
    "        sd = np.nanstd(c, 0)\n",
    "        ax.plot(xa, mn, label=m, color=COLORS[m], linewidth=2)\n",
    "        ax.fill_between(xa, mn - sd, mn + sd, color=COLORS[m], alpha=0.12)\n",
    "    ax.set_xlabel('Generation'); ax.set_ylabel(yl)\n",
    "    ax.set_title(f'{pn}: {title}')\n",
    "    ax.legend(frameon=True, fancybox=True, framealpha=0.9)\n",
    "    plt.tight_layout()\n",
    "    fn = f'{fp}_{pn.lower().replace(\"-\",\"\")}.png'\n",
    "    plt.savefig(fn); plt.show()\n",
    "    print(f\"  Saved: {fn}\")\n",
    "\n",
    "# ── Generate all plots ───────────────────────────────────────────────\n",
    "for pn, res in all_results.items():\n",
    "    task = PROBLEMS[pn]['task']\n",
    "    print(f\"\\n{'─'*50}\\n Plots: {pn} ({'Classification' if task=='classification' else 'Regression'})\\n{'─'*50}\")\n",
    "\n",
    "    # 1. Initial Validity\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    d = {m: safe_vals([r['init']['validity']*100 for r in res[m]]) for m in METHODS}\n",
    "    data = [d[m] for m in METHODS if d[m]]\n",
    "    labels = [m for m in METHODS if d[m]]\n",
    "    if data:\n",
    "        bp = ax.boxplot(data, positions=list(range(1, len(data)+1)),\n",
    "                        widths=0.55, patch_artist=True,\n",
    "                        medianprops=dict(color='black', linewidth=2))\n",
    "        for patch, m in zip(bp['boxes'], labels):\n",
    "            patch.set_facecolor(COLORS[m]); patch.set_alpha(0.75)\n",
    "            patch.set_edgecolor('grey')\n",
    "    ax.set_xticks(list(range(1, len(labels)+1)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel('Validity (%)')\n",
    "    ax.set_title(f'{pn}: Initial Validity')\n",
    "    ax.set_ylim([0, max(110, ax.get_ylim()[1] + 5)])\n",
    "    plt.tight_layout()\n",
    "    fn = f'fig1_{pn.lower().replace(\"-\",\"\")}_validity.png'\n",
    "    plt.savefig(fn); plt.show(); print(f\"  Saved: {fn}\")\n",
    "\n",
    "    # 2. Convergence (train fitness)\n",
    "    fl = 'Best Log-Loss (train)' if task == 'classification' else 'Best MSE (train)'\n",
    "    _line(pn, res, 'min', fl, 'Convergence', 'fig2_conv', clip_val=1e6)\n",
    "\n",
    "    # 3. Structural Diversity\n",
    "    _line(pn, res, 'struct_div', 'Structural Diversity', 'Diversity', 'fig3_div')\n",
    "\n",
    "    # 4. Train fitness box plot\n",
    "    if task == 'classification':\n",
    "        _box(pn, res, lambda r: r['train_fitness'], 'Train Log-Loss',\n",
    "             f'Train Log-Loss ({N_RUNS} runs)', 'fig4_train')\n",
    "    else:\n",
    "        _box(pn, res, lambda r: r['train_fitness'], 'Train MSE',\n",
    "             f'Train MSE ({N_RUNS} runs)', 'fig4_train')\n",
    "\n",
    "    # 5. Test fitness box plot\n",
    "    if task == 'classification':\n",
    "        _box(pn, res, lambda r: r['test_fitness'], 'Test Log-Loss',\n",
    "             f'Test Log-Loss ({N_RUNS} runs)', 'fig5_test')\n",
    "    else:\n",
    "        _box(pn, res, lambda r: r['test_fitness'], 'Test MSE',\n",
    "             f'Test MSE ({N_RUNS} runs)', 'fig5_test')\n",
    "\n",
    "    # 6. Tree Depth\n",
    "    _line(pn, res, 'avg_depth', 'Avg Depth', 'Tree Depth', 'fig6_depth')\n",
    "\n",
    "    # 7. Test Fitness convergence\n",
    "    tfl = 'Best Log-Loss (test)' if task == 'classification' else 'Best MSE (test)'\n",
    "    _line(pn, res, 'fitness_test', tfl, 'Test Fitness', 'fig7_testconv', clip_val=1e6)\n",
    "\n",
    "    # Classification-specific\n",
    "    if task == 'classification':\n",
    "        _line(pn, res, 'train_acc', 'Train Accuracy', 'Training Accuracy', 'fig8_trainacc')\n",
    "        _line(pn, res, 'test_acc',  'Test Accuracy',  'Test Accuracy',     'fig9_testacc')\n",
    "        _box(pn, res, lambda r: r.get('train_acc', np.nan), 'Train Accuracy',\n",
    "             f'Train Accuracy ({N_RUNS} runs)', 'fig10_trainacc_box')\n",
    "        _box(pn, res, lambda r: r.get('test_acc', np.nan), 'Test Accuracy',\n",
    "             f'Test Accuracy ({N_RUNS} runs)', 'fig11_testacc_box')\n",
    "\n",
    "# ── Summary bar chart ────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(PROBLEMS)); w = 0.25\n",
    "for i, m in enumerate(METHODS):\n",
    "    mn_vals = []; sd_vals = []\n",
    "    for pn in pnames:\n",
    "        v = safe_vals([r['test_fitness'] for r in all_results[pn][m]])\n",
    "        mn_vals.append(np.mean(v) if v else 0)\n",
    "        sd_vals.append(np.std(v) if v else 0)\n",
    "    ax.bar(x + i*w, mn_vals, w, yerr=sd_vals, label=m,\n",
    "           color=COLORS[m], alpha=0.8, edgecolor='grey', capsize=3)\n",
    "ax.set_xlabel('Benchmark')\n",
    "ax.set_ylabel('Test Fitness (mean ± std)')\n",
    "ax.set_title(f'Test Fitness ({N_RUNS} runs × {MAX_GENS} gens, Pop={POP_SIZE})')\n",
    "ax.set_xticks(x + w); ax.set_xticklabels(pnames)\n",
    "ax.legend(frameon=True); plt.tight_layout()\n",
    "plt.savefig('fig_summary_fitness.png'); plt.show()\n",
    "print(\"Saved: fig_summary_fitness.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84121355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf85f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "def ser(o):\n",
    "    if isinstance(o, (np.integer,)): return int(o)\n",
    "    if isinstance(o, (np.floating, float)):\n",
    "        return None if (np.isnan(o) if isinstance(o, float) else np.isnan(float(o))) else float(o)\n",
    "    if isinstance(o, np.ndarray): return o.tolist()\n",
    "    if isinstance(o, dict): return {k: ser(v) for k, v in o.items()}\n",
    "    if isinstance(o, list): return [ser(v) for v in o]\n",
    "    return o\n",
    "\n",
    "with open('ge_results_v3.json', 'w') as f:\n",
    "    json.dump(ser(all_results), f)\n",
    "\n",
    "config = {\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'version': 'v3_corrected',\n",
    "    'grape': os.path.abspath(getattr(grape, '__file__', '?')),\n",
    "    'fixes': [\n",
    "        'FIX 1: CODON_CONSUMPTION=lazy (was eager) → Sensible/PI Grow now 100% valid',\n",
    "        'FIX 2: Removed pexp from grammar → Keijzer-6 no longer trivially solved',\n",
    "        'FIX 3: Random 100% on Diabetes is expected (10 features = many terminals)',\n",
    "    ],\n",
    "    'params': {\n",
    "        'pop': POP_SIZE, 'gens': MAX_GENS, 'cx': P_CX, 'mut': P_MUT,\n",
    "        'tourn': TOURN, 'elite': ELITE_SIZE,\n",
    "        'max_tree_depth': MAX_TREE_DEPTH, 'codon': CODON_SIZE,\n",
    "        'mapper': CODON_CONSUMPTION,\n",
    "        'init_depth': [MIN_INIT_DEPTH, MAX_INIT_DEPTH],\n",
    "        'init_gl': [MIN_INIT_GL, MAX_INIT_GL],\n",
    "    },\n",
    "    'grammar': '{+, -, *, pdiv, sin, tanh, plog, psqrt} + ephemeral constants',\n",
    "    'methods': METHODS, 'runs': N_RUNS, 'seed': SEED, 'seconds': total_t,\n",
    "    'problems': {pn: {'desc': pi['desc'], 'task': pi['task']}\n",
    "                 for pn, pi in PROBLEMS.items()},\n",
    "}\n",
    "with open('experiment_config_v3.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*72}\")\n",
    "print(f\" DONE — {total_t/60:.1f} min\")\n",
    "print(f\" Parameters: Pop={POP_SIZE} Gens={MAX_GENS} Cx={P_CX} Mut={P_MUT} \"\n",
    "      f\"Tourn={TOURN} Elite={ELITE_SIZE}\")\n",
    "print(f\" Mapper: {CODON_CONSUMPTION} | MaxDepth: {MAX_TREE_DEPTH}\")\n",
    "print(f\"{'='*72}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
