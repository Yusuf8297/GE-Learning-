{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score,\n",
    "    average_precision_score, make_scorer\n",
    ")\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib\n",
    "# Use TkAgg backend for interactive plots in VSCode\n",
    "# If TkAgg doesn't work, try 'Qt5Agg' or remove this line entirely\n",
    "try:\n",
    "    matplotlib.use('TkAgg')\n",
    "except:\n",
    "    pass  # Fall back to default backend\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Enable interactive mode for VSCode\n",
    "plt.ion()\n",
    "\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    import torch\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"[INFO] TabNet not available\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"[INFO] XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    SMOTE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SMOTE_AVAILABLE = False\n",
    "    print(\"[INFO] SMOTE not available\")\n",
    "\n",
    "N_JOBS = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GE-based HPO Framework - Modified Version\")\n",
    "print(f\"Available CPU cores for parallel processing: {N_JOBS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def compute_confidence_interval(data, confidence=0.95):\n",
    "    if len(data) < 2:\n",
    "        return (np.mean(data), np.mean(data))\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return (mean - h, mean + h)\n",
    "\n",
    "\n",
    "def compute_statistics(data):\n",
    "    if len(data) == 0:\n",
    "        return {'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'median': 0, \n",
    "                'ci_lower': 0, 'ci_upper': 0}\n",
    "    ci_lower, ci_upper = compute_confidence_interval(data)\n",
    "    return {\n",
    "        'mean': float(np.mean(data)),\n",
    "        'std': float(np.std(data)),\n",
    "        'min': float(np.min(data)),\n",
    "        'max': float(np.max(data)),\n",
    "        'median': float(np.median(data)),\n",
    "        'ci_lower': float(ci_lower),\n",
    "        'ci_upper': float(ci_upper)\n",
    "    }\n",
    "\n",
    "\n",
    "def paired_ttest(scores1, scores2):\n",
    "    if len(scores1) != len(scores2) or len(scores1) < 2:\n",
    "        return (np.nan, np.nan)\n",
    "    t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
    "    return (float(t_stat), float(p_value))\n",
    "\n",
    "\n",
    "def wilcoxon_test(scores1, scores2):\n",
    "    if len(scores1) != len(scores2) or len(scores1) < 2:\n",
    "        return (np.nan, np.nan)\n",
    "    try:\n",
    "        stat, p_value = stats.wilcoxon(scores1, scores2)\n",
    "        return (float(stat), float(p_value))\n",
    "    except Exception:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "def compute_sensitivity_specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    else:\n",
    "        sensitivity = specificity = ppv = npv = 0\n",
    "        tn = fp = fn = tp = 0\n",
    "    \n",
    "    return {\n",
    "        'sensitivity': float(sensitivity),\n",
    "        'specificity': float(specificity),\n",
    "        'ppv': float(ppv),\n",
    "        'npv': float(npv),\n",
    "        'tp': int(tp),\n",
    "        'tn': int(tn),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn)\n",
    "    }\n",
    "\n",
    "\n",
    "class HyperparameterPruningAnalyzer:\n",
    "    \n",
    "    DEFAULT_HYPERPARAMS = {\n",
    "        'TabNet': {\n",
    "            'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.3, 'lambda_sparse': 1e-3\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': 100, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, \n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8\n",
    "        },\n",
    "        'SVM': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'},\n",
    "        'LogisticRegression': {'C': 1.0, 'penalty': 'l2', 'max_iter': 1000},\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    SEARCH_SPACE = {\n",
    "        'TabNet': {\n",
    "            'n_d': [8, 16, 32, 64],\n",
    "            'n_a': [8, 16, 32, 64],\n",
    "            'n_steps': [3, 5, 7],\n",
    "            'lambda_sparse': [1e-4, 1e-3, 1e-2]\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [5, 10, 15, None]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pruning_results = {}\n",
    "    \n",
    "    def analyze_model(self, model_name, optimized_configs, best_config, \n",
    "                      default_fitness=None, optimized_fitness=None):\n",
    "        if model_name not in self.SEARCH_SPACE:\n",
    "            return None\n",
    "        \n",
    "        search_space = self.SEARCH_SPACE[model_name]\n",
    "        default_params = self.DEFAULT_HYPERPARAMS.get(model_name, {})\n",
    "        \n",
    "        total_combinations = 1\n",
    "        for param, values in search_space.items():\n",
    "            total_combinations *= len(values)\n",
    "        \n",
    "        param_frequency = {}\n",
    "        for param in search_space.keys():\n",
    "            param_frequency[param] = {}\n",
    "            for value in search_space[param]:\n",
    "                param_frequency[param][str(value)] = 0\n",
    "        \n",
    "        for config in optimized_configs:\n",
    "            for param, value in config.items():\n",
    "                if param in param_frequency:\n",
    "                    str_value = str(value)\n",
    "                    if str_value in param_frequency[param]:\n",
    "                        param_frequency[param][str_value] += 1\n",
    "        \n",
    "        effective_space = 1\n",
    "        pruned_params = {}\n",
    "        for param, freq_dict in param_frequency.items():\n",
    "            selected_values = [v for v, count in freq_dict.items() if count > 0]\n",
    "            pruned_params[param] = {\n",
    "                'original_choices': search_space[param],\n",
    "                'selected_values': selected_values,\n",
    "                'selection_frequency': freq_dict,\n",
    "                'reduction_ratio': len(selected_values) / len(search_space[param])\n",
    "            }\n",
    "            effective_space *= max(1, len(selected_values))\n",
    "        \n",
    "        space_reduction = 1 - (effective_space / total_combinations)\n",
    "        \n",
    "        param_changes = {}\n",
    "        for param in search_space.keys():\n",
    "            default_val = default_params.get(param)\n",
    "            optimized_val = best_config.get(param)\n",
    "            param_changes[param] = {\n",
    "                'default': default_val,\n",
    "                'optimized': optimized_val,\n",
    "                'changed': str(default_val) != str(optimized_val)\n",
    "            }\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'pre_prune': {\n",
    "                'search_space': search_space,\n",
    "                'total_combinations': total_combinations,\n",
    "                'default_params': default_params,\n",
    "                'default_fitness': default_fitness,\n",
    "                'n_hyperparameters': len(search_space)\n",
    "            },\n",
    "            'post_prune': {\n",
    "                'best_config': best_config,\n",
    "                'optimized_fitness': optimized_fitness,\n",
    "                'effective_combinations': effective_space,\n",
    "                'pruned_params': pruned_params,\n",
    "                'n_selected_hyperparameters': sum(1 for p in pruned_params.values() \n",
    "                                                   if len(p['selected_values']) > 0)\n",
    "            },\n",
    "            'reduction_metrics': {\n",
    "                'space_reduction_ratio': space_reduction,\n",
    "                'space_reduction_percent': space_reduction * 100,\n",
    "                'original_combinations': total_combinations,\n",
    "                'effective_combinations': effective_space,\n",
    "                'params_changed': sum(1 for p in param_changes.values() if p['changed']),\n",
    "                'total_params': len(param_changes)\n",
    "            },\n",
    "            'param_changes': param_changes,\n",
    "            'fitness_improvement': {\n",
    "                'default': default_fitness,\n",
    "                'optimized': optimized_fitness,\n",
    "                'absolute_gain': (optimized_fitness - default_fitness) if default_fitness and optimized_fitness else None,\n",
    "                'relative_gain_percent': ((optimized_fitness - default_fitness) / default_fitness * 100) \n",
    "                                         if default_fitness and optimized_fitness and default_fitness > 0 else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.pruning_results[model_name] = result\n",
    "        return result\n",
    "    \n",
    "    def get_summary_table(self):\n",
    "        rows = []\n",
    "        for model_name, result in self.pruning_results.items():\n",
    "            rows.append({\n",
    "                'Model': model_name,\n",
    "                'Pre-Prune HP': result['pre_prune']['n_hyperparameters'],\n",
    "                'Post-Prune HP': result['post_prune']['n_selected_hyperparameters'],\n",
    "                'Pre-Prune Combos': result['pre_prune']['total_combinations'],\n",
    "                'Post-Prune Combos': result['post_prune']['effective_combinations'],\n",
    "                'Reduction (%)': f\"{result['reduction_metrics']['space_reduction_percent']:.1f}\",\n",
    "                'Default AUC': f\"{result['fitness_improvement']['default']:.4f}\" if result['fitness_improvement']['default'] else 'N/A',\n",
    "                'Optimized AUC': f\"{result['fitness_improvement']['optimized']:.4f}\" if result['fitness_improvement']['optimized'] else 'N/A',\n",
    "                'AUC Gain (%)': f\"{result['fitness_improvement']['relative_gain_percent']:.2f}\" if result['fitness_improvement']['relative_gain_percent'] else 'N/A'\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def get_all_results(self):\n",
    "        return self.pruning_results\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(y_true, y_pred, y_proba=None):\n",
    "        metrics = {\n",
    "            'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "            'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "            'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "            'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "            'mcc': float(matthews_corrcoef(y_true, y_pred)),\n",
    "            'kappa': float(cohen_kappa_score(y_true, y_pred)),\n",
    "        }\n",
    "        \n",
    "        sens_spec = compute_sensitivity_specificity(y_true, y_pred)\n",
    "        metrics.update(sens_spec)\n",
    "        \n",
    "        if y_proba is not None:\n",
    "            metrics['roc_auc'] = float(roc_auc_score(y_true, y_proba))\n",
    "            metrics['avg_precision'] = float(average_precision_score(y_true, y_proba))\n",
    "        else:\n",
    "            metrics['roc_auc'] = np.nan\n",
    "            metrics['avg_precision'] = np.nan\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_train_val_gap(train_metrics, val_metrics):\n",
    "        gap = {}\n",
    "        for key in train_metrics:\n",
    "            if key in val_metrics and isinstance(train_metrics[key], (int, float)):\n",
    "                if not np.isnan(train_metrics[key]) and not np.isnan(val_metrics[key]):\n",
    "                    gap[f'{key}_gap'] = float(train_metrics[key] - val_metrics[key])\n",
    "        return gap\n",
    "\n",
    "\n",
    "class HyperparameterGrammar:\n",
    "    \n",
    "    def __init__(self, grammar_str=None):\n",
    "        self.params = {}\n",
    "        self.param_order = []\n",
    "        if grammar_str:\n",
    "            self._parse_string(grammar_str)\n",
    "    \n",
    "    def _parse_string(self, grammar_str):\n",
    "        for line in grammar_str.strip().split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            if '::=' in line:\n",
    "                parts = line.split('::=')\n",
    "                param = parts[0].strip().strip('<>').replace('-', '_').replace(' ', '')\n",
    "                choices = [c.strip() for c in parts[1].strip().split('|') if c.strip()]\n",
    "                self.params[param] = choices\n",
    "                self.param_order.append(param)\n",
    "    \n",
    "    def get_choices(self, param):\n",
    "        return self.params.get(param.strip('<>').replace('-', '_'), [])\n",
    "    \n",
    "    def total_search_space(self):\n",
    "        space = 1\n",
    "        for choices in self.params.values():\n",
    "            space *= len(choices)\n",
    "        return space\n",
    "\n",
    "\n",
    "def convert_value(value_str):\n",
    "    value_str = str(value_str).strip()\n",
    "    if value_str.lower() == 'true':\n",
    "        return True\n",
    "    if value_str.lower() == 'false':\n",
    "        return False\n",
    "    if value_str.lower() == 'none':\n",
    "        return None\n",
    "    try:\n",
    "        if 'e' in value_str.lower() or '.' in value_str:\n",
    "            return float(value_str)\n",
    "        return int(value_str)\n",
    "    except ValueError:\n",
    "        return value_str\n",
    "\n",
    "\n",
    "class GEMapper:\n",
    "    \n",
    "    def __init__(self, grammar, max_wraps=2):\n",
    "        self.grammar = grammar\n",
    "        self.max_wraps = max_wraps\n",
    "    \n",
    "    def decode(self, chromosome):\n",
    "        config = {}\n",
    "        codon_idx = 0\n",
    "        wraps = 0\n",
    "        \n",
    "        for param in self.grammar.param_order:\n",
    "            choices = self.grammar.get_choices(param)\n",
    "            if not choices:\n",
    "                continue\n",
    "            if codon_idx >= len(chromosome):\n",
    "                codon_idx = 0\n",
    "                wraps += 1\n",
    "                if wraps > self.max_wraps:\n",
    "                    return config, False\n",
    "            codon = chromosome[codon_idx]\n",
    "            choice_idx = codon % len(choices)\n",
    "            config[param] = convert_value(choices[choice_idx])\n",
    "            codon_idx += 1\n",
    "        return config, True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GEResult:\n",
    "    best_config: Dict[str, Any]\n",
    "    best_fitness: float\n",
    "    best_chromosome: List[int]\n",
    "    history: Dict[str, List]\n",
    "    generations: int\n",
    "    evaluations: int\n",
    "    runtime_seconds: float\n",
    "\n",
    "\n",
    "class GEOptimizer:\n",
    "    \n",
    "    def __init__(self, grammar, fitness_fn, pop_size=20, generations=8,\n",
    "                 chromosome_length=None, codon_max=255, crossover_rate=0.8,\n",
    "                 mutation_rate=0.1, tournament_size=3, elitism=2,\n",
    "                 maximize=True, seed=None, verbose=True):\n",
    "        \n",
    "        self.grammar = grammar\n",
    "        self.mapper = GEMapper(grammar)\n",
    "        self.fitness_fn = fitness_fn\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.codon_max = codon_max\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.tournament_size = tournament_size\n",
    "        self.elitism = elitism\n",
    "        self.maximize = maximize\n",
    "        self.verbose = verbose\n",
    "        self.chromosome_length = chromosome_length or max(len(grammar.param_order) * 3, 20)\n",
    "        \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.history = {'best_fitness': [], 'avg_fitness': []}\n",
    "    \n",
    "    def _init_population(self):\n",
    "        return [[random.randint(0, self.codon_max) for _ in range(self.chromosome_length)]\n",
    "                for _ in range(self.pop_size)]\n",
    "    \n",
    "    def _evaluate(self, chrom):\n",
    "        config, valid = self.mapper.decode(chrom)\n",
    "        if not valid:\n",
    "            return -float('inf') if self.maximize else float('inf'), config, False\n",
    "        try:\n",
    "            fitness = self.fitness_fn(config)\n",
    "            return fitness, config, True\n",
    "        except Exception:\n",
    "            return -float('inf') if self.maximize else float('inf'), config, False\n",
    "    \n",
    "    def _tournament(self, pop, scores):\n",
    "        indices = random.sample(range(len(pop)), self.tournament_size)\n",
    "        if self.maximize:\n",
    "            winner = max(indices, key=lambda i: scores[i])\n",
    "        else:\n",
    "            winner = min(indices, key=lambda i: scores[i])\n",
    "        return list(pop[winner])\n",
    "    \n",
    "    def _crossover(self, p1, p2):\n",
    "        if random.random() > self.crossover_rate:\n",
    "            return p1[:], p2[:]\n",
    "        pt = random.randint(1, len(p1) - 1)\n",
    "        return p1[:pt] + p2[pt:], p2[:pt] + p1[pt:]\n",
    "    \n",
    "    def _mutate(self, chrom):\n",
    "        for i in range(len(chrom)):\n",
    "            if random.random() < self.mutation_rate:\n",
    "                chrom[i] = random.randint(0, self.codon_max)\n",
    "        return chrom\n",
    "    \n",
    "    def evolve(self):\n",
    "        start_time = time.time()\n",
    "        population = self._init_population()\n",
    "        best_ever = (None, -float('inf') if self.maximize else float('inf'), None)\n",
    "        total_evals = 0\n",
    "        \n",
    "        for gen in range(self.generations):\n",
    "            scores = []\n",
    "            configs = []\n",
    "            for chrom in population:\n",
    "                fit, cfg, valid = self._evaluate(chrom)\n",
    "                scores.append(fit)\n",
    "                configs.append(cfg)\n",
    "            total_evals += len(population)\n",
    "            \n",
    "            valid_scores = [s for s in scores if s != -float('inf') and s != float('inf')]\n",
    "            if valid_scores:\n",
    "                if self.maximize:\n",
    "                    best_idx = np.argmax(scores)\n",
    "                    if scores[best_idx] > best_ever[1]:\n",
    "                        best_ever = (list(population[best_idx]), scores[best_idx], configs[best_idx])\n",
    "                else:\n",
    "                    best_idx = np.argmin(scores)\n",
    "                    if scores[best_idx] < best_ever[1]:\n",
    "                        best_ever = (list(population[best_idx]), scores[best_idx], configs[best_idx])\n",
    "                \n",
    "                best_fit = max(valid_scores) if self.maximize else min(valid_scores)\n",
    "                avg_fit = np.mean(valid_scores)\n",
    "            else:\n",
    "                best_fit = avg_fit = 0\n",
    "            \n",
    "            self.history['best_fitness'].append(best_fit)\n",
    "            self.history['avg_fitness'].append(avg_fit)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"    Gen {gen+1:2d}/{self.generations}: Best={best_fit:.4f}, Avg={avg_fit:.4f}\")\n",
    "            \n",
    "            sorted_pop = sorted(zip(population, scores), key=lambda x: x[1], reverse=self.maximize)\n",
    "            new_pop = [list(sorted_pop[i][0]) for i in range(self.elitism)]\n",
    "            \n",
    "            while len(new_pop) < self.pop_size:\n",
    "                p1 = self._tournament(population, scores)\n",
    "                p2 = self._tournament(population, scores)\n",
    "                c1, c2 = self._crossover(p1, p2)\n",
    "                new_pop.append(self._mutate(c1))\n",
    "                if len(new_pop) < self.pop_size:\n",
    "                    new_pop.append(self._mutate(c2))\n",
    "            \n",
    "            population = new_pop\n",
    "        \n",
    "        runtime = time.time() - start_time\n",
    "        return GEResult(\n",
    "            best_config=best_ever[2],\n",
    "            best_fitness=best_ever[1],\n",
    "            best_chromosome=best_ever[0],\n",
    "            history=self.history,\n",
    "            generations=len(self.history['best_fitness']),\n",
    "            evaluations=total_evals,\n",
    "            runtime_seconds=runtime\n",
    "        )\n",
    "\n",
    "\n",
    "def create_tabnet_grammar():\n",
    "    return HyperparameterGrammar(grammar_str=\"\"\"\n",
    "<n_d> ::= 8 | 16 | 32 | 64\n",
    "<n_a> ::= 8 | 16 | 32 | 64\n",
    "<n_steps> ::= 3 | 5 | 7\n",
    "<lambda_sparse> ::= 1e-4 | 1e-3 | 1e-2\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_random_forest_grammar():\n",
    "    return HyperparameterGrammar(grammar_str=\"\"\"\n",
    "<n_estimators> ::= 100 | 200 | 300\n",
    "<max_depth> ::= 5 | 10 | 15 | None\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_xgboost_grammar():\n",
    "    return HyperparameterGrammar(grammar_str=\"\"\"\n",
    "<n_estimators> ::= 100 | 200 | 300\n",
    "<learning_rate> ::= 0.01 | 0.05 | 0.1\n",
    "<max_depth> ::= 3 | 5 | 7\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_svm_grammar():\n",
    "    return HyperparameterGrammar(grammar_str=\"\"\"\n",
    "<C> ::= 0.1 | 1.0 | 10.0\n",
    "<kernel> ::= linear | rbf\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_logistic_regression_grammar():\n",
    "    return HyperparameterGrammar(grammar_str=\"\"\"\n",
    "<C> ::= 0.1 | 1.0 | 10.0\n",
    "<penalty> ::= l1 | l2\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_gradient_boosting_grammar():\n",
    "    return HyperparameterGrammar(grammar_str=\"\"\"\n",
    "<n_estimators> ::= 100 | 200 | 300\n",
    "<learning_rate> ::= 0.05 | 0.1 | 0.2\n",
    "<max_depth> ::= 3 | 5\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    DATA_CSV: str = r\"C:\\Users\\awwal\\Desktop\\MLEA_experiments\\data.csv\"\n",
    "    TARGET_COLUMN: str = \"diagnosis\"\n",
    "    TEST_SIZE: float = 0.1\n",
    "    VAL_SIZE: float = 0.111\n",
    "    \n",
    "    # =====================================================\n",
    "    # MODIFIED: Increased GE generations from 10 to 30\n",
    "    # =====================================================\n",
    "    GE_POP_SIZE: int = 20  # Increased population for better exploration\n",
    "    GE_GENERATIONS: int = 30  # CHANGED: 10 -> 30\n",
    "    GE_GENERATIONS_BASELINE: int = 30  # CHANGED: 10 -> 30\n",
    "    GE_CROSSOVER_RATE: float = 0.85\n",
    "    GE_MUTATION_RATE: float = 0.15\n",
    "    GE_TOURNAMENT_SIZE: int = 3\n",
    "    GE_ELITISM: int = 2\n",
    "    \n",
    "    # 30 independent runs for statistical robustness\n",
    "    N_INDEPENDENT_RUNS: int = 30\n",
    "    \n",
    "    # Parallel execution settings\n",
    "    N_PARALLEL_JOBS: int = -1  # -1 uses all available cores\n",
    "    \n",
    "    # =====================================================\n",
    "    # MODIFIED: Improved TabNet settings for better performance\n",
    "    # =====================================================\n",
    "    TABNET_MAX_EPOCHS: int = 100  # CHANGED: 25 -> 100 (more training time)\n",
    "    TABNET_PATIENCE: int = 15  # CHANGED: 6 -> 15 (more patience for convergence)\n",
    "    TABNET_BATCH_SIZE: int = 128  # CHANGED: 256 -> 128 (smaller batches often help)\n",
    "    TABNET_GAMMA: float = 1.5  # CHANGED: 1.3 -> 1.5 (coefficient for feature reusage)\n",
    "    \n",
    "    # Additional TabNet settings for improved performance\n",
    "    TABNET_N_INDEPENDENT_FINAL: int = 5  # Train multiple final models and ensemble\n",
    "    TABNET_VIRTUAL_BATCH_SIZE: int = 64  # Ghost batch normalization\n",
    "    TABNET_MOMENTUM: float = 0.02  # Batch normalization momentum\n",
    "    TABNET_MASK_TYPE: str = 'entmax'  # 'sparsemax' or 'entmax' for attention\n",
    "    \n",
    "    # SMOTE for class imbalance handling\n",
    "    USE_SMOTE: bool = True\n",
    "    \n",
    "    RESULTS_DIR: str = \"ge_results_modified\"\n",
    "    RANDOM_SEED: int = 42\n",
    "    VERBOSE: bool = False  # Disabled for parallel execution\n",
    "    \n",
    "    # =====================================================\n",
    "    # MODIFIED: Show plots interactively instead of saving\n",
    "    # =====================================================\n",
    "    SAVE_PLOTS: bool = True  # Still save plots\n",
    "    SHOW_PLOTS: bool = True  # CHANGED: False -> True (show plots in VSCode)\n",
    "    PLOT_DPI: int = 150\n",
    "\n",
    "\n",
    "class TabNetOptimizer:\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                 config, feature_names=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.config = config\n",
    "        self.feature_names = feature_names or [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "    \n",
    "    def _fitness_holdout(self, params, seed_offset=0):\n",
    "        \"\"\"Evaluate fitness using holdout validation with SMOTE.\"\"\"\n",
    "        try:\n",
    "            X_tr = self.X_train.copy()\n",
    "            y_tr = self.y_train.copy()\n",
    "            \n",
    "            # Apply SMOTE for class imbalance\n",
    "            if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "                smote = SMOTE(random_state=self.config.RANDOM_SEED + seed_offset)\n",
    "                X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
    "            \n",
    "            model = TabNetClassifier(\n",
    "                n_d=params['n_d'],\n",
    "                n_a=params['n_a'],\n",
    "                n_steps=params['n_steps'],\n",
    "                gamma=self.config.TABNET_GAMMA,\n",
    "                lambda_sparse=params['lambda_sparse'],\n",
    "                momentum=self.config.TABNET_MOMENTUM,\n",
    "                mask_type=self.config.TABNET_MASK_TYPE,\n",
    "                verbose=0,\n",
    "                seed=self.config.RANDOM_SEED + seed_offset\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(self.X_val, self.y_val)],\n",
    "                eval_metric=['auc'],\n",
    "                max_epochs=self.config.TABNET_MAX_EPOCHS,\n",
    "                patience=self.config.TABNET_PATIENCE,\n",
    "                batch_size=self.config.TABNET_BATCH_SIZE,\n",
    "                virtual_batch_size=self.config.TABNET_VIRTUAL_BATCH_SIZE,\n",
    "                drop_last=False\n",
    "            )\n",
    "            \n",
    "            y_pred_proba = model.predict_proba(self.X_val)[:, 1]\n",
    "            return roc_auc_score(self.y_val, y_pred_proba)\n",
    "        except Exception as e:\n",
    "            print(f\"    [Warning] TabNet training error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_default_params(self):\n",
    "        default_params = HyperparameterPruningAnalyzer.DEFAULT_HYPERPARAMS.get('TabNet', {})\n",
    "        return self._fitness_holdout(default_params, seed_offset=9999)\n",
    "    \n",
    "    def _run_single_optimization(self, run_idx):\n",
    "        \"\"\"Run a single GE optimization - designed for parallel execution.\"\"\"\n",
    "        grammar = create_tabnet_grammar()\n",
    "        seed = self.config.RANDOM_SEED + run_idx * 1000\n",
    "        \n",
    "        fitness_fn = lambda p: self._fitness_holdout(p, seed_offset=run_idx * 100)\n",
    "        \n",
    "        optimizer = GEOptimizer(\n",
    "            grammar=grammar,\n",
    "            fitness_fn=fitness_fn,\n",
    "            pop_size=self.config.GE_POP_SIZE,\n",
    "            generations=self.config.GE_GENERATIONS,\n",
    "            crossover_rate=self.config.GE_CROSSOVER_RATE,\n",
    "            mutation_rate=self.config.GE_MUTATION_RATE,\n",
    "            tournament_size=self.config.GE_TOURNAMENT_SIZE,\n",
    "            elitism=self.config.GE_ELITISM,\n",
    "            maximize=True,\n",
    "            seed=seed,\n",
    "            verbose=False  # Disable verbose for parallel\n",
    "        )\n",
    "        \n",
    "        result = optimizer.evolve()\n",
    "        result.best_config['gamma'] = self.config.TABNET_GAMMA\n",
    "        \n",
    "        return {\n",
    "            'run_idx': run_idx,\n",
    "            'best_config': result.best_config,\n",
    "            'best_fitness': result.best_fitness,\n",
    "            'generations': result.generations,\n",
    "            'evaluations': result.evaluations,\n",
    "            'runtime': result.runtime_seconds,\n",
    "            'history': {\n",
    "                'best_fitness': result.history['best_fitness'],\n",
    "                'avg_fitness': result.history['avg_fitness']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def optimize_parallel(self, n_jobs=-1):\n",
    "        \"\"\"Run all independent optimizations in parallel using joblib.\"\"\"\n",
    "        print(f\"  Running {self.config.N_INDEPENDENT_RUNS} optimizations in parallel...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "            delayed(self._run_single_optimization)(run_idx) \n",
    "            for run_idx in range(self.config.N_INDEPENDENT_RUNS)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def optimize_single_run(self, run_idx):\n",
    "        \"\"\"Run a single optimization (for sequential execution).\"\"\"\n",
    "        return self._run_single_optimization(run_idx)\n",
    "    \n",
    "    def train_final_model(self, params, seed=42):\n",
    "        X_train_full = np.vstack([self.X_train, self.X_val])\n",
    "        y_train_full = np.concatenate([self.y_train, self.y_val])\n",
    "        \n",
    "        if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "            smote = SMOTE(random_state=seed)\n",
    "            X_train_full, y_train_full = smote.fit_resample(X_train_full, y_train_full)\n",
    "        \n",
    "        model = TabNetClassifier(\n",
    "            n_d=params['n_d'],\n",
    "            n_a=params['n_a'],\n",
    "            n_steps=params['n_steps'],\n",
    "            gamma=params.get('gamma', self.config.TABNET_GAMMA),\n",
    "            lambda_sparse=params['lambda_sparse'],\n",
    "            momentum=self.config.TABNET_MOMENTUM,\n",
    "            mask_type=self.config.TABNET_MASK_TYPE,\n",
    "            verbose=0,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        X_tr, X_es, y_tr, y_es = train_test_split(\n",
    "            X_train_full, y_train_full, test_size=0.1, \n",
    "            stratify=y_train_full, random_state=seed\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_es, y_es)],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs=self.config.TABNET_MAX_EPOCHS,\n",
    "            patience=self.config.TABNET_PATIENCE,\n",
    "            batch_size=self.config.TABNET_BATCH_SIZE,\n",
    "            virtual_batch_size=self.config.TABNET_VIRTUAL_BATCH_SIZE,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        y_train_pred = model.predict(X_tr)\n",
    "        y_train_proba = model.predict_proba(X_tr)[:, 1]\n",
    "        results['train_metrics'] = MetricsCalculator.compute_all_metrics(y_tr, y_train_pred, y_train_proba)\n",
    "        \n",
    "        y_test_pred = model.predict(self.X_test)\n",
    "        y_test_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "        results['test_metrics'] = MetricsCalculator.compute_all_metrics(self.y_test, y_test_pred, y_test_proba)\n",
    "        \n",
    "        results['train_val_gap'] = MetricsCalculator.compute_train_val_gap(\n",
    "            results['train_metrics'], results['test_metrics']\n",
    "        )\n",
    "        \n",
    "        results['feature_importance'] = self._get_feature_importance(model)\n",
    "        \n",
    "        return model, results\n",
    "    \n",
    "    def _get_feature_importance(self, model):\n",
    "        try:\n",
    "            importance = model.feature_importances_\n",
    "            importance_dict = {}\n",
    "            for i, imp in enumerate(importance):\n",
    "                importance_dict[self.feature_names[i]] = float(imp)\n",
    "            \n",
    "            sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return {\n",
    "                'raw_importance': importance.tolist(),\n",
    "                'feature_ranking': sorted_importance,\n",
    "                'top_10_features': sorted_importance[:10],\n",
    "                'feature_names': self.feature_names\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "class BaselineOptimizer:\n",
    "    \n",
    "    MODEL_GRAMMARS = {\n",
    "        'RandomForest': create_random_forest_grammar,\n",
    "        'LogisticRegression': create_logistic_regression_grammar,\n",
    "        'SVM': create_svm_grammar,\n",
    "        'GradientBoosting': create_gradient_boosting_grammar,\n",
    "        'XGBoost': create_xgboost_grammar,\n",
    "    }\n",
    "    \n",
    "    FIXED_PARAMS = {\n",
    "        'RandomForest': {'max_features': 'sqrt', 'min_samples_split': 2},\n",
    "        'LogisticRegression': {'max_iter': 1000},\n",
    "        'SVM': {'gamma': 'scale'},\n",
    "        'GradientBoosting': {'subsample': 0.8},\n",
    "        'XGBoost': {'subsample': 0.8, 'colsample_bytree': 0.8},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                 config, feature_names=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.config = config\n",
    "        self.feature_names = feature_names or [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "    \n",
    "    def _create_model(self, model_type, params, seed):\n",
    "        full_params = {**self.FIXED_PARAMS.get(model_type, {}), **params}\n",
    "        \n",
    "        if model_type == 'RandomForest':\n",
    "            max_depth = full_params.get('max_depth')\n",
    "            if max_depth == 'None' or max_depth is None:\n",
    "                max_depth = None\n",
    "            return RandomForestClassifier(\n",
    "                n_estimators=full_params['n_estimators'],\n",
    "                max_depth=max_depth,\n",
    "                max_features=full_params.get('max_features', 'sqrt'),\n",
    "                random_state=seed,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'LogisticRegression':\n",
    "            penalty = full_params.get('penalty', 'l2')\n",
    "            solver = 'liblinear' if penalty == 'l1' else 'lbfgs'\n",
    "            return LogisticRegression(\n",
    "                C=full_params['C'],\n",
    "                penalty=penalty,\n",
    "                solver=solver,\n",
    "                max_iter=full_params.get('max_iter', 1000),\n",
    "                random_state=seed\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'SVM':\n",
    "            return SVC(\n",
    "                C=full_params['C'],\n",
    "                kernel=full_params['kernel'],\n",
    "                gamma=full_params.get('gamma', 'scale'),\n",
    "                probability=True,\n",
    "                random_state=seed\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'GradientBoosting':\n",
    "            return GradientBoostingClassifier(\n",
    "                n_estimators=full_params['n_estimators'],\n",
    "                learning_rate=full_params['learning_rate'],\n",
    "                max_depth=full_params['max_depth'],\n",
    "                subsample=full_params.get('subsample', 0.8),\n",
    "                random_state=seed\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'XGBoost':\n",
    "            if not XGBOOST_AVAILABLE:\n",
    "                raise ImportError(\"XGBoost not available\")\n",
    "            return XGBClassifier(\n",
    "                n_estimators=full_params['n_estimators'],\n",
    "                learning_rate=full_params['learning_rate'],\n",
    "                max_depth=full_params['max_depth'],\n",
    "                subsample=full_params.get('subsample', 0.8),\n",
    "                colsample_bytree=full_params.get('colsample_bytree', 0.8),\n",
    "                random_state=seed,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                verbosity=0\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    def _fitness_holdout(self, model_type, params, seed_offset=0):\n",
    "        \"\"\"Evaluate fitness using holdout validation (no CV for speed).\"\"\"\n",
    "        try:\n",
    "            seed = self.config.RANDOM_SEED + seed_offset\n",
    "            model = self._create_model(model_type, params, seed)\n",
    "            \n",
    "            X_tr = self.X_train.copy()\n",
    "            y_tr = self.y_train.copy()\n",
    "            \n",
    "            if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "                smote = SMOTE(random_state=seed)\n",
    "                X_tr, y_tr = smote.fit_resample(X_tr, y_tr)\n",
    "            \n",
    "            model.fit(X_tr, y_tr)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_proba = model.predict_proba(self.X_val)[:, 1]\n",
    "                return roc_auc_score(self.y_val, y_pred_proba)\n",
    "            else:\n",
    "                y_pred = model.predict(self.X_val)\n",
    "                return accuracy_score(self.y_val, y_pred)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_default_params(self, model_type):\n",
    "        default_params = HyperparameterPruningAnalyzer.DEFAULT_HYPERPARAMS.get(model_type, {})\n",
    "        search_params = {}\n",
    "        search_space = HyperparameterPruningAnalyzer.SEARCH_SPACE.get(model_type, {})\n",
    "        for param in search_space.keys():\n",
    "            if param in default_params:\n",
    "                search_params[param] = default_params[param]\n",
    "        return self._fitness_holdout(model_type, search_params, seed_offset=9999)\n",
    "    \n",
    "    def train_final_model(self, model_type, params, seed=42):\n",
    "        X_train_full = np.vstack([self.X_train, self.X_val])\n",
    "        y_train_full = np.concatenate([self.y_train, self.y_val])\n",
    "        \n",
    "        if self.config.USE_SMOTE and SMOTE_AVAILABLE:\n",
    "            smote = SMOTE(random_state=seed)\n",
    "            X_train_full, y_train_full = smote.fit_resample(X_train_full, y_train_full)\n",
    "        \n",
    "        model = self._create_model(model_type, params, seed)\n",
    "        model.fit(X_train_full, y_train_full)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        y_train_pred = model.predict(X_train_full)\n",
    "        y_train_proba = model.predict_proba(X_train_full)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        results['train_metrics'] = MetricsCalculator.compute_all_metrics(X_train_full, y_train_pred, y_train_proba)\n",
    "        \n",
    "        y_test_pred = model.predict(self.X_test)\n",
    "        y_test_proba = model.predict_proba(self.X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        results['test_metrics'] = MetricsCalculator.compute_all_metrics(self.y_test, y_test_pred, y_test_proba)\n",
    "        \n",
    "        results['train_test_gap'] = MetricsCalculator.compute_train_val_gap(\n",
    "            results['train_metrics'], results['test_metrics']\n",
    "        )\n",
    "        \n",
    "        results['feature_importance'] = self._get_feature_importance(model, model_type)\n",
    "        \n",
    "        return model, results\n",
    "    \n",
    "    def _get_feature_importance(self, model, model_type):\n",
    "        try:\n",
    "            importance = None\n",
    "            \n",
    "            if model_type in ['RandomForest', 'GradientBoosting', 'XGBoost']:\n",
    "                importance = model.feature_importances_\n",
    "            elif model_type == 'LogisticRegression':\n",
    "                importance = np.abs(model.coef_[0])\n",
    "            elif model_type == 'SVM':\n",
    "                if model.kernel == 'linear':\n",
    "                    importance = np.abs(model.coef_[0])\n",
    "                else:\n",
    "                    return {'note': 'Feature importance not available for non-linear SVM'}\n",
    "            \n",
    "            if importance is not None:\n",
    "                if importance.sum() > 0:\n",
    "                    importance = importance / importance.sum()\n",
    "                \n",
    "                importance_dict = {}\n",
    "                for i, imp in enumerate(importance):\n",
    "                    importance_dict[self.feature_names[i]] = float(imp)\n",
    "                \n",
    "                sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                return {\n",
    "                    'raw_importance': importance.tolist(),\n",
    "                    'feature_ranking': sorted_importance,\n",
    "                    'top_10_features': sorted_importance[:10]\n",
    "                }\n",
    "            \n",
    "            return {'note': 'Feature importance not available'}\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _run_single_optimization(self, model_type, run_idx):\n",
    "        \"\"\"Run a single GE optimization - designed for parallel execution.\"\"\"\n",
    "        if model_type not in self.MODEL_GRAMMARS:\n",
    "            raise ValueError(f\"Unknown model: {model_type}\")\n",
    "        \n",
    "        grammar = self.MODEL_GRAMMARS[model_type]()\n",
    "        seed = self.config.RANDOM_SEED + run_idx * 1000\n",
    "        \n",
    "        fitness_fn = lambda p: self._fitness_holdout(model_type, p, seed_offset=run_idx * 100)\n",
    "        \n",
    "        optimizer = GEOptimizer(\n",
    "            grammar=grammar,\n",
    "            fitness_fn=fitness_fn,\n",
    "            pop_size=self.config.GE_POP_SIZE,\n",
    "            generations=self.config.GE_GENERATIONS_BASELINE,\n",
    "            crossover_rate=self.config.GE_CROSSOVER_RATE,\n",
    "            mutation_rate=self.config.GE_MUTATION_RATE,\n",
    "            tournament_size=self.config.GE_TOURNAMENT_SIZE,\n",
    "            elitism=self.config.GE_ELITISM,\n",
    "            maximize=True,\n",
    "            seed=seed,\n",
    "            verbose=False  # Disable verbose for parallel\n",
    "        )\n",
    "        \n",
    "        result = optimizer.evolve()\n",
    "        full_config = {**self.FIXED_PARAMS.get(model_type, {}), **result.best_config}\n",
    "        \n",
    "        return {\n",
    "            'run_idx': run_idx,\n",
    "            'best_config': full_config,\n",
    "            'best_fitness': result.best_fitness,\n",
    "            'generations': result.generations,\n",
    "            'evaluations': result.evaluations,\n",
    "            'runtime': result.runtime_seconds,\n",
    "            'history': {\n",
    "                'best_fitness': result.history['best_fitness'],\n",
    "                'avg_fitness': result.history['avg_fitness']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def optimize_parallel(self, model_type, n_jobs=-1):\n",
    "        \"\"\"Run all independent optimizations in parallel using joblib.\"\"\"\n",
    "        print(f\"  Running {self.config.N_INDEPENDENT_RUNS} optimizations in parallel...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "            delayed(self._run_single_optimization)(model_type, run_idx) \n",
    "            for run_idx in range(self.config.N_INDEPENDENT_RUNS)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def optimize_single_run(self, model_type, run_idx):\n",
    "        \"\"\"Run a single optimization (for sequential execution).\"\"\"\n",
    "        return self._run_single_optimization(model_type, run_idx)\n",
    "\n",
    "\n",
    "class ResultsPlotter:\n",
    "    \n",
    "    def __init__(self, results, config, output_dir=None):\n",
    "        self.results = results\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir or config.RESULTS_DIR\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        self.colors = {\n",
    "            'TabNet': '#2ecc71',\n",
    "            'RandomForest': '#3498db',\n",
    "            'XGBoost': '#e74c3c',\n",
    "            'SVM': '#9b59b6',\n",
    "            'LogisticRegression': '#f39c12',\n",
    "            'GradientBoosting': '#1abc9c'\n",
    "        }\n",
    "    \n",
    "    def _get_color(self, name):\n",
    "        return self.colors.get(name, '#95a5a6')\n",
    "    \n",
    "    def _save_and_show(self, filename):\n",
    "        \"\"\"Helper method to save and optionally show plots.\"\"\"\n",
    "        if self.config.SAVE_PLOTS:\n",
    "            plt.savefig(os.path.join(self.output_dir, filename),\n",
    "                       dpi=self.config.PLOT_DPI, bbox_inches='tight')\n",
    "        if self.config.SHOW_PLOTS:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "    \n",
    "    def plot_all(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"GENERATING PLOTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        plot_methods = [\n",
    "            ('Test performance comparison', self.plot_test_performance_comparison),\n",
    "            ('Sensitivity/Specificity', self.plot_sensitivity_specificity),\n",
    "            ('Train-Test gap', self.plot_train_test_gap),\n",
    "            ('Boxplot (runs)', self.plot_runs_boxplot),\n",
    "            ('Convergence curves', self.plot_convergence_summary),\n",
    "            ('Best vs Average fitness', self.plot_best_vs_average_fitness),\n",
    "            ('Feature importance (all models)', self.plot_all_feature_importance),\n",
    "            ('Feature importance heatmap', self.plot_feature_importance_heatmap),\n",
    "            ('HP pruning analysis', self.plot_hyperparameter_pruning),\n",
    "            ('HP count comparison', self.plot_hp_count_comparison),\n",
    "            ('Pruning tables', self.plot_pruning_comparison_table),\n",
    "        ]\n",
    "        \n",
    "        for idx, (name, method) in enumerate(plot_methods, 1):\n",
    "            print(f\"  [{idx}/{len(plot_methods)}] {name}...\", end=\" \", flush=True)\n",
    "            try:\n",
    "                method()\n",
    "                print(\"done\")\n",
    "            except Exception as e:\n",
    "                print(f\"failed ({e})\")\n",
    "                plt.close('all')\n",
    "        \n",
    "        print(f\"\\nAll plots saved to: {self.output_dir}\")\n",
    "        \n",
    "        if self.config.SHOW_PLOTS:\n",
    "            print(\"\\nPlots are being displayed. Close plot windows to continue...\")\n",
    "            plt.ioff()  # Turn off interactive mode\n",
    "            plt.show()  # This will block until all windows are closed\n",
    "    \n",
    "    def plot_test_performance_comparison(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        test_aucs = []\n",
    "        test_accs = []\n",
    "        test_f1s = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'best_run_test_metrics' in data:\n",
    "                model_names.append(name)\n",
    "                metrics = data['best_run_test_metrics']\n",
    "                test_aucs.append(metrics.get('roc_auc', 0))\n",
    "                test_accs.append(metrics.get('accuracy', 0))\n",
    "                test_f1s.append(metrics.get('f1', 0))\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle('Test Set Performance Comparison', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        colors = [self._get_color(name) for name in model_names]\n",
    "        \n",
    "        for ax, values, title, ylabel in [\n",
    "            (axes[0], test_aucs, 'ROC-AUC', 'AUC'),\n",
    "            (axes[1], test_accs, 'Accuracy', 'Accuracy'),\n",
    "            (axes[2], test_f1s, 'F1 Score', 'F1')\n",
    "        ]:\n",
    "            bars = ax.bar(x, values, color=colors, edgecolor='black', linewidth=1)\n",
    "            for bar, val in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_title(title)\n",
    "            ax.set_ylim(0, 1.1)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('test_performance.png')\n",
    "    \n",
    "    def plot_sensitivity_specificity(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        sensitivities = []\n",
    "        specificities = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'best_run_test_metrics' in data:\n",
    "                model_names.append(name)\n",
    "                metrics = data['best_run_test_metrics']\n",
    "                sensitivities.append(metrics.get('sensitivity', 0))\n",
    "                specificities.append(metrics.get('specificity', 0))\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, sensitivities, width, label='Sensitivity (TPR)',\n",
    "                      color='#e74c3c', edgecolor='black')\n",
    "        bars2 = ax.bar(x + width/2, specificities, width, label='Specificity (TNR)',\n",
    "                      color='#3498db', edgecolor='black')\n",
    "        \n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title('Sensitivity vs Specificity (Test Set)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylim(0, 1.15)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('sensitivity_specificity.png')\n",
    "    \n",
    "    def plot_train_test_gap(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        train_aucs = []\n",
    "        test_aucs = []\n",
    "        gaps = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'best_run_train_metrics' in data and 'best_run_test_metrics' in data:\n",
    "                model_names.append(name)\n",
    "                train_auc = data['best_run_train_metrics'].get('roc_auc', 0)\n",
    "                test_auc = data['best_run_test_metrics'].get('roc_auc', 0)\n",
    "                train_aucs.append(train_auc)\n",
    "                test_aucs.append(test_auc)\n",
    "                gaps.append(train_auc - test_auc)\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        fig.suptitle('Train vs Test Performance', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, train_aucs, width, label='Train AUC', color='#2ecc71', edgecolor='black')\n",
    "        axes[0].bar(x + width/2, test_aucs, width, label='Test AUC', color='#e74c3c', edgecolor='black')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[0].set_ylabel('AUC')\n",
    "        axes[0].set_title('Train vs Test AUC')\n",
    "        axes[0].set_ylim(0, 1.1)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        colors = ['#e74c3c' if g > 0.05 else '#f39c12' if g > 0.02 else '#2ecc71' for g in gaps]\n",
    "        bars = axes[1].bar(x, gaps, color=colors, edgecolor='black')\n",
    "        \n",
    "        for bar, gap in zip(bars, gaps):\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                        f'{gap:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        axes[1].axhline(y=0.05, color='red', linestyle='--', label='High overfit')\n",
    "        axes[1].axhline(y=0.02, color='orange', linestyle='--', label='Moderate overfit')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[1].set_ylabel('AUC Gap (Train - Test)')\n",
    "        axes[1].set_title('Overfitting Gap')\n",
    "        axes[1].legend(loc='upper right')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('train_test_gap.png')\n",
    "    \n",
    "    def plot_runs_boxplot(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        model_names = []\n",
    "        all_fitness = []\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            if 'all_runs' in data:\n",
    "                valid_runs = [r['best_fitness'] for r in data['all_runs'] if 'error' not in r]\n",
    "                if valid_runs:\n",
    "                    model_names.append(name)\n",
    "                    all_fitness.append(valid_runs)\n",
    "        \n",
    "        if not model_names:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        colors = [self._get_color(name) for name in model_names]\n",
    "        \n",
    "        bp = ax.boxplot(all_fitness, labels=model_names, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_ylabel('Best Fitness (Validation AUC)', fontsize=11)\n",
    "        ax.set_title(f'Distribution Across {self.config.N_INDEPENDENT_RUNS} Independent Runs', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('boxplot_runs.png')\n",
    "    \n",
    "    def plot_convergence_summary(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        has_data = False\n",
    "        for name, data in models_data.items():\n",
    "            if 'convergence_summary' in data:\n",
    "                conv = data['convergence_summary']\n",
    "                best_mean = conv.get('best_mean') or conv.get('mean', [])\n",
    "                best_std = conv.get('best_std') or conv.get('std', [])\n",
    "                if best_mean:\n",
    "                    has_data = True\n",
    "                    generations = range(len(best_mean))\n",
    "                    mean = np.array(best_mean)\n",
    "                    std = np.array(best_std) if best_std else np.zeros_like(mean)\n",
    "                    \n",
    "                    color = self._get_color(name)\n",
    "                    ax.plot(generations, mean, label=name, color=color, linewidth=2)\n",
    "                    ax.fill_between(generations, mean - std, mean + std, color=color, alpha=0.2)\n",
    "        \n",
    "        if not has_data:\n",
    "            plt.close()\n",
    "            return\n",
    "        \n",
    "        ax.set_xlabel('Generation', fontsize=11)\n",
    "        ax.set_ylabel('Best Fitness (AUC)', fontsize=11)\n",
    "        ax.set_title(f'Mean Convergence ({self.config.N_INDEPENDENT_RUNS} Runs) - {self.config.GE_GENERATIONS} Generations', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('convergence.png')\n",
    "    \n",
    "    def plot_best_vs_average_fitness(self):\n",
    "        models_data = self.results.get('models', {})\n",
    "        n_models = len(models_data)\n",
    "        \n",
    "        if n_models == 0:\n",
    "            return\n",
    "        \n",
    "        n_cols = min(3, n_models)\n",
    "        n_rows = (n_models + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "        fig.suptitle('Best Individual vs Population Average Fitness', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes = axes.flatten() if n_models > 1 else axes\n",
    "        \n",
    "        plot_idx = 0\n",
    "        for name, data in models_data.items():\n",
    "            if plot_idx >= len(axes):\n",
    "                break\n",
    "            \n",
    "            ax = axes[plot_idx] if n_models > 1 else axes[0]\n",
    "            color = self._get_color(name)\n",
    "            \n",
    "            conv = data.get('convergence_summary', {})\n",
    "            \n",
    "            best_mean = np.array(conv.get('best_mean', []))\n",
    "            best_std = np.array(conv.get('best_std', []))\n",
    "            avg_mean = np.array(conv.get('avg_mean', []))\n",
    "            avg_std = np.array(conv.get('avg_std', []))\n",
    "            \n",
    "            if len(best_mean) == 0:\n",
    "                ax.text(0.5, 0.5, 'No convergence data', ha='center', va='center',\n",
    "                       transform=ax.transAxes, fontsize=11)\n",
    "                ax.set_title(name, fontweight='bold')\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "            \n",
    "            generations = np.arange(len(best_mean))\n",
    "            \n",
    "            ax.plot(generations, best_mean, color=color, linewidth=2.5,\n",
    "                   label='Best Individual', marker='o', markersize=5,\n",
    "                   markevery=max(1, len(generations)//8))\n",
    "            if len(best_std) > 0:\n",
    "                ax.fill_between(generations, best_mean - best_std, best_mean + best_std,\n",
    "                              color=color, alpha=0.2)\n",
    "            \n",
    "            if len(avg_mean) > 0:\n",
    "                ax.plot(generations, avg_mean, color='#7f8c8d', linewidth=2,\n",
    "                       linestyle='--', label='Population Average', marker='s', \n",
    "                       markersize=4, markevery=max(1, len(generations)//8))\n",
    "                if len(avg_std) > 0:\n",
    "                    ax.fill_between(generations, avg_mean - avg_std, avg_mean + avg_std,\n",
    "                                  color='#7f8c8d', alpha=0.15)\n",
    "            \n",
    "            final_best = best_mean[-1]\n",
    "            final_avg = avg_mean[-1] if len(avg_mean) > 0 else 0\n",
    "            gap = final_best - final_avg if len(avg_mean) > 0 else 0\n",
    "            \n",
    "            stats_text = f'Final Best: {final_best:.4f}'\n",
    "            if len(avg_mean) > 0:\n",
    "                stats_text += f'\\nFinal Avg: {final_avg:.4f}\\nGap: {gap:.4f}'\n",
    "            \n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "                   verticalalignment='top', family='monospace',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', \n",
    "                            alpha=0.9, edgecolor='gray'))\n",
    "            \n",
    "            ax.set_xlabel('Generation', fontsize=10)\n",
    "            ax.set_ylabel('Fitness (AUC)', fontsize=10)\n",
    "            ax.set_title(name, fontweight='bold', fontsize=12)\n",
    "            ax.legend(loc='lower right', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_ylim(0.5, 1.02)\n",
    "            \n",
    "            plot_idx += 1\n",
    "        \n",
    "        for idx in range(plot_idx, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('best_vs_average_fitness.png')\n",
    "    \n",
    "    def plot_all_feature_importance(self):\n",
    "        \"\"\"Plot feature importance for ALL models in a grid layout.\"\"\"\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        models_with_importance = {}\n",
    "        for name, data in models_data.items():\n",
    "            fi = data.get('feature_importance', {})\n",
    "            if 'top_10_features' in fi and fi['top_10_features']:\n",
    "                models_with_importance[name] = fi['top_10_features']\n",
    "        \n",
    "        if not models_with_importance:\n",
    "            return\n",
    "        \n",
    "        n_models = len(models_with_importance)\n",
    "        n_cols = min(3, n_models)\n",
    "        n_rows = (n_models + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(7*n_cols, 5*n_rows))\n",
    "        fig.suptitle('Feature Importance by Model (Top 10 Features)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for idx, (model_name, top_features) in enumerate(models_with_importance.items()):\n",
    "            ax = axes_flat[idx]\n",
    "            \n",
    "            features = [f[0][:20] for f in top_features]\n",
    "            importances = [f[1] for f in top_features]\n",
    "            \n",
    "            y_pos = np.arange(len(features))\n",
    "            color = self._get_color(model_name)\n",
    "            bars = ax.barh(y_pos, importances, color=color, edgecolor='black', alpha=0.8)\n",
    "            \n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(features, fontsize=9)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel('Importance', fontsize=10)\n",
    "            ax.set_title(model_name, fontsize=12, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            for bar, imp in zip(bars, importances):\n",
    "                ax.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "                       f'{imp:.4f}', va='center', fontsize=8)\n",
    "        \n",
    "        for idx in range(n_models, len(axes_flat)):\n",
    "            axes_flat[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('feature_importance_all_models.png')\n",
    "    \n",
    "    def plot_feature_importance_heatmap(self):\n",
    "        \"\"\"Plot comparative feature importance heatmap across all models.\"\"\"\n",
    "        models_data = self.results.get('models', {})\n",
    "        \n",
    "        all_importance = {}\n",
    "        all_features = set()\n",
    "        \n",
    "        for name, data in models_data.items():\n",
    "            fi = data.get('feature_importance', {})\n",
    "            if 'feature_ranking' in fi:\n",
    "                all_importance[name] = dict(fi['feature_ranking'])\n",
    "                all_features.update(all_importance[name].keys())\n",
    "            elif 'top_10_features' in fi:\n",
    "                all_importance[name] = dict(fi['top_10_features'])\n",
    "                all_features.update(all_importance[name].keys())\n",
    "        \n",
    "        if not all_importance or len(all_importance) < 2:\n",
    "            return\n",
    "        \n",
    "        avg_importance = {}\n",
    "        for feat in all_features:\n",
    "            values = [all_importance[m].get(feat, 0) for m in all_importance.keys()]\n",
    "            avg_importance[feat] = np.mean(values)\n",
    "        \n",
    "        top_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        top_feature_names = [f[0] for f in top_features]\n",
    "        \n",
    "        model_names = list(all_importance.keys())\n",
    "        heatmap_data = np.zeros((len(top_feature_names), len(model_names)))\n",
    "        \n",
    "        for j, model in enumerate(model_names):\n",
    "            for i, feat in enumerate(top_feature_names):\n",
    "                heatmap_data[i, j] = all_importance[model].get(feat, 0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        im = ax.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')\n",
    "        \n",
    "        ax.set_xticks(np.arange(len(model_names)))\n",
    "        ax.set_yticks(np.arange(len(top_feature_names)))\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_yticklabels([f[:25] for f in top_feature_names], fontsize=10)\n",
    "        \n",
    "        for i in range(len(top_feature_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                val = heatmap_data[i, j]\n",
    "                color = 'white' if val > heatmap_data.max() * 0.5 else 'black'\n",
    "                ax.text(j, i, f'{val:.3f}', ha='center', va='center',\n",
    "                       color=color, fontsize=8, fontweight='bold')\n",
    "        \n",
    "        ax.set_title('Comparative Feature Importance Across Models', fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(im, ax=ax, shrink=0.8, label='Importance')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('feature_importance_heatmap.png')\n",
    "    \n",
    "    def plot_hyperparameter_pruning(self):\n",
    "        \"\"\"Plot search space reduction analysis.\"\"\"\n",
    "        pruning_data = self.results.get('hyperparameter_pruning', {})\n",
    "        \n",
    "        if not pruning_data:\n",
    "            return\n",
    "        \n",
    "        model_names = list(pruning_data.keys())\n",
    "        pre_prune_combos = []\n",
    "        post_prune_combos = []\n",
    "        reduction_pct = []\n",
    "        default_aucs = []\n",
    "        optimized_aucs = []\n",
    "        \n",
    "        for name in model_names:\n",
    "            data = pruning_data[name]\n",
    "            pre_prune_combos.append(data['pre_prune']['total_combinations'])\n",
    "            post_prune_combos.append(data['post_prune']['effective_combinations'])\n",
    "            reduction_pct.append(data['reduction_metrics']['space_reduction_percent'])\n",
    "            default_aucs.append(data['fitness_improvement'].get('default', 0))\n",
    "            optimized_aucs.append(data['fitness_improvement'].get('optimized', 0))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('GE Hyperparameter Pruning Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1 = axes[0, 0]\n",
    "        bars1 = ax1.bar(x - width/2, pre_prune_combos, width, label='Pre-Prune',\n",
    "                       color='#e74c3c', edgecolor='black')\n",
    "        bars2 = ax1.bar(x + width/2, post_prune_combos, width, label='Post-Prune',\n",
    "                       color='#2ecc71', edgecolor='black')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax1.set_ylabel('Number of Combinations')\n",
    "        ax1.set_title('Search Space Size (log scale)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        ax2 = axes[0, 1]\n",
    "        colors = [self._get_color(name) for name in model_names]\n",
    "        bars = ax2.bar(x, reduction_pct, color=colors, edgecolor='black')\n",
    "        for bar, pct in zip(bars, reduction_pct):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Reduction (%)')\n",
    "        ax2.set_title('Search Space Reduction')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        ax2.set_ylim(0, 100)\n",
    "        \n",
    "        ax3 = axes[1, 0]\n",
    "        valid_idx = [i for i, (d, o) in enumerate(zip(default_aucs, optimized_aucs)) \n",
    "                    if d and o and d > 0 and o > 0]\n",
    "        if valid_idx:\n",
    "            valid_models = [model_names[i] for i in valid_idx]\n",
    "            valid_default = [default_aucs[i] for i in valid_idx]\n",
    "            valid_optimized = [optimized_aucs[i] for i in valid_idx]\n",
    "            \n",
    "            x_valid = np.arange(len(valid_models))\n",
    "            ax3.bar(x_valid - width/2, valid_default, width, label='Default HP',\n",
    "                   color='#95a5a6', edgecolor='black')\n",
    "            ax3.bar(x_valid + width/2, valid_optimized, width, label='GE-Optimized',\n",
    "                   color='#2ecc71', edgecolor='black')\n",
    "            ax3.set_xticks(x_valid)\n",
    "            ax3.set_xticklabels(valid_models, rotation=45, ha='right')\n",
    "            ax3.set_ylabel('Validation AUC')\n",
    "            ax3.set_title('Default vs Optimized Performance')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3, axis='y')\n",
    "            ax3.set_ylim(0.5, 1.05)\n",
    "        \n",
    "        ax4 = axes[1, 1]\n",
    "        if valid_idx:\n",
    "            gains = [(optimized_aucs[i] - default_aucs[i]) * 100 / default_aucs[i] \n",
    "                    for i in valid_idx]\n",
    "            colors_gain = ['#2ecc71' if g > 0 else '#e74c3c' for g in gains]\n",
    "            bars = ax4.bar(x_valid, gains, color=colors_gain, edgecolor='black')\n",
    "            for bar, g in zip(bars, gains):\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, \n",
    "                        bar.get_height() + (0.1 if g > 0 else -0.3),\n",
    "                        f'{g:.2f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "            ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            ax4.set_xticks(x_valid)\n",
    "            ax4.set_xticklabels(valid_models, rotation=45, ha='right')\n",
    "            ax4.set_ylabel('AUC Improvement (%)')\n",
    "            ax4.set_title('Performance Gain from GE Optimization')\n",
    "            ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('hyperparameter_pruning.png')\n",
    "    \n",
    "    def plot_hp_count_comparison(self):\n",
    "        \"\"\"Plot pre-prune vs post-prune hyperparameter COUNT comparison.\"\"\"\n",
    "        pruning_data = self.results.get('hyperparameter_pruning', {})\n",
    "        \n",
    "        if not pruning_data:\n",
    "            return\n",
    "        \n",
    "        model_names = list(pruning_data.keys())\n",
    "        pre_hp_count = []\n",
    "        post_hp_count = []\n",
    "        \n",
    "        for name in model_names:\n",
    "            data = pruning_data[name]\n",
    "            pre_hp_count.append(data['pre_prune']['n_hyperparameters'])\n",
    "            post_hp_count.append(data['post_prune']['n_selected_hyperparameters'])\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, pre_hp_count, width, label='Pre-Prune (Full Search Space)',\n",
    "                      color='#e74c3c', edgecolor='black')\n",
    "        bars2 = ax.bar(x + width/2, post_hp_count, width, label='Post-Prune (Selected by GE)',\n",
    "                      color='#2ecc71', edgecolor='black')\n",
    "        \n",
    "        for bar in bars1:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                   f'{int(bar.get_height())}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        for bar in bars2:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                   f'{int(bar.get_height())}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        for i, (pre, post) in enumerate(zip(pre_hp_count, post_hp_count)):\n",
    "            reduction = pre - post\n",
    "            if reduction > 0:\n",
    "                ax.annotate(f'-{reduction}', xy=(i, max(pre, post) + 0.5),\n",
    "                           ha='center', fontsize=10, color='darkred', fontweight='bold')\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11)\n",
    "        ax.set_ylabel('Number of Hyperparameters', fontsize=12)\n",
    "        ax.set_title('Pre-Prune vs Post-Prune Hyperparameter Count', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='upper right', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_ylim(0, max(pre_hp_count) + 2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_and_show('hp_count_comparison.png')\n",
    "    \n",
    "    def plot_pruning_comparison_table(self):\n",
    "        \"\"\"Create visual tables showing parameter changes for each model.\"\"\"\n",
    "        pruning_data = self.results.get('hyperparameter_pruning', {})\n",
    "        \n",
    "        if not pruning_data:\n",
    "            return\n",
    "        \n",
    "        for model_name, data in pruning_data.items():\n",
    "            param_changes = data.get('param_changes', {})\n",
    "            if not param_changes:\n",
    "                continue\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, max(3, len(param_changes) * 0.8)))\n",
    "            ax.axis('off')\n",
    "            \n",
    "            table_data = []\n",
    "            for param, change in param_changes.items():\n",
    "                default_val = str(change['default'])\n",
    "                optimized_val = str(change['optimized'])\n",
    "                changed = 'Yes' if change['changed'] else 'No'\n",
    "                table_data.append([param, default_val, optimized_val, changed])\n",
    "            \n",
    "            metrics = data.get('reduction_metrics', {})\n",
    "            fitness = data.get('fitness_improvement', {})\n",
    "            \n",
    "            table = ax.table(\n",
    "                cellText=table_data,\n",
    "                colLabels=['Parameter', 'Pre-Prune (Default)', 'Post-Prune (GE)', 'Changed'],\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.3, 0.25, 0.25, 0.2]\n",
    "            )\n",
    "            \n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(10)\n",
    "            table.scale(1.2, 1.5)\n",
    "            \n",
    "            for i in range(4):\n",
    "                table[(0, i)].set_facecolor('#3498db')\n",
    "                table[(0, i)].set_text_props(color='white', fontweight='bold')\n",
    "            \n",
    "            for row_idx, row in enumerate(table_data):\n",
    "                if row[3] == 'Yes':\n",
    "                    table[(row_idx + 1, 2)].set_facecolor('#d5f5e3')\n",
    "                    table[(row_idx + 1, 3)].set_facecolor('#d5f5e3')\n",
    "            \n",
    "            title = f'{model_name}: Hyperparameter Changes by GE'\n",
    "            summary = f\"Combinations: {metrics.get('original_combinations', 'N/A')} -> {metrics.get('effective_combinations', 'N/A')} \"\n",
    "            summary += f\"(Reduction: {metrics.get('space_reduction_percent', 0):.1f}%)\"\n",
    "            \n",
    "            if fitness.get('relative_gain_percent'):\n",
    "                summary += f\" | AUC Gain: {fitness['relative_gain_percent']:.2f}%\"\n",
    "            \n",
    "            ax.set_title(title, fontsize=12, fontweight='bold', pad=20)\n",
    "            fig.text(0.5, 0.02, summary, ha='center', fontsize=10, style='italic')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self._save_and_show(f'pruning_table_{model_name}.png')\n",
    "\n",
    "\n",
    "class GEExperiment:\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or Config()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_names = None\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.results = {}\n",
    "        self.pruning_analyzer = HyperparameterPruningAnalyzer()\n",
    "        \n",
    "        np.random.seed(self.config.RANDOM_SEED)\n",
    "        random.seed(self.config.RANDOM_SEED)\n",
    "        if TABNET_AVAILABLE:\n",
    "            torch.manual_seed(self.config.RANDOM_SEED)\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"LOADING DATA\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        df = pd.read_csv(self.config.DATA_CSV)\n",
    "        print(f\"  Loaded: {df.shape[0]} samples, {df.shape[1]} columns\")\n",
    "        \n",
    "        unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]\n",
    "        if unnamed_cols:\n",
    "            df = df.drop(columns=unnamed_cols)\n",
    "        \n",
    "        id_cols = [col for col in df.columns if str(col).lower() == 'id']\n",
    "        if id_cols:\n",
    "            df = df.drop(columns=id_cols)\n",
    "        \n",
    "        target_col = self.config.TARGET_COLUMN\n",
    "        if target_col not in df.columns:\n",
    "            target_col = df.columns[-1]\n",
    "        \n",
    "        print(f\"  Target column: {target_col}\")\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object':\n",
    "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
    "        \n",
    "        if X.isnull().any().any():\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "        \n",
    "        if y.dtype == 'object':\n",
    "            y = self.label_encoder.fit_transform(y)\n",
    "        else:\n",
    "            y = y.values\n",
    "        \n",
    "        X = X.values.astype(np.float32)\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        print(f\"  Features: {X.shape[1]}\")\n",
    "        print(f\"  Class distribution: {np.bincount(y)}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def prepare_splits(self, X, y):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DATA SPLITS (80/10/10)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.config.TEST_SIZE, stratify=y, \n",
    "            random_state=self.config.RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=self.config.VAL_SIZE, stratify=y_temp,\n",
    "            random_state=self.config.RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        total = len(y)\n",
    "        print(f\"  Train: {len(self.y_train)} samples ({100*len(self.y_train)/total:.1f}%)\")\n",
    "        print(f\"  Val:   {len(self.y_val)} samples ({100*len(self.y_val)/total:.1f}%)\")\n",
    "        print(f\"  Test:  {len(self.y_test)} samples ({100*len(self.y_test)/total:.1f}%)\")\n",
    "        \n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "    \n",
    "    def _process_convergence_data(self, all_convergence):\n",
    "        if not all_convergence:\n",
    "            return {\n",
    "                'best_mean': [], 'best_std': [],\n",
    "                'avg_mean': [], 'avg_std': [],\n",
    "                'mean': [], 'std': []\n",
    "            }\n",
    "        \n",
    "        best_histories = [c['best_fitness'] for c in all_convergence]\n",
    "        avg_histories = [c['avg_fitness'] for c in all_convergence]\n",
    "        \n",
    "        max_gen = max(len(h) for h in best_histories)\n",
    "        \n",
    "        padded_best = []\n",
    "        padded_avg = []\n",
    "        for best, avg in zip(best_histories, avg_histories):\n",
    "            if len(best) < max_gen:\n",
    "                best = best + [best[-1]] * (max_gen - len(best))\n",
    "            if len(avg) < max_gen:\n",
    "                avg = avg + [avg[-1]] * (max_gen - len(avg))\n",
    "            padded_best.append(best)\n",
    "            padded_avg.append(avg)\n",
    "        \n",
    "        best_array = np.array(padded_best)\n",
    "        avg_array = np.array(padded_avg)\n",
    "        \n",
    "        return {\n",
    "            'best_mean': best_array.mean(axis=0).tolist(),\n",
    "            'best_std': best_array.std(axis=0).tolist(),\n",
    "            'avg_mean': avg_array.mean(axis=0).tolist(),\n",
    "            'avg_std': avg_array.std(axis=0).tolist(),\n",
    "            'mean': best_array.mean(axis=0).tolist(),\n",
    "            'std': best_array.std(axis=0).tolist()\n",
    "        }\n",
    "    \n",
    "    def run(self):\n",
    "        experiment_start = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"GE-BASED HYPERPARAMETER OPTIMIZATION - MODIFIED VERSION\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"  Population Size:      {self.config.GE_POP_SIZE}\")\n",
    "        print(f\"  Generations:          {self.config.GE_GENERATIONS}  <-- INCREASED\")\n",
    "        print(f\"  Independent Runs:     {self.config.N_INDEPENDENT_RUNS}\")\n",
    "        print(f\"  Parallel Jobs:        {self.config.N_PARALLEL_JOBS}\")\n",
    "        print(f\"  SMOTE Enabled:        {self.config.USE_SMOTE}\")\n",
    "        print(f\"  TabNet Max Epochs:    {self.config.TABNET_MAX_EPOCHS}  <-- INCREASED\")\n",
    "        print(f\"  TabNet Patience:      {self.config.TABNET_PATIENCE}  <-- INCREASED\")\n",
    "        print(f\"  TabNet Batch Size:    {self.config.TABNET_BATCH_SIZE}\")\n",
    "        print(f\"  TabNet Gamma:         {self.config.TABNET_GAMMA}\")\n",
    "        print(f\"  Show Plots:           {self.config.SHOW_PLOTS}  <-- ENABLED\")\n",
    "        print(f\"  Validation:           Holdout (no CV)\")\n",
    "        print(f\"  Data Split:           80% train / 10% val / 10% test\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        X, y = self.load_data()\n",
    "        self.prepare_splits(X, y)\n",
    "        \n",
    "        results = {\n",
    "            'config': {\n",
    "                'n_independent_runs': self.config.N_INDEPENDENT_RUNS,\n",
    "                'ge_pop_size': self.config.GE_POP_SIZE,\n",
    "                'ge_generations': self.config.GE_GENERATIONS,\n",
    "                'tabnet_max_epochs': self.config.TABNET_MAX_EPOCHS,\n",
    "                'tabnet_patience': self.config.TABNET_PATIENCE,\n",
    "                'tabnet_batch_size': self.config.TABNET_BATCH_SIZE,\n",
    "                'validation': 'holdout',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            },\n",
    "            'models': {},\n",
    "            'statistical_tests': {},\n",
    "            'hyperparameter_pruning': {}\n",
    "        }\n",
    "        \n",
    "        all_model_fitness = {}\n",
    "        \n",
    "        if TABNET_AVAILABLE:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"TabNet - {self.config.N_INDEPENDENT_RUNS} Independent Runs (Parallel)\")\n",
    "            print(f\"  -> {self.config.GE_GENERATIONS} generations, {self.config.TABNET_MAX_EPOCHS} max epochs\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            tabnet_opt = TabNetOptimizer(\n",
    "                self.X_train, self.y_train, self.X_val, self.y_val,\n",
    "                self.X_test, self.y_test, self.config, self.feature_names\n",
    "            )\n",
    "            \n",
    "            print(\"\\n  Evaluating default hyperparameters...\")\n",
    "            default_fitness = tabnet_opt.evaluate_default_params()\n",
    "            print(f\"    Default AUC: {default_fitness:.4f}\")\n",
    "            \n",
    "            # Run parallel optimization\n",
    "            run_start = time.time()\n",
    "            parallel_results = tabnet_opt.optimize_parallel(n_jobs=self.config.N_PARALLEL_JOBS)\n",
    "            parallel_time = time.time() - run_start\n",
    "            \n",
    "            # Process parallel results\n",
    "            all_runs = []\n",
    "            all_convergence = []\n",
    "            all_configs = []\n",
    "            \n",
    "            for result in parallel_results:\n",
    "                if 'error' not in result:\n",
    "                    all_runs.append(result)\n",
    "                    all_convergence.append(result['history'])\n",
    "                    all_configs.append(result['best_config'])\n",
    "                else:\n",
    "                    all_runs.append(result)\n",
    "            \n",
    "            print(f\"\\n  Completed {len(all_configs)} runs in {parallel_time:.1f}s\")\n",
    "            \n",
    "            fitness_values = np.array([r['best_fitness'] for r in all_runs if 'error' not in r])\n",
    "            all_model_fitness['TabNet'] = fitness_values\n",
    "            \n",
    "            valid_runs = [r for r in all_runs if 'error' not in r]\n",
    "            if valid_runs:\n",
    "                best_run_idx = np.argmax([r['best_fitness'] for r in valid_runs])\n",
    "                best_run = valid_runs[best_run_idx]\n",
    "                \n",
    "                print(f\"  Best validation AUC: {best_run['best_fitness']:.4f}\")\n",
    "                print(\"\\n  Training final model...\")\n",
    "                _, final_eval = tabnet_opt.train_final_model(best_run['best_config'], \n",
    "                                                             seed=self.config.RANDOM_SEED)\n",
    "                \n",
    "                convergence_summary = self._process_convergence_data(all_convergence)\n",
    "                \n",
    "                pruning_result = self.pruning_analyzer.analyze_model(\n",
    "                    'TabNet', all_configs, best_run['best_config'],\n",
    "                    default_fitness=default_fitness, optimized_fitness=best_run['best_fitness']\n",
    "                )\n",
    "                results['hyperparameter_pruning']['TabNet'] = pruning_result\n",
    "                \n",
    "                results['models']['TabNet'] = {\n",
    "                    'all_runs': all_runs,\n",
    "                    'runs_summary': {\n",
    "                        'best_fitness': compute_statistics(fitness_values),\n",
    "                        'n_successful_runs': len(fitness_values)\n",
    "                    },\n",
    "                    'convergence_summary': convergence_summary,\n",
    "                    'best_run': best_run,\n",
    "                    'best_run_train_metrics': final_eval['train_metrics'],\n",
    "                    'best_run_test_metrics': final_eval['test_metrics'],\n",
    "                    'feature_importance': final_eval['feature_importance'],\n",
    "                    'default_fitness': default_fitness\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n  TabNet: Test AUC={final_eval['test_metrics']['roc_auc']:.4f}\")\n",
    "        \n",
    "        baseline_models = ['RandomForest', 'LogisticRegression', 'SVM', 'GradientBoosting']\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            baseline_models.append('XGBoost')\n",
    "        \n",
    "        baseline_opt = BaselineOptimizer(\n",
    "            self.X_train, self.y_train, self.X_val, self.y_val,\n",
    "            self.X_test, self.y_test, self.config, self.feature_names\n",
    "        )\n",
    "        \n",
    "        for model_name in baseline_models:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"{model_name} - {self.config.N_INDEPENDENT_RUNS} Independent Runs (Parallel)\")\n",
    "            print(f\"  -> {self.config.GE_GENERATIONS_BASELINE} generations\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            print(\"\\n  Evaluating default hyperparameters...\")\n",
    "            default_fitness = baseline_opt.evaluate_default_params(model_name)\n",
    "            print(f\"    Default AUC: {default_fitness:.4f}\")\n",
    "            \n",
    "            # Run parallel optimization\n",
    "            run_start = time.time()\n",
    "            parallel_results = baseline_opt.optimize_parallel(model_name, n_jobs=self.config.N_PARALLEL_JOBS)\n",
    "            parallel_time = time.time() - run_start\n",
    "            \n",
    "            # Process parallel results\n",
    "            all_runs = []\n",
    "            all_convergence = []\n",
    "            all_configs = []\n",
    "            \n",
    "            for result in parallel_results:\n",
    "                if 'error' not in result:\n",
    "                    all_runs.append(result)\n",
    "                    all_convergence.append(result['history'])\n",
    "                    all_configs.append(result['best_config'])\n",
    "                else:\n",
    "                    all_runs.append(result)\n",
    "            \n",
    "            print(f\"\\n  Completed {len(all_configs)} runs in {parallel_time:.1f}s\")\n",
    "            \n",
    "            fitness_values = np.array([r['best_fitness'] for r in all_runs if 'error' not in r])\n",
    "            all_model_fitness[model_name] = fitness_values\n",
    "            \n",
    "            valid_runs = [r for r in all_runs if 'error' not in r]\n",
    "            if valid_runs:\n",
    "                best_run_idx = np.argmax([r['best_fitness'] for r in valid_runs])\n",
    "                best_run = valid_runs[best_run_idx]\n",
    "                \n",
    "                print(f\"  Best validation AUC: {best_run['best_fitness']:.4f}\")\n",
    "                print(f\"\\n  Training final {model_name} model...\")\n",
    "                _, final_eval = baseline_opt.train_final_model(model_name, best_run['best_config'],\n",
    "                                                               seed=self.config.RANDOM_SEED)\n",
    "                \n",
    "                pruning_result = self.pruning_analyzer.analyze_model(\n",
    "                    model_name, all_configs, best_run['best_config'],\n",
    "                    default_fitness=default_fitness, optimized_fitness=best_run['best_fitness']\n",
    "                )\n",
    "                results['hyperparameter_pruning'][model_name] = pruning_result\n",
    "            else:\n",
    "                best_run = None\n",
    "                final_eval = {'train_metrics': {}, 'test_metrics': {}, 'feature_importance': {}}\n",
    "            \n",
    "            convergence_summary = self._process_convergence_data(all_convergence)\n",
    "            \n",
    "            results['models'][model_name] = {\n",
    "                'all_runs': all_runs,\n",
    "                'runs_summary': {\n",
    "                    'best_fitness': compute_statistics(fitness_values) if len(fitness_values) > 0 else {},\n",
    "                    'n_successful_runs': len(fitness_values)\n",
    "                },\n",
    "                'convergence_summary': convergence_summary,\n",
    "                'best_run': best_run,\n",
    "                'best_run_train_metrics': final_eval.get('train_metrics', {}),\n",
    "                'best_run_test_metrics': final_eval.get('test_metrics', {}),\n",
    "                'feature_importance': final_eval.get('feature_importance', {}),\n",
    "                'default_fitness': default_fitness\n",
    "            }\n",
    "            \n",
    "            if final_eval.get('test_metrics'):\n",
    "                print(f\"  {model_name}: Test AUC={final_eval['test_metrics'].get('roc_auc', 0):.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"HYPERPARAMETER PRUNING SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        pruning_summary = self.pruning_analyzer.get_summary_table()\n",
    "        print(pruning_summary.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STATISTICAL TESTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        model_names = list(all_model_fitness.keys())\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                m1, m2 = model_names[i], model_names[j]\n",
    "                scores1, scores2 = all_model_fitness[m1], all_model_fitness[m2]\n",
    "                \n",
    "                if len(scores1) == len(scores2) and len(scores1) > 0:\n",
    "                    w_stat, w_p = wilcoxon_test(scores1, scores2)\n",
    "                    \n",
    "                    results['statistical_tests'][f'{m1}_vs_{m2}'] = {\n",
    "                        'wilcoxon_statistic': w_stat,\n",
    "                        'wilcoxon_p': w_p,\n",
    "                        'mean_diff': float(scores1.mean() - scores2.mean())\n",
    "                    }\n",
    "                    \n",
    "                    sig = \"***\" if w_p < 0.001 else \"**\" if w_p < 0.01 else \"*\" if w_p < 0.05 else \"\"\n",
    "                    print(f\"  {m1} vs {m2}: p={w_p:.4f} {sig}\")\n",
    "        \n",
    "        total_time = time.time() - experiment_start\n",
    "        results['total_runtime_seconds'] = total_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TEST SET RESULTS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Model':<20} {'AUC':>8} {'Acc':>8} {'Sens':>8} {'Spec':>8} {'F1':>8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for name, data in results['models'].items():\n",
    "            if 'best_run_test_metrics' in data and data['best_run_test_metrics']:\n",
    "                m = data['best_run_test_metrics']\n",
    "                print(f\"{name:<20} {m.get('roc_auc', 0):>8.4f} {m.get('accuracy', 0):>8.4f} \"\n",
    "                      f\"{m.get('sensitivity', 0):>8.4f} {m.get('specificity', 0):>8.4f} \"\n",
    "                      f\"{m.get('f1', 0):>8.4f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Runtime: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "        \n",
    "        os.makedirs(self.config.RESULTS_DIR, exist_ok=True)\n",
    "        results_file = os.path.join(\n",
    "            self.config.RESULTS_DIR,\n",
    "            f\"ge_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        )\n",
    "        \n",
    "        def convert(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=convert)\n",
    "        print(f\"\\nResults saved: {results_file}\")\n",
    "        \n",
    "        self.results = results\n",
    "        \n",
    "        plotter = ResultsPlotter(results, self.config)\n",
    "        plotter.plot_all()\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    config = Config()\n",
    "    experiment = GEExperiment(config)\n",
    "    return experiment.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"GE-based Hyperparameter Optimization - MODIFIED VERSION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nKEY MODIFICATIONS:\")\n",
    "    print(\"  [1] Generations:        10 -> 30  (3x more evolution)\")\n",
    "    print(\"  [2] TabNet Max Epochs:  25 -> 100 (4x more training)\")\n",
    "    print(\"  [3] TabNet Patience:    6  -> 15  (better convergence)\")\n",
    "    print(\"  [4] TabNet Batch Size:  256 -> 128 (finer gradients)\")\n",
    "    print(\"  [5] TabNet Gamma:       1.3 -> 1.5 (feature reusage)\")\n",
    "    print(\"  [6] Show Plots:         False -> True (interactive)\")\n",
    "    print(\"  [7] Population Size:    15 -> 20  (better exploration)\")\n",
    "    print(\"\\nConfiguration Summary:\")\n",
    "    print(f\"  Population Size:      {Config.GE_POP_SIZE}\")\n",
    "    print(f\"  Generations:          {Config.GE_GENERATIONS}\")\n",
    "    print(f\"  Independent Runs:     {Config.N_INDEPENDENT_RUNS}\")\n",
    "    print(f\"  Parallel Jobs:        {Config.N_PARALLEL_JOBS} (-1 = all cores)\")\n",
    "    print(f\"  Validation:           Holdout (no CV)\")\n",
    "    print(f\"  SMOTE:                {Config.USE_SMOTE}\")\n",
    "    print(f\"  TabNet Max Epochs:    {Config.TABNET_MAX_EPOCHS}\")\n",
    "    print(f\"  TabNet Patience:      {Config.TABNET_PATIENCE}\")\n",
    "    print(f\"  TabNet Batch Size:    {Config.TABNET_BATCH_SIZE}\")\n",
    "    print(f\"  TabNet Gamma:         {Config.TABNET_GAMMA}\")\n",
    "    print(f\"  Show Plots:           {Config.SHOW_PLOTS}\")\n",
    "    print(f\"  Models:               6 (TabNet + 5 baselines)\")\n",
    "    print(\"\\nExpected improvements for TabNet:\")\n",
    "    print(\"  - More generations allow better HP exploration\")\n",
    "    print(\"  - More epochs allow model to fully converge\")\n",
    "    print(\"  - Higher patience prevents premature stopping\")\n",
    "    print(\"  - Smaller batch size can improve generalization\")\n",
    "    print(\"  - entmax mask provides sharper feature selection\")\n",
    "    print(\"\\nEstimated Runtime: 30-60 minutes (with parallel execution)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = run_experiment()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
